2018 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023
Dual-Encoder VAE-GAN With Spatiotemporal
Features for Emotional EEG Data Augmentation
Chenxi Tian, Yuliang Ma , Jared Cammon , Feng Fang, Yingchun Zhang , and Ming Meng
Abstract— The current data scarcity problem in EEG-based emotion recognition tasks leads to difficulty in building high-precision models using existing deep learning methods. To tackle this problem, a dual encoder variational autoencoder-generative adversarial network (DEVAE-GAN) incorporating spatiotemporal features is proposed to generate high-quality artificial samples. First, EEG data for different emotions are preprocessed as differential entropy features under five frequency bands and divided into segments with a 5s time window. Secondly, each feature segment is processed in two forms: the temporal morphology data and the spatial morphology data distributed according to the electrode position. Finally, the proposed dual encoder is trained to extract information from these two features, concatenate the two pieces of information as latent variables, and feed them into the decoder to generate artificial samples. To evaluate the effectiveness, a systematic experimental study was conducted in this work on the SEED dataset. First, the original training dataset is augmented with different numbers of generated samples; then, the augmented training datasets are used to train the deep neural network to construct the sentiment model. The results show that the augmented datasets generated by the proposed method have an average accuracy of 97.21% on all subjects, which is a 5% improvement compared to the original dataset, and the similarity between the generated data and the original data distribution is proved. These results demonstrate that our proposed model can effectively learn the distribution of raw data to generate high-quality artificial samples, which can effectively train a high-precision affective model.
Index Terms— Emotion recognition, electroencephalogram, dual-encoder, variational autoencoder-generative adversarial network, data augmentation.
I. INTRODUCTION
E
MOTIONAL computing is a computational approach to measure and analyze the physiological signals in response
Manuscript received 7 February 2023; revised 26 March 2023; accepted 7 April 2023. Date of publication 13 April 2023; date of current version 19 April 2023. This work was supported in part by the National Natural Science Foundation of China under Grant 62071161, Grant 62271181, Grant 61971168, and Grant 61372023; and in part by the Graduate Research Innovation Fund of Hangzhou Dianzi University under Grant CXJJ2022156. (Corresponding authors: Yuliang Ma; Ming Meng.)
Chenxi Tian, Yuliang Ma, and Ming Meng are with the School of Automation, Hangzhou Dianzi University, Hangzhou, Zhejiang 310018, China (e-mail: cxtian@hdu.edu.cn; mayuliang@hdu.edu.cn; mnming@hdu.edu.cn). Jared Cammon, Feng Fang, and Yingchun Zhang are with the Department of Biomedical Engineering, University of Houston, Houston, TX 77204 USA (e-mail: jacammon@cougarnet.uh.edu; ranjitfeng@gmail.com; yzhang94@uh.edu). Digital Object Identifier 10.1109/TNSRE.2023.3266810
to different expressions of emotions [1]. Emotions are physiological changes in response to external stimuli and have two main categories: external representations and internal changes. External features include human body movements [2], facial expressions [3], voice [4], etc. Physiological changes include blood pressure and EEG signals [5], etc. Compared with external signs, physiological signals, such as EEG, spontaneously produced by the human body are not susceptible to the impact of subjective will. They are more responsive to the potential emotional state of an individual. Therefore EEG signal-based emotion recognition has been widely used in different fields. However, with research development, the limitations of EEG-based emotion recognition have gradually emerged. Traditional analysis methods require a great deal of prior knowledge to obtain the in-depth features of EEG, which leads to a considerable amount of time and energy consumption being used for the process of artificial feature extraction. Deep learning can mine underlying essential features from data, so many researchers have applied it to EEG emotional classification [6], [7], [8]. Generally, deep learning methods require many labeled data as training samples. However, unlike image recognition datasets, few public EEG datasets can be used for emotion recognition. The EEG data is challenging to collect and has poor universality among different subjects. As a result, it is difficult to obtain a sufficient amount of labeled data for training deep learning models. Generating high-quality artificial samples to expand the existing datasets may be an efficient solution. Researchers typically generate human samples by transforming raw data, an approach called data augmentation. However, due to the strong temporal correlation of EEG signals, the data cannot be expanded by geometric transformation and other methods. therefore, traditional methods are unsuitable for augmenting EEG signals. On the other hand, the deep generative model can learn the distribution representation from the sample and generate more realistic EEG samples. Goodfellow [9] proposed the generative adversarial network (GAN) in 2014. In 2018, Luo and Lu [10] introduce a Conditional Wasserstein GAN (CWGAN) framework for EEG data augmentation to generate realistic-like EEG data in differential entropy (DE) form and screen high-quality data for classification. Whereas the traditional method is only applicable to the generation of a single data, it is best to consider the relationship between EEG signals in continuous time during data generation.
This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/


TIAN et al.: DEVAE-GAN WITH SPATIOTEMPORAL FEATURES FOR EMOTIONAL EEG DATA AUGMENTATION 2019
To address this problem, a dual encoder variational autoencoder-generative adversarial network incorporating spatiotemporal features is proposed to generate artificial samples, namely, DEVAE-GAN, which considers both the relative position relationship between electrodes and the temporal characteristics of EEG signals. The differential entropy features are first processed into two forms of data corresponding to each other: the features distributed by time series and the features distributed by electrode positions, which are fed into the two encoders of the model. The encoders of the Variational autoencoder (VAE) can map the raw data to the latent space of a specific distribution and input the latent vectors into the decoder to decode them into artificial samples. In the proposed model, one encoder is used to extract information from the processed temporal features, and the other encoder is used to extract spatial information from the features distributed by EEG electrode positions. Finally, the two extracted latent variables are concatenated and decoded, completing the DEVAE-GAN which can learn the distribution of EEG data in a more accurate and multifaceted way to generate artificial samples scientifically. The main contributions of this paper can be summarized as follows: (1) Employs a VAE upgraded with a GAN. (2) Includes a Dual encoder that incorporates both the temporal correlation of EEG and spatial correlation of EEG electrodes. The EEG differential entropy feature data are processed into two corresponding forms for feature extraction: onedimensional differential entropy data distributed by temporal order and two-dimensional differential entropy data distributed by electrode location. Experiments on the publicly available dataset SEED were conducted to analyze the usability of the model and the quality of the generated data in terms of classification accuracy, the similarity of data distribution, and significance tests. The proposed method is compared with classical data generation models. The results showed that the artificial samples generated by the proposed model are usable. The structure of this paper is as follows: Section II reviews the relevant work of emotion recognition methods based on EEG, data augmentation methods based on sequence data, and augmentation methods for EEG. Section III presents the proposed DEVAE-GAN method. Section IV describes the experimental parameter settings. Section V gives the experimental results and discusses them through a systematic comparison of the effects of classical and proposed methods. Section VI discusses the performance of the different models and analyzes the reasons from different aspects. Finally, Section VII provides the conclusions.
II. RELATED WORK A. Emotion Recognition Based on EEG
Among various physiological signals, EEG has been proven to contain rich emotional features and therefore has been widely used in emotion recognition studies [11]. Early researchers used EEG for emotion recognition by pre-extracting features, including time-domain, frequencydomain, time-frequency, and nonlinear features. For example,
Kashihara [12] obtained event-related potential by stimulating subjects and took statistical features such as signal mean and standard difference as EEG features. Tripathi et al. [13] extracted features such as skewness and kurtosis of EEG signals in the DEAP dataset. They used deep neural networks (DNN) and convolutional neural networks (CNN) to classify the features in both validity and arousal dimensions, obtaining 4.51 and 4.96 percentage points of improvement respectively. Zhang and Lee [14] used the amplitude difference between symmetrical electrodes and facial expressions for emotion recognition. Zheng et al. [15] systematically evaluated the performance of various widely used feature extraction methods on DEAP and SEED datasets and found that the differential entropy feature has the best performance. Therefore, this paper uses the DE feature as input. With the rapid development of deep learning, many researchers have applied deep learning methods to EEG emotion recognition. Qing et al. [16] extracted differential entropy features from multiple datasets. They selected decision trees, K-nearest neighbors, and random forest as classifiers and researched emotion classification. Li et al. [17] applied a hierarchical convolutional neural network (HCNN) to EEG classification, extracted the differential entropy features at specific time intervals of each channel, retained the electrode position information, and achieved a classification accuracy of 88.2%. Zhang et al. [18] proposed a spatial-temporal recurrent neural network (STRNN) to analyze both spatial and temporal dependencies of EEG signals and achieved an accuracy of 89.50%. Alhagry [19] et al. applied the long short-term memory network (LSTM) to the DEAP dataset. They reached an average accuracy of 85.65% and 85.45% in arousal and valence, respectively. Wei et al. [20] proposed to combine the transformer with a capsule network and use the open dataset DEAP and DREAMER to achieve an accuracy rate of over 98% at valence, arousal, and dominance dimensions. Wang et al. [21] constructed multimodal mood data and used a variety of methods to test the quality of the database.
B. Data Augmentation Based on Sequence Data
Data augmentation refers to transforming original data to generate new data through specific means. Conventional data augmentation methods were first used in the field of computer vision, including geometric transformation, window clipping, and noise addition. Similar techniques have gradually been used to generate time series data. Iwana and Uchida [22] realized the augmentation of time series data through window warping, flipping, downsampling, and other methods. At the same time, they proposed methods such as window clipping and window slicing to generate multiple new shorter sequences by sliding sampling on the same sequence. Zhou et al. [23] proposed that a new sequence can be generated for numerical data by randomly adding noise to each value. The overall property of the sequence is not affected in this process. Additionally, data augmentation methods based on deep learning have been gradually applied to generate time series data. Such techniques were first used in the field of computer vision as well. The generative adversarial network proposed


2020 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023
by Goodfellow [9] was shown to generate high-quality fake data. On this basis, depth data augmentation models such as deep convolutional GAN (DCGAN) [24], GAN based on Wasserstein distance [25], and GAN with VAE [26] were subsequently proposed. Other examples of deep learning-based augmentation methods are the CGAN-based time conditional GAN (time-conditional GAN, T-CGAN) submitted by Ramponi et al. [27] where he used one-dimensional convolution with the time stamp as input so the model can deal with cases with irregular time intervals. Donahue et al. [28] regarded the spectrum diagram of audio data as pictures and used DCGAN to generate spectrum diagrams. In the Bidirectional long short-term memory convolutional neural network (LSTMCNN) GAN proposed by Zhu et al. [29] Recurrent Neural Network (RNN) is used as a generator, and CNN is used as a discriminator to generate ECG data. EEG signals belong to time series signals, so the above experimental method brings a new idea for the augmentation of EEG data.
C. EEG Data Augmentation
The augmentation of EEG signals can be analogous to the augmentation of other time series signals. Krell and Kim [30] generated artificial EEG signals using a rotation distortion method similar to image affine and rotation distortion. Lotte [31] generated EEG data by combining and distorting the original experimental data. Inspired by other time series data generation methods, Wang et al. [32] directly added different Gaussian noises to the original features. Then, they applied deep neural network output classification accuracy to verify the quality of the artificial data. With the continuous development of GAN-derived models, Luo et al. [33] proposed a method of EEG differential entropy feature sample generation based on CWGAN. Two different classifiers were used to test the quality of generated samples. With the addition of the artificial differential entropy feature samples, the classification accuracy can reach 93.5%, which is 10.2 percentage points higher than the accuracy achieved without adding generated data samples. The VAE-GAN with dual discriminators proposed by Bao et al. [34] transforms differential entropy features into TP-DE features with twodimensional distribution concerning electrode relative position information, generating artificial EEG samples using the information extracted from the raw data, and validating the usability of the generated samples on different classifiers. Zhang et al. [35] proposed a self-supervised generation adversarial network to achieve the end-to-end generation of EEG data. The above methods can effectively improve the model identification accuracy and contribute significantly to the data scarcity problem faced by current EEG data.
III. METHODS
This paper designs a dual-encoder VAE-GAN (DEVAEGAN) with spatiotemporal features to generate EEG data. Illustrated in Fig. 1, the proposed framework consists of two parts: feature extraction and generative network.
A. Feature Extraction
The raw EEG signal contains various redundant information in addition to emotional information. Using a generator to generate the raw signal directly may be disturbed by the redundant information. Therefore, generating the extracted EEG features can effectively avoid the interference of the redundant information and obtain more targeted data suitable for emotion recognition. Previous studies have shown that differential entropy features are the most effective for emotion classification tasks. Luo et al. [33] also improved the accuracy by artificially generating differential entropy samples. Therefore, this work also generates artificial differential entropy samples to expand the dataset. Differential entropy (DE) is a feature extracted from the five frequency bands of the EEG signal in each channel over a specific period. Some studies [36] show that the value of the differential entropy feature is the logarithmic spectral energy of the EEG sequence with a fixed length in a certain frequency band. DE features were extracted from the following five frequency bands: δ: 1-3Hz, θ: 4-7Hz, α: 8-13Hz, β: 14-30Hz, γ : 31-50Hz. The differential entropy characteristic of a certain frequency band is calculated using (1) shown below. Pi can be regarded as an energy spectrum and is equivalent to the value of signal variance multiplying a constant coefficient N ( N is the length of the fixed time window) [37].
hi (x) =
1
2 log 2π eσ 2 =
1
2 log (Pi ) +
1
2 log
( 2π e
N
)
(1)
Considering the dynamic nature of the emotion recognition task based on EEG, this work uses a linear dynamic system approach to filter components unrelated to the emotional state. Since the differential entropy feature does not contain the electrode position distribution information and the timing information, the processed differential entropy feature was used as the input of the network. The differential entropy feature of each unit period extracted from the SEED dataset is a size of 310 with dimensions 62 channels × 5 frequency bands. Bashivan et al. [38] used polar projection to determine the location of electrode distribution, as shown in Fig. 2. Bao et al. [39] mapped the eigenvalues of differential entropy extracted from the SEED dataset one-to-one with the electrode positions in the topology. Considering that the EEG values of non-electrode places were not 0, they used the Clough-Tocher interpolation method to estimate the unmeasured eigenvalues of differential entropy between electrodes. Fig. 2(a) shows the 62 electrodes in two-dimensional coordinates and Fig. 2(b) the visualization of the differential entropy values of a certain frequency band after interpolation. The defect of the work in [33] and [34] is that only the features in a certain period are considered, but the temporal and spatial correlations in continuous periods are not considered. This paper proposes a method to address this limitation in the previous research. Fig. 3 illustrates the steps to cut the differential entropy of the EEG data for a given subject. First, t seconds of differential entropy data are continuously taken from the first second, and t seconds of data are continually taken from every second after


TIAN et al.: DEVAE-GAN WITH SPATIOTEMPORAL FEATURES FOR EMOTIONAL EEG DATA AUGMENTATION 2021
Fig. 1. DEVAE-GAN Framework. On the left is the feature extraction part, which processes the features into temporal and spatial features. On the right is the generation network, which uses two encoders to simultaneously process the two forms of data, and uses the decoder to generate artificial samples.
Fig. 2. Spatial feature and electrode position. (a) 62 electrodes in two-dimensional coordinates. (b) Differential entropy features of theta band for subject 10 after interpolation.
that. To ensure the integrity of the data involved in the training, if the following data is less than t seconds, the previous data will be used to make up for it. In this work, t = 5. Assuming the data duration of a subject in one experiment is T seconds, T slices of data with a duration of t seconds are obtained. The size of a slice of data taken out is 5 seconds × 5 bands × 62 channels. Transform the data into 5 seconds with 310 dimensions; this form of data is only concerned with timing information and does not contain information on channel position. The data is transformed into 5 seconds × 5 bands × 16 × 16. This data is obtained by projecting the DE data in 62 coordinates into 2D coordinates by polar coordinates and interpolating them by Clough-Tocher. The data were transformed into dimensions 25 × 16 × 16, that is, data in relative information about the electrode positions without considering the time sequence but only the electrode positions.
B. VAE
The VAE is a latent variable generative model [40]. The model conducts variational inference on the input data x using an encoder to generate the mean vector and deviation vector of the probability of latent variables, which were consistent with the distribution z of a certain latent variable. The latent vector is resampled from the latent variable distribution. The Decoder reconstructs the original data’s probability distribution according to the latent variable’s probability distribution and generates the artificial sample. The goal of the encoder is to maximize the occurrence probability p(x) of each original data x in the training set. Still, the calculation time complexity
of p(x) is too high. The probability of latent variable z cannot be directly observed, so VAE uses the results qφ(z | x)calculated by the encoder to approximate the true posterior probability pθ (xˆ | z). The Kullback-Leibler divergence (KLD) calculates the distance between the two distributions. The VAE assumes and qφ(z | x) ∼ N (μ (x f
) , 6 (x f
)). The KL divergence between p(z) and qφ(z | x) is expressed as
K L (qφ(z | x)∥ p(z))
= K L (N (μ (xi ) , 6 (xi )) ∥N (0s I ))
=
1
2
(
tr (6 (xi )) + μ (xi )T μ (xi ) − k − log (det (6 (xi )))
=
1
2
[
6 (xi ) + μ2 (xi ) − 1 − log (6 (xi ))
]
(2)
The final goal of VAE is:
max E L B O =
∑
i
E L B Oi
=
∑
i
∑
j
[(xi − xi ′)2 +
1
2 6(xi ) + μ2(xi ) − 1
− log 6(xi )] (3)
where x refers to the reconstructed data, the objective function includes the reconstructed loss and the KL divergence. The reconstruction loss ensures that the reconstructed data relying on the latent variable z is as identical as possible to the original data. The KL divergence between p(z) and qφ(z | xi ) is used to calculate the amount of information lost when qφ(z | xi ) is used to represent p(z). This term ensures the diversity of latent variables.


2022 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023
Fig. 3. Feature cutting process. The left part of the picture represents the data of one subject in one experiment, the middle part is the extracted data slice, and the right part is the processed spatiotemporal features. Where c = 62 is the number of channels, b = 5 represents the five frequency bands, and t represents the duration of each data segment, processing one copy of the cut data can get data in both temporal and spatial forms corresponding to each other.
C. WGAN
Since Goodfellow proposed GAN in 2014, the training of GAN has faced many difficulties until Wasserstein GAN [25] appeared. Wasserstein distance, also known as Earth-Mover (EM) distance, is defined as:
W
(Pr , Pg
) =
1
K sup
∥ f lk ≤k
Ex∼Pr [ f (x)] − Ex∼Pg [ f (x)] (4)
Now the Lipschitz constant of the function f is called K. In the experiment of WGAN, function f is the Discriminator to be trained. In the earliest studies, Arjovsky used reclipping to implement the 1-Lipschitz constraint, which limits the discriminator parameters to a certain range. However, this weighting clipping method tends to maximize or minimize parameter values. Another approach is to add a gradient penalty into the loss function [41]:
mθGin
mθaDx
W (Pr , Pg) = Exr ∼Pr [D(xr )] − Exg∼Pg [D(xg)]
− λExˆr ∼Xˆ [(||∇xˆ D(xˆ)||2 − 1)2] (5)
where λ is the hyperparameter that determines the proportion of the gradient penalty term. xˆ represents data sampled from a mixed sample of generated and original samples.
xˆ = αxr + (1 − α)xg, α ∼ U (0, 1), xr ∼ Xr , xg ∼ Xg (6)
D. VAE-GAN
Since the original GAN model of the discriminator is required to learn similarity measures from images that cannot be expressed intuitively, Larsen et al. [26] proposed to transform the image properties obtained by the discriminator into the reconstruction error of the VAE, adding the feature encoding part of the real samples to the GAN. The decoder of the VAE is regarded as the generator. The loss function of VAE-GAN consists of 3 terms, the first term is the KL loss in VAE and the second term is the reconstruction loss. In the first layer of the discriminator, denotes the data sampled from a mixture of the generated and original samples. The third term is the loss of the discriminator.
L = L prior + L Distl
llike + Lgan
= K L(qφ(z|x)|| pθ (z)) − Eqφ(z|x)[log p(Di sl (x)|z)]
+ log(Dis(x)) + log(1 − Dis(Gen(z))) (7)
The VAE-GAN framework can avoid the pattern collapse problem caused by GAN.
E. DEVAE-GAN
Considering the specificity of EEG signals, this work decides to use two encoders to simultaneously learn the spatiotemporal information of EEG signals, namely, Dual-encoder VAE-GAN (DEVAE-GAN). The proposed data extraction method in III-A mentions that the data are processed into temporal and spatial distribution forms. The encoder-I is used to extract time sequence information, while encoder II is used to extract spatial distribution information. Finally, vectors sampled from latent variable distributions ztime and zspace are obtained. A new latent variable distribution zconcat containing complementary information is received by concatenating the two vectors. The loss of DEVAE-GAN is formulated as
lossDEV AE−G AN
= K L(qφ(zconcat |x)|| pθ (zconcat ))
− Eqφ(zconcat |x)[log p(Di sl (x )|zconcat )]
+ log(Di s(x)) + log(1 − Di s(Gen(zconcat ))) (8)
To avoid the instability of GAN training, this work introduces the principle of WGAN-GP into DEVAE-GAN. The final loss function is defined as:
mθGin
mθaDx
loss = K L(qφ(zconcat |x)|| pθ (zconcat ))
− Eqφ(zconcat |x)[log p(Di sl (x )|zconcat )]
+ Exr ∼Pr [Di sl (xr )] − Exg∼Pg [Di sl (xg)]
− λExˆr ∼Xˆ [(||∇xˆ Di sl (xˆ)||2 − 1)2] (9)
The specific training process is shown in Table I.
IV. EXPERIMENT SETTINGS A. Dataset
The SJTU Emotion EEG Dataset (SEED) is a collection of EEG datasets provided by the Brain-like Computing and Machine Intelligence (BCMI) laboratory, one of the most widely used affective EEG datasets [36], [42]. The SEED dataset contains the EEG signals of 15 subjects. Each video clip can elicit exactly one of three emotions: positive, negative, and neutral. The order of presentation is arranged so that


TIAN et al.: DEVAE-GAN WITH SPATIOTEMPORAL FEATURES FOR EMOTIONAL EEG DATA AUGMENTATION 2023
TABLE I THE ALGORITHM FLOW OF DEVAE-GAN
Fig. 4. Flowchart of data augmentation based DEVAE-GAN. The diamond represents the original data, and the triangle represents the generated data, which is mixed into the expanded dataset. The quality of the generated samples was tested by DNN for classification accuracy.
two film clips targeting the same emotion are not shown consecutively. There were five movie clips for each emotion. Each subject participated in the experiments three times, each of 15 trials, for a total of 45 experiments. Each experiment was spaced at least one week apart. SEED-IV dataset experimental paradigm is the same as SEED, except that it contains four emotions [43]. The signals were sampled at 1,000 Hz with an ESI Neuro Scan System from a 62-electrode headset.
B. Training Parameter Settings
The model structure details of this work are shown in Table II. The optimizer uses the Adam optimizer, the learning rate is set to 0.0001, the batch size is set to 64, the epoch is formed to 150, and the LeakyReLU activation function is used. The coefficient of the gradient penalty term in the loss function λ = 10. The data were standardized before all experiments started. The experimental parameters were set the same in all comparison networks. All the methods in this paper are implemented in Python, and the deep neural network is implemented in PyTorch. The training process is shown in Fig. 4. The labels in the EEG classification task are different from those in the image segmentation task and cannot be generated as the last channel. Therefore, the experiment in this paper generates data for each emotion separately without labels. In the experiment, the EEG signals of each subject
TABLE II DEVAE-GAN ARCHITECTURE. ‘N’ INDICATES THE BATCH SIZE
were divided into the data of each emotion, and each subject’s emotion was trained separately. After the training, the model for each emotion was obtained with the same structure but different parameters. Fifteen thousand samples were generated for each subject. Each subject has three experiments. During the test, the data of one experiment were randomly selected to be used as the original dataset of dataset expansion. Specific evaluation methods will be introduced in the following experimental methods.
V. RESULTS
This section systematically evaluates the effectiveness and generalization ability of the network proposed in this paper from various aspects. Firstly, the datasets are expanded by different quantities of artificial features, and DNN evaluates the classification performance of the expanded datasets of different sizes. The experiment is conducted on the data of each subject. Then, it is extended to different classifiers to test the generalization ability of the generated data. Finally, the similarity between the generated and original data was detected by the similarity measure index.
A. Different Numbers of Appended Training Data
This paper conducted experiments on each subject of the SEED data set and used DNN as the classifier. There were 3394 samples for each subject in each experiment. After the training, 0, 200, 500, 1000, 2000, 3000, 5000, 8000, 10000, and 15000 artificial samples were generated and added to the training dataset, and DNN was used for classification. Table III shows the average accuracy of different models on 15 subjects. When the original dataset was used, the average accuracy was 92.21%. When VAE generated 15,000 samples, the average accuracy rate was 95.91%. After 8000 samples are involved, the classification accuracy rates of the datasets expanded by


2024 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023
Fig. 5. Accuracy improvement after adding samples generated by different models.
WGAN and DCGAN are 93.86% and 95.94%, respectively. With the addition of 3000 samples, VAE-GAN reached the highest accuracy of 93.44%, but with the number of samples increasing, the accuracy decreased even lower than the initial accuracy. After using the above four methods to expand the data set, the accuracy rate is improved by 3.7%, 1.75%, 3.73%, and 1.23%, respectively, compared with the initial accuracy. The method DEVAE-GAN proposed in this paper achieved the optimal accuracy of 97.21% when 15,000 samples were added, which increased by 5% compared with the initial accuracy, showing the best performance among all models.
B. The Performance of Different Generation Models on Each Subject
To verify the generality of generation models on different subjects, this paper compared the accuracy of 15 subjects in the SEED data set with the dataset size changing under the generation models. Due to the special data format in this paper, all classical networks underwent structural changes, and all comparison experiments were conducted under the same experimental environment with the same hyperparameters. Fig. 5 shows the accuracy improvement after adding samples generated by different models. After expanding the dataset, the classification accuracy of the proposed method can be improved by 1.59% - 7.79% compared with only using the original dataset. By observing the data of each subject, it can be seen that, except for the good performance in subjects 6 and 12, VAE-GAN is poor in other subjects compared with other models, and its accuracy improvement is even lower than zero. In subjects 1 and 2, the performance of WGAN was poor. DCGAN performed poorly on subject 3. VAE was not as effective as other models in subject 2. As shown in Fig. 5, the DEVAE-GAN algorithm proposed in this paper has good universality on 15 subjects and even achieved 7.79% improvement in subject 10, which is the best performance among all models.
C. Performance on Different Classifiers and Datasets
In this section, we send the data generated by the proposed model into different classifiers to analyze the recognition accuracy. Two deep neural networks (DNN and CNN) and
Fig. 6. The effect of data augmentation on classification accuracy on different classifiers.
Fig. 7. The effect of data augmentation on classification accuracy on different datasets.
two traditional machine learning methods (SVM and random Forest) were selected. The result is shown in Fig. 6. As seen from Fig. 6, although the initial accuracy of each classifier is different, with the increased amount of added data, the accuracy has been improved to various degrees and the little improvement in accuracy of some classifiers is due to the high initial accuracy. As shown in Fig. 7, the proposed methods also show a nice performance on SEED-IV.
D. Other Evaluation Indicators
1) Similarity Measurement: To prove the reliability of the generation model proposed in this paper, we verify the similarity between the original data and the generated data by using multiple indexes which can calculate the similarity between the distributions. This work uses KL divergence, JS divergence, and EM distance to evaluate the distributions’ similarity. The smaller the value is, the more similar the distribution between the two is. The KL divergence and EM distance can be calculated by referring to formulas (2) and (4) respectively. The KL divergence is known to have an asymmetry, i.e. Therefore, some researchers introduced Jenson’s Shannon divergence (JS divergence). In general, JS divergence is symmetric.


TIAN et al.: DEVAE-GAN WITH SPATIOTEMPORAL FEATURES FOR EMOTIONAL EEG DATA AUGMENTATION 2025
TABLE III CLASSIFICATION ACCURACY AFTER ADDING DIFFERENT NUMBERS OF SAMPLES IN THE TRAINING SET
TABLE IV SIMILARITY MEASUREMENT BETWEEN THE ORIGINAL DATA AND THE SAMPLE GENERATED BY DIFFERENT MODELS
JS divergence is defined as
J S(P∥Q) =
K L(P∥M) + K L(Q∥M)
2 , M=
P+Q
2 (10)
By substituting the KL divergence formula with JS divergence, we can get the following:
J S(P∥Q) =
1
2
∑
p(x) log
( p(x)
p(x) + q(x)
)
+
1
2
∑
q(x) log
( p(x)
p(x) + q(x)
)
+ log 2 (11)
The three indicators calculated from the samples generated by each model are shown in Table IV. The best results are in bold. It can be seen that DEVAE-GAN shows the best performance in the three indexes, followed by WGAN. 2) Significance Test: The significance test is to make a hypothesis about the parameters or distribution form of the population and then use the sample information to judge whether the hypothesis is reasonable, that is, to consider whether there is a significant difference between the real situation of the population and the original hypothesis. In this work, a certain amount of samples were randomly selected to conduct significance tests for the T-test and the rank-sum test. The t-test applies to small sample comparisons between two groups with homogeneous measurement data, normal distribution, and variance. When the sample size is large enough, it can also be used for comparison between ordinary data. The rank-sum test is a nonparametric test. It does not depend on the specific form of the overall distribution and can be applied without considering the distribution of the studied object and whether it is known. The significance test results are shown in Table V. The known significance test hypothesizes that there is a significant difference between the two. In this paper, it is assumed that the distribution of artificial samples and original data is similar. the original hypothesis is abandoned when p>0.05 is selected; there is no significant difference between the generated sample and the original data. As can be seen from
TABLE V SIGNIFICANCE TEST OF SAMPLE AND ORIGINAL DATA GENERATED BY SEED DATASET
Table IV, the samples generated by WGAN, DCGAN, and DEVAE-GAN showed no significant difference from the original data (p>0.05), which indicates that these models can effectively simulate the distribution of real data. The correlation between the artificial samples generated by the proposed method DEVAE-GAN and the actual data was the highest, p = 0.949 by t-test and p = 0.600 by rank-sum test. This proves that the proposed method outperforms all the other methods. Furthermore, we randomly selected part of the data of a subject and drew its box diagram, as shown in Fig. 8. The box diagram is a statistical diagram used to show the dispersion of a group of data, which is mainly used to reflect the distribution characteristics of the original data. The similarity of distribution characteristics of multiple groups of data can be judged by the shape. Fig. 8 shows that the data generated by our method most closely resembles the distribution of the original data.
VI. DISCUSSION
In this study, we proposed a dual encoder variational autoencoder-generative adversarial network by incorporating spatiotemporal features, to generate high-quality artificial samples and solve the current data scarcity problem in EEG-based emotion recognition tasks. Our results show that the proposed method performs well in generating EEG differential entropy features. From the aspects of classification accuracy, objective index, and significance test, we conducted a comprehensive evaluation of its performance by comparing different subjects, different classifiers, and different generation models, which proved the effectiveness and good generalization ability of our method. First, when different numbers of artificial samples are added to the dataset, the classification accuracy of the method proposed in this paper and the other methods have been improved by different degrees. Among them, the DEVAE-GAN proposed in this paper performed best, whereas the VAE-GAN


2026 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 31, 2023
Fig. 8. Box diagram drawn by random extraction of partial data generated by each model.
has the worst performance. It is considered that the structure is more stable due to the introduction of the dual encoder. Meanwhile, the original data’s timing and location distribution information is extracted. Therefore, more information can be considered in the generation process, and more reliable artificial samples can be generated. The proposed method also shows superior performance in different subjects. In the experiments of other generative models, some subjects even experienced a decrease in accuracy due to the increased number of sample generation, which also proved that the artificial samples have poor quality. After expanding the dataset, the classification accuracy of the proposed method can be improved a lot compared with only using the original dataset. It also has good results on different classifiers and different datasets. This also proves that our model has good generality. Through the comparative experiments of various distribution similarity evaluation indexes, it can be proved that the method proposed in this paper can well learn the distribution of real samples and generate similar data. Since the introduction of Wasserstein distance makes the training more stable and the generated artificial sample quality better, the JS divergence, KL divergence, and EM distance of WGAN and DEVAE-GAN proposed in this paper are all lower than those of the other three comparison methods. VAE has the worst score because it has no discriminator to guide its training. The improvement of classification accuracy is particularly obvious in DNN. This is because DNN is composed of a series of fully connected layers with a relatively large number of parameters, so a large amount of data is needed for training to avoid overfitting and other problems. This also confirms our initial hypothesis: deep learning requires a large amount of data for training, and effective data enhancement can effectively solve the problem of difficult EEG collection and few high-quality data. It also proves that our model has good universality, reliability, and stability. The lacking of this work is that due to the complex network structure, the time complexity is relatively high.
VII. CONCLUSION
This paper proposes a data enhancement model for emotion recognition based on EEG: dual-encoder VAE-GAN with spatiotemporal features. The model considers the spatial distribution information and temporal correlation of EEG signals and extracts the temporal and spatial features to guide the generator to generate more realistic samples. In this paper, we use the generated data to augment the original dataset to improve the accuracy of EEG-based emotion recognition and demonstrate the effectiveness of the proposed method in SEED data. First of all, this paper studied the changes in classification accuracy when different amounts of samples were added. Compared with the original dataset, the classification accuracy trained on the fixed datasets was improved by an average of 5%. In addition, the number of artificial samples corresponding to the highest accuracy was also different for different subjects. Furthermore, multiple indicators were used to test the distribution similarity between real samples and artificial samples, and it was explained that the reason for the improvement in accuracy was that the generated data distributed similarly to the original data, which proved that the method proposed in this paper has good generalization ability and data generation effectiveness. This paper uses the differential entropy feature instead of the original EEG signal, which still requires pretreatment operation. Therefore, we will try to realize end-to-end EEG data generation in future work.
REFERENCES
[1] R. W. Picard, Affective Computing. Cambridge, MA, USA: MIT Press, 1997. [2] A. H. Mary, Z. B. Kadhim, and Z. S. Sharqi, “Face recognition and emotion recognition from facial expression using deep learning neural network,” IOP Conf. Ser., Mater. Sci. Eng., vol. 928, no. 3, Nov. 2020, Art. no. 032061. [3] M. Garber-Barron and M. Si, “Using body movement and posture for emotion detection in non-acted scenarios,” in Proc. IEEE Int. Conf. Fuzzy Syst., Jun. 2012, pp. 1–8. [4] T. Bänziger, D. Grandjean, and K. R. Scherer, “Emotion recognition from expressions in face, voice, and body: The multimodal emotion recognition test (MERT),” Emotion, vol. 9, no. 5, pp. 691–704, 2009. [5] L. Bao-Liang, Z. Ya-Qian, and Z. Wei-Long, “Emotional braincomputer interface,” J. Intell. Sci. Technol., vol. 3, no. 1, pp. 36–48, 2021. [6] D. Li et al., “Emotion recognition of subjects with hearing impairment based on fusion of facial expression and EEG topographic map,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 437–445, 2023. [7] T. Song, W. Zheng, P. Song, and Z. Cui, “EEG emotion recognition using dynamical graph convolutional neural networks,” IEEE Trans. Affect. Comput., vol. 11, no. 3, pp. 531–541, Jul./Sep. 2020. [8] W. Niu, C. Ma, X. Sun, M. Li, and Z. Gao, “A brain network analysisbased double way deep neural network for emotion recognition,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 917–925, 2023.
[9] I. J. Goodfellow et al., “Generative adversarial nets,” in Proc. Int. Conf. Neural Inf. Process. Syst. Cambridge, MA, USA: MIT Press, 2014, pp. 2672–2680. [10] Y. Luo and B.-L. Lu, “EEG data augmentation for emotion recognition using a conditional Wasserstein GAN,” in Proc. 40th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), Jul. 2018, pp. 2535–2538.
[11] J. H. Chen, L. Li, and K. X. Qian, “Emotion recognition based on multiple physiological signals,” J. Biomed. Eng. Res., vol. 25, no. 3, pp. 141–146, 2006. [12] K. Kashihara, “A brain-computer interface for potential non-verbal facial communication based on EEG signals related to specific emotions,” Frontiers Neurosci., vol. 8, p. 244, Aug. 2014.


TIAN et al.: DEVAE-GAN WITH SPATIOTEMPORAL FEATURES FOR EMOTIONAL EEG DATA AUGMENTATION 2027
[13] S. Tripathi, S. Acharya, R. Sharma, S. Mittal, and S. Bhattacharya, “Using deep and convolutional neural networks for accurate emotion classification on DEAP data,” in Proc. 29th IAAI Conf., 2017, pp. 4746–4752. [14] Q. Zhang and M. Lee, “A hierarchical positive and negative emotion understanding system based on integrated analysis of visual and brain signals,” Neurocomputing, vol. 73, nos. 16–18, pp. 3264–3272, Oct. 2010. [15] W.-L. Zheng, J.-Y. Zhu, and B.-L. Lu, “Identifying stable patterns over time for emotion recognition from EEG,” IEEE Trans. Affect. Comput., vol. 10, no. 3, pp. 417–429, Jul./Sep. 2017. [16] C. Qing, R. Qiao, X. Xu, and Y. Cheng, “Interpretable emotion recognition using EEG signals,” IEEE Access, vol. 7, pp. 94160–94170, 2019. [17] J. Li, Z. Zhang, and H. He, “Hierarchical convolutional neural networks for EEG-based emotion recognition,” Cogn. Comput., vol. 10, no. 2, pp. 368–380, 2018. [18] T. Zhang, W. Zheng, Z. Cui, Y. Zong, and Y. Li, “Spatial-temporal recurrent neural network for emotion recognition,” IEEE Trans. Cybern., vol. 49, no. 3, pp. 839–847, Mar. 2018. [19] S. Alhagry, A. Aly, and R. A. El-Khoribi, “Emotion recognition based on EEG using LSTM recurrent neural network,” Int. J. Adv. Comput. Sci. Appl., vol. 8, no. 10, p. 355, 2017. [20] Y. Wei, Y. Liu, C. Li, J. Cheng, R. Song, and X. Chen, “TC-Net: A transformer capsule network for EEG-based emotion recognition,” Comput. Biol. Med., vol. 152, Jan. 2023, Art. no. 106463. [21] Q. Wang, M. Wang, Y. Yang, and X. Zhang, “Multi-modal emotion recognition using EEG and speech signals,” Comput. Biol. Med., vol. 149, Oct. 2022, Art. no. 105907. [22] B. K. Iwana and S. Uchida, “An empirical survey of data augmentation for time series classification with neural networks,” PLoS ONE, vol. 16, no. 7, Jul. 2021, Art. no. e0254841. [23] Y. Zhou, X. Zha, and J. Lan, “Transient stability prediction of power systems based on deep residual network and data augmentation,” Electr. Power, vol. 53, no. 1, pp. 22–31, 2020. [24] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing adversarial examples,” 2014, arXiv:1412.6572. [25] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial networks,” in Proc. Int. Conf. Mach. Learn., 2017, pp. 214–223. [26] A. B. L. Larsen, S. K. Sønderby, H. Larochelle, and O. Winther, “Autoencoding beyond pixels using a learned similarity metric,” in Proc. Int. Conf. Mach. Learn., 2016, pp. 1558–1566.
[27] G. Ramponi, P. Protopapas, M. Brambilla, and R. Janssen, “T-CGAN: Conditional generative adversarial network for data augmentation in noisy time series with irregular sampling,” 2018, arXiv:1811.08295. [28] C. Donahue, J. McAuley, and M. Puckette, “Adversarial audio synthesis,” 2018, arXiv:1802.04208.
[29] F. Zhu, F. Ye, Y. Fu, Q. Liu, and B. Shen, “Electrocardiogram generation with a bidirectional LSTM-CNN generative adversarial network,” Sci. Rep., vol. 9, no. 1, p. 6734, May 2019. [30] M. M. Krell and S. K. Kim, “Rotational data augmentation for electroencephalographic data,” in Proc. 39th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), Jul. 2017, pp. 471–474. [31] F. Lotte, “Signal processing approaches to minimize or suppress calibration time in oscillatory activity-based brain-computer interfaces,” Proc. IEEE, vol. 103, no. 6, pp. 871–890, Jun. 2015. [32] F. Wang, S.-H. Zhong, J. Peng, J. Jiang, and Y. Liu, “Data augmentation for EEG-based emotion recognition with deep convolutional neural networks,” in Proc. Int. Conf. Multimedia Modeling. Cham, Switzerland: Springer, 2018, pp. 82–93. [33] Y. Luo, L.-Z. Zhu, Z.-Y. Wan, and B.-L. Lu, “Data augmentation for enhancing EEG-based emotion recognition with deep generative models,” J. Neural Eng., vol. 17, no. 5, Oct. 2020, Art. no. 056021. [34] G. Bao et al., “Data augmentation for EEG-based emotion recognition using generative adversarial networks,” Frontiers Comput. Neurosci., vol. 15, p. 115, Dec. 2021. [35] Z. Zhang, S.-H. Zhong, and Y. Liu, “GANSER: A self-supervised data augmentation framework for EEG-based emotion recognition,” IEEE Trans. Affect. Comput., early access, Apr. 26, 2022, doi: 10.1109/TAFFC.2022.3170369. [36] R.-N. Duan, J.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for EEG-based emotion classification,” in Proc. 6th Int. IEEE/EMBS Conf. Neural Eng. (NER), Nov. 2013, pp. 81–84. [37] L.-C. Shi, Y.-Y. Jiao, and B.-L. Lu, “Differential entropy feature for EEG-based vigilance estimation,” in Proc. 35th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), Jul. 2013, pp. 6627–6630.
[38] P. Bashivan, I. Rish, M. Yeasin, and N. Codella, “Learning representations from EEG with deep recurrent-convolutional neural networks,” 2015, arXiv:1511.06448.
[39] G. Bao et al., “Two-level domain adaptation neural network for EEG-based emotion recognition,” Frontiers Hum. Neurosci., vol. 14, Jan. 2021, Art. no. 605246. [40] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” 2013, arXiv:1312.6114.
[41] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” in Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017, pp. 1–11. [42] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks,” IEEE Trans. Auton. Mental Develop., vol. 7, no. 3, pp. 162–175, Sep. 2015. [43] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “EmotionMeter: A multimodal framework for recognizing human emotions,” IEEE Trans. Cybern., vol. 49, no. 3, pp. 1110–1122, Mar. 2019, doi: 10.1109/TCYB.2018.2797176.