学校代码:10126 学号: 32209021
分类号: TP391 编号:
硕士学位论文
(学 术 学 位)
基于深度学习的 EEG 脑电图信号语音合成
2025 年 4 月 25 日
作 者 姓 名 方元
学 科 专 业 计算机科学与技术
研 究 方 向 脑机接口
指 导 教 师 张学良 教授
培 养 单 位 内蒙古大学计算机学院


内蒙古大学硕士学位论文
I
基于深度学习的 EEG 脑电图信号语音合成
摘要
大脑是人体的中枢指挥系统,脑信号反映了神经元之间复杂且精细的交互活动。对脑信
号的深入研究为人机交互技术开辟了新的可能路径。其中脑信号语音合成技术旨在从脑电信
号中提取语音相关的潜在特征,并借助语音合成模型重构自然语音。该技术在帮助语言障碍
患者恢复交流能力、推动脑机接口技术的发展以及提升智能人机交互系统的自然性与适应性
方面,具有重要的应用前景与研究价值。近年来,相关研究取得了显著进展,已能够在植入
式脑电设备上实现语音合成。然而,在采用非植入式设备采集的脑电图信号上,当前的语音
合成效果仍然有限。尽管脑电图信号精度较低,但非植入设备具有更高的便利性,因此研究
基于脑电图的语音合成具有重要意义。
近年来,深度学习在脑电图语音合成领域得到了广泛应用,多数研究采用端到端方式进
行训练。然而,由于该任务本身的高度非线性、低信噪比特性,以及受限的数据量,现有方
法在拟合大规模模型方面存在一定困难,限制了脑电图语音合成性能的进一步提升。一些工
作关注到了这一问题,尝试对脑电信号进行空间滤波、频域滤波获取其中和语音更相关的特
征以简化问题,但是增加这些传统的预处理方法通常会损失脑电图信号中大量信息,增加滤
波的预处理并不能完美的解决脑电图语音合成中的问题。
因此,本课题聚焦于如何更好的在数据受限情形下,将目前更先进的模型应用到脑电图
语音合成。本文提出了两个算法,基于交叉注意力机制听觉脑电信号语音转换算法和基于离
散扩散模型说话脑电信号语音转换算法。本课题提出的模型在推动脑电图语音合成上提出了
新的方法,并一定程度上克服了数据受限使用复杂模型效果差的缺点。
本课题针对脑电图语音合成任务,分别进行了以下工作:
(1)针对听觉脑电图语音合成任务,本研究提出了基于交叉注意力机制听觉脑电信号语
音转换算法。该方法选用 WaveNet 作为主干网络。提出了多级粒度预测策略,将困难的语音
合成任务分解为四个从简单到复杂的子任务引导模型学习脑电图到语音的转换。此外,本研
究设计了一种混合损失函数,结合不同粒度特征的学习,显著提升了模型训练的稳定性。并
且采用了 Mixup 数据增强方式,有效提高了数据的多样性,提升了模型的泛化能力。在 2024
听觉脑电图挑战赛数据集上的实验结果表明,相比于现有听觉脑电图语音合成方案,本文所
提出的模型在皮尔逊相关系数上有显著提升。


内蒙古大学硕士学位论文
II
(2)对于说话脑电图语音合成任务,本研究提出了基于离散扩散模型说话脑电信号语音
转换算法。该方法首次将离散扩散模型应用到脑电图语音合成任务上,通过脑电图预训练模
型 LaBram 提取语音特征,通过引入预训练模型的先验知识缓解数据量受限问题,使用神经
网络音频编码器 WavTokenizer 离散化语音,将语音压缩到一个离散紧致的低维空间上,完成
脑电图到语音的转换。本文还提出基于双向 Mamba 的线性注意力机制的模型,通过 Mamba
对时序特征建模。在自采的说话脑电图数据集实验结果表明,该方法相比于现有说话脑电图
语音合成方法,本文所提出的方法取得最优的性能。
关键词:深度学习;脑机接口;脑电信号语音合成;卷积神经网络;离散扩散模型


内蒙古大学硕士学位论文
III
Speech Synthesis via EEG signal Based on Deep Learning
ABSTRACT
The brain serves as the central command system of the human body, and brain signals
represent the complex interactions between neurons. The study of brain signals opens new
possibilities for human-computer interaction. Brain signal-based speech synthesis aims to extract
speech-related features from electroencephalogram (EEG) signals and generate natural speech
through speech synthesis models. This technology holds significant potential for facilitating
communication for individuals with speech impairments, advancing brain-computer interface (BCI)
development, and enhancing intelligent human-computer interaction. In recent years, research in
this area has made remarkable progress, achieving speech synthesis using implanted brain-electrical
devices. However, speech synthesis from EEG signals recorded with non-invasive devices remains
challenging. While EEG signals generally exhibit lower accuracy, non-invasive devices offer
greater convenience, making EEG-based speech synthesis a research area of significant importance.
Deep learning has been widely applied in EEG-based speech synthesis, with most studies
employing end-to-end training approaches. However, due to the highly nonlinear nature of the task,
low signal-to-noise ratio (SNR), and limited data availability, existing methods struggle to fit
large-scale models, leading to suboptimal synthesis performance. Some studies have attempted to
address these challenges by applying spatial and frequency-domain filtering to extract more
speech-relevant components from EEG signals, thereby simplifying the problem. However, these
traditional preprocessing techniques often result in the loss of valuable EEG information and fail to
provide a perfect solution to the challenges of EEG-based speech synthesis.
In light of these limitations, this study focuses on effectively applying advanced models to
EEG-based speech synthesis under data-constrained scenarios. Specifically, we propose two novel
models: a WaveNet model guided by cross-attention mechanisms and a speech EEG synthesis
model based on discrete diffusion. These models introduce new methodologies for advancing
EEG-based speech synthesis and, to some extent, mitigate the performance degradation caused by
the data limitations of complex models.
This study explores EEG-to-Speech Synthesis tasks and presents the following contributions:
(1) Auditory EEG to speech synthesis: We propose a WaveNet model guided by


内蒙古大学硕士学位论文
IV
cross-attention mechanisms, using WaveNet as the backbone. A multi-granularity prediction
strategy is introduced, decomposing the complex speech synthesis task into four progressively
challenging sub-tasks to guide the model in learning the mapping from EEG to speech. Additionally,
we design a Hybrid Loss function, which facilitates multi-granularity feature learning and improves
model training stability. To enhance data diversity and model generalization, we employ the Mixup
data augmentation technique. Experimental results on the 2024 Auditory EEG Challenge dataset
demonstrate that our method significantly outperforms the current state-of-the-art auditory
EEG-based speech synthesis approach in terms of Pearson correlation coefficient.
(2) Spoken EEG to speech synthesis: We propose a speech EEG synthesis model based on
discrete diffusion, marking the first application of discrete diffusion models to EEG-based speech
synthesis. The method leverages the LaBram pre-trained EEG model to extract speech-related
features and introduces prior knowledge from pre-trained models to mitigate the challenges posed
by limited data availability. Additionally, the WavTokenizer neural audio encoder is employed to
discretize speech, compressing it into a compact low-dimensional representation for EEG-to-speech
transformation. Furthermore, we introduce a bidirectional Mamba-based linear attention mechanism,
leveraging Mamba for efficient temporal feature modeling. Experimental results on a self-collected
overt speech EEG dataset demonstrate that our proposed method achieves superior performance
compared to the existing state-of-the-art EEG-based speech synthesis models.
Keywords: Deep Learning; Brain-Computer Interface; EEG-to-Speech; Convolutional Neural
Network; Discrete Diffusion Model


目录
第一章 引言...............................................................................................1
1.1 研究背景与意义...............................................................................1
1.2 国内外研究现状...............................................................................2
1.3 论文主要内容和创新点...................................................................4
1.4 论文结构...........................................................................................5
第二章 脑电信号语音合成相关概念描述 ..............................................7
2.1 脑电图信号预处理...........................................................................7
2.1.1 脑电图信号采样和量化.............................................................8
2.1.2 生物电信号的过滤.....................................................................9
2.1.3 重采样.........................................................................................9
2.1.4 时域滤波.....................................................................................9
2.1.5 空间信息提取...........................................................................10
2.2 训练目标.........................................................................................10
2.3 脑电图信号与语音多模态任务 ....................................................11
2.3.1 听觉注意力解码.......................................................................11
2.3.2 听觉脑电图语音合成...............................................................11
2.3.3 说话、想象脑电图语音合成...................................................12
2.4 本章小节.........................................................................................12 第三章 基于交叉注意力机制听觉脑电信号语音转换算法 ................13
3.1 引言.................................................................................................13
3.2 网络介绍.........................................................................................13
3.2.1 WaveNet 模块 ...........................................................................14
3.2.2 多层交叉注意力模块...............................................................15
3.2.3 多级粒度预测策略...................................................................17
3.2.4 Mixup 数据增强 ........................................................................19
3.2.5 损失函数...................................................................................20
3.3 实验设置.........................................................................................21
3.3.1 数据集介绍...............................................................................21
3.3.2 参数设置...................................................................................21
3.3.3 基线系统...................................................................................22
3.3.4 评估指标...................................................................................23
3.4 实验结果分析.................................................................................24
3.4.1 基线方法对比与分析...............................................................24
3.4.2 消融实验对比与分析...............................................................25


3.5 本章小结.........................................................................................26 第四章 基于离散扩散模型说话脑电信号语音转换算法 ....................28
4.1 引言.................................................................................................28
4.2 网络介绍.........................................................................................28
4.2.1 音频编码器 WavTokenizer .....................................................29
4.2.2 脑电图预训练模型 LaBram ...................................................31
4.2.3 离散扩散模型...........................................................................32
4.2.4 损失函数...................................................................................34
4.3 实验设置.........................................................................................35
4.3.1 数据集介绍...............................................................................35
4.3.2 参数设置...................................................................................35
4.3.3 基线系统...................................................................................36
4.3.4 评价指标...................................................................................37
4.4 实验结果分析.................................................................................38
4.4.1 基线方法对比与分析...............................................................38
4.4.2 消融实验对比与分析...............................................................39
4.5 本章小结.........................................................................................40
第五章 总结与展望 ................................................................................42
5.1 工作总结.........................................................................................42
5.2 未来展望.........................................................................................42
参考文献...................................................................................................44


内蒙古大学硕士学位论文
1
第一章 引言
1.1 研究背景与意义
脑机接口[1]是一种通过解码人脑活动,实现大脑与外部设备直接通信的技术。它摆脱了
传统神经肌肉途径的限制,将大脑的意图直接转化为可操作的控制指令。脑机接口是一门融
合神经科学、信号处理、机器学习、工程学等多个学科的交叉领域,已被广泛应用于医疗康
复、辅助沟通、人机交互以及虚拟现实等诸多场景。例如,在临床医疗领域,脑机接口为运
动障碍患者提供了控制假肢、轮椅等外部设备的可能,也为语言功能障碍患者带来了新的沟
通手段。
近年来,随着脑机接口技术的快速发展,基于脑信号的语音合成技术受到了学术界与产
业界的广泛关注。脑信号语音合成通过对脑电信号中与语言相关的活动进行解码,直接生成
自然、流畅的语音,无需额外的物理输入设备。相比传统的辅助沟通技术(如文字转语音系
统),该技术具有更直观、高效和便捷的特点,特别适合于严重语言障碍或失语症患者。
当前常见的脑信号采集技术包括脑电图(Electroencephalography,EEG)、皮层脑电图
(Electrocorticography,ECoG)、脑磁图(Magnetoencephalography,MEG)和功能性磁共振
成像(Functional Magnetic Resonance Imaging,fMRI)等,其中脑电图因其非侵入性、高时
间分辨率及设备便捷性,已成为脑机接口研究中最受青睐的信号类型之一。然而,脑电信号
通常表现出非线性、低信噪比、时空复杂性等特点,这使得脑电信号的精准解码长期以来都
是一项极具挑战的任务。
在过去几十年里,传统脑电图信号处理方法主要基于时频分析、空间滤波、特征提取和
分类器设计等信号处理手段。尽管这些方法能够完成简单的脑机接口任务(如设备控制、简
单意图识别),但在复杂的语音合成任务中表现出明显不足,有一些工作使用信号处理方式
完成想象语音二分类任务,比如 D’Zmura 在 2009 的工作[2]以及 DaSalla 在 2009 年的工作[3],
但是传统信号处理方法不足以完成复杂的语音合成任务,难以有效捕获脑信号中的深层特征
与内在非线性关系。
近年来,以深度学习为代表的人工智能技术取得了突破性进展,极大地推动了脑电图语
音合成领域的发展[4-16]。深度学习技术能够从大量复杂的脑电数据中自动提取高维抽象特征,
并有效建模脑电信号与语音信号之间的非线性映射关系。基于深度学习的脑电图语音合成方
法克服了传统方法的局限,在语音生成的自然度和可懂度等方面取得了显著提升,展现了良


内蒙古大学硕士学位论文
2
好的应用前景。
然而,深度学习在脑电图语音合成领域的研究仍面临诸多挑战。脑电图与语音数据的采
集与对齐成本较高,导致高质量数据集匮乏;同时,深度学习模型的训练过程复杂且对数据
量需求较大,模型的可解释性较弱,难以明确揭示大脑活动与语音特征之间的具体映射机制。
因此,如何在数据受限的条件下,进一步提高脑电图语音合成的性能,并提高模型的泛化能
力与可解释性,成为当前亟待解决的重要问题。
综上所述,开展脑电图语音合成技术的研究,不仅能够推动脑机接口技术的进一步发展,
提升语言障碍患者的生活质量,也为智能人机交互技术的发展提供了新的思路和方法。因此,
本课题的研究具有重要的学术意义和广泛的应用价值。
1.2 国内外研究现状
随着人工智能和深度学习技术的发展,人们对脑机接口越发关注,脑电图语音合成的研
究也逐渐成为了国内外学术界和工业界的热点之一。各大高校和企业也纷纷加入到这一领域
的研究中,各研究机构和公司也在不断推进脑电图语音合成术的研发,为未来的智能人机交
互提供更好的支持。脑电图语音合成这一技术通过解码大脑中的语言相关脑电信号,直接生
成自然的语音信号,不仅为语言障碍患者带来了新的沟通方式,也为未来的智能人机交互提
供了重要支持。在本小节中,本研究将对基于深度学习的脑电图语音合成技术进行介绍,探
究其发展趋势和应用前景。
传统的脑电图信号解码方法主要基于数字信号处理和经典机器学习方法,如共空间模式
(Common Spatial Pattern,CSP)和线性判别分析(Linear Discriminant Analysis,LDA)[17]
等,通过预处理、特征提取和分类实现简单任务[2][3]。然而,这些传统方法受限于脑电信号本
身的高度非线性、低信噪比和个体差异性较大的特点,难以有效捕捉脑信号与语音信号之间
复杂的非线性映射关系,因此在语音合成等复杂任务中的表现往往不尽人意。
近年来,随着深度学习技术的迅速发展,脑电图语音合成研究取得了显著进展,相关方
法在信号解码精度和语音自然度等方面实现了重大突破,成为脑机接口研究中的前沿课题之
一[4-16]。深度学习方法能够自动从原始脑电数据中提取高维复杂特征,显著提升了语音生成的
自然性和可懂度。
在早期研究中,卷积神经网络(Convolutional Neural Network,CNN)和循环神经网络
(Recurrent Neural Network,RNN)被广泛应用于脑电图解码与语音合成。卷积神经网络擅
长从局部区域提取空间特征,循环神经网络则能够捕捉时间序列中的上下文依赖关系。国际


内蒙古大学硕士学位论文
3
上,一些团队采用全卷积网络完成脑电图信号的解码,这些工作包括 VLAAI[18]用 EEG 解码
出语音信号的频谱包络,网络使用卷积神经网络;Katthi and Ganapathy 在 2021 年提出全卷积
网络合成语音信号的频谱包络[19],Thornton 2022 年提出的全卷积网络合成语音信号的频谱包
络[14]。一些团队采用卷积神经网络模块负责空间表征、循环神经网络模块建模时间动态,从
而减轻传统特征工程的负担,研究者 Sakthi[16] 以及 Krishna[15] 将 CNN 与 LSTM 相结合,
用于解码听觉脑电信号并生成基础语音输出,在提升解码准确度的同时兼顾了模型的实时性
与稳健性。国内北大的的团队也有这类的工作[20]。
随着注意力机制的发展,Transformer 模型[21]在脑电语音合成任务上展现出更强的全局表
征能力和泛化性能。这些工作包括北大团队提出的 ConvConcatNet[22],和 Krishna 在 2021 年
的工作[8]和 Piao 在 2023 年的工作[5]、以及 YE Lee 提出的工作 EEG-Transformer[10],大幅提升
了语音生成的自然度与可懂度。
近几年生成对抗网络(Generative Adversarial Network,GAN)[23]和扩散模型(Diffusion
Models)[24-27]在脑电图语音合成领域逐渐成为研究热点,进一步推动了生成质量与多样性的
提升。国外研究团队借助生成对抗网络实现了基于脑信号的低分辨率语音合成包括 Krishna
在 2021 年提出的基于 GRU 的生成对抗网络完成脑电地图合成语音[13],以及 YE Lee 提出的
基于 GAN 的脑电图语音合成模型[11],这些工作验证了生成式模型在脑电图转换语音信号任
务中的潜能。最近也随着扩散模型的进展,近期也出现了 Diff-E[12],AudioDiffusion[28]等基于
扩散的脑电图语音合成工作。整体而言,深度学习在脑电语音合成方向的进展经历了从 CNN、
RNN 到 Transformer,再到对抗生成网络与扩散模型等生成式模型的演进过程,为提高脑电
图语音合成的适用性和准确度奠定了坚实基础。
深度学习在脑电图驱动的语音合成任务中展现出显著优势,主要体现在以下几个方面:
首先,深度学习能够在高维空间中捕捉复杂的非线性关系,通过多层网络结构对原始脑电信
号进行逐级抽象,提取出与语音特征高度相关的内在模式。与传统方法相比,这种自动化的
特征学习过程极大降低了对手工特征工程的依赖,也更易在不同受试者、不同语音任务场景
中迁移与泛化。其次,深度学习模型在设计上具备较高的灵活性,研究者可以根据任务需求
自由调整网络结构或嵌入先验知识,以提升对特定场景或特定类型脑电信号的适应能力。例
如,通过在网络层间加入注意力机制或定制模块,可以更精准地捕捉关键时频信息并过滤冗
余噪声,从而在语音合成质量上获得显著改善。第三,深度学习方法在面对高噪声或低信噪
比数据时,能够通过大规模数据训练或数据增强策略提取稳健特征,对随机噪声具有较强的


内蒙古大学硕士学位论文
4
鲁棒性。此外,依托高性能计算硬件与并行训练框架,深度网络可快速迭代,持续优化网络
结构和参数,进而在较短周期内实现性能飞跃。尽管当前仍然面临数据匮乏、模型解释性不
足、跨个体适用性有待提高等瓶颈,但深度学习所提供的强大建模能力与灵活性,已在多个
脑机接口实验中显示出可观的应用前景,为脑电图语音合成技术的发展带来了源源不断的动
力。
尽管近年来深度学习在脑电图语音合成领域取得了显著进步,但仍面临多重挑战:首先,
高质量的脑电与语音对齐数据难以获取,尤其是对语言障碍患者或特定实验环境的研究,数
据采集过程要求精准的时间同步和严格的实验设计,导致数据量严重不足。其次,脑电图信
号普遍存在低信噪比与高度非线性的问题,环境干扰、个体差异以及神经活动的复杂性都使
得模型难以准确提取与语音映射直接相关的特征。此外,深度学习模型通常被视为“黑箱”,
其内部机制和特征提取过程缺乏可解释性,一旦缺乏足够的可视化与可解释手段,难以在医
疗或高风险领域获得信任和广泛应用。更为关键的是,大型深度模型对数据量和计算资源的
依赖较强,在脑电图这样的小规模、多异质数据环境中很容易出现过拟合或迁移学习受阻的
问题,如何在有限数据的条件下高效地建模和提升模型泛化能力仍有待深入研究。最后,由
于脑电图语音合成在实际应用中往往需要实时性、低功耗和较高的鲁棒性,这对模型架构设
计、硬件平台支持以及噪声控制提出了更高要求,成为制约技术落地的又一瓶颈。
因此,目前脑电图语音合成技术的研究趋势主要集中在如何进一步提高模型性能、实现
高效的数据利用、增强模型的泛化性与可解释性等方向。开展脑电图语音合成技术研究不仅
可以推动脑机接口领域的发展,也将对辅助沟通技术和智能人机交互的创新产生重要影响。
1.3 论文主要内容和创新点
本文聚焦于脑电图语音合成任务,具体的研究内容包含以下两个方面:
(1)研究基于交叉注意力机制听觉脑电信号语音转换算法,采用了 WaveNet[6][29]模型融
合了交叉注意力机制,实现听觉脑电图信号到梅尔谱图的转换:为提升模型的可解释性并在
数据受限情形下更高效地利用数据,本文针对脑电信号与语音信号之间的模态鸿沟,提出了
一种逐步细化语音频谱的思路。具体而言,先从粗粒度的频谱包络(Envelope)开始,再依
次生成十个子带与八十个子带的梅尔谱图(Mel-Spectrogram)最后生成幅度谱(Magnitude
Spectrogram),使语音频谱的构建过程更加透明可控。在模型结构上,引入交叉注意力机制
将脑电图特征与语音特征进行融合,从而缩小不同模态间的语义差距。为缓解过拟合并提高
模型的泛化性能,进一步采用 Mixup[30] 数据增强策略,以及综合损失函数来稳定训练流程。


内蒙古大学硕士学位论文
5
通过上述改进,实现了更高效的脑电图信号到梅尔谱图转换,在生成的语音质量上取得了显
著提升。
(2)研究基于离散扩散模型说话脑电信号语音转换算法,该算法采用基于 Mamba[31][32][33]
的离散扩散模型,实现说话脑电图信号到时域语音转换:为增强模型在语音合成任务中的生
成能力,本文引入了脑电预训练模型 LaBram [34]来提取脑电特征,并结合向量量化(Vector
Quantization,VQ)技术[35],将脑电信号映射为具有语义丰富度的离散序列。借助离散扩散
模型[36][37][38],在离散空间中进行脑电到语音的跨模态生成,有效避免了连续模型对噪声的敏
感性。扩散模型通过逐步恢复信号的方式生成语音离散序列,并最终通过神经网络音频编解
码器(Neural Audio Codec, NAC)[39][40][41]还原成高质量语音信号。该方法在数据有限的条件
下展现出更强的鲁棒性与泛化能力。本课题首次将离散扩散模型引入脑电图语音合成任务,
对跨模态生成具有重要参考价值。
综上所述,本文在增加模型可解释性与提升模型生成能力这两大方向上分别提出了相应
的创新方法。基于交叉注意力机制听觉脑电信号语音转换算法与基于离散扩散模型说话脑电
信号语音转换算法,为脑电图语音合成提供了新的解决思路与技术手段,兼具实用性与前瞻
性。研究结果表明,这两种方法均对脑电图语音合成任务具有显著的提升作用,未来在辅助
语言障碍患者、智能人机交互等应用中具有广阔的前景。
1.4 论文结构
本文的研究工作将分为 5 个章节进行具体介绍,总体结构如下:
第一章:引言。在引言中,首先对脑机接口-脑电图语音合成研究背景与意义进行阐述,
介绍近年来国内外所提出的脑电信号语音解码算法、脑电信号语音合成算法,并指出了各自
存在的问题。最后描述了论文的整体组织结构。
第二章:脑电信号语音合成相关概念描述。在本章介绍了基于深度学习的脑电信号语音
合成整体流程、脑电信号的预处理、训练目标以及脑电图与语音相关的不同任务。
第三章:基于交叉注意力机制听觉脑电信号语音转换算法。本章首先介绍了研究问题以
及所提出的解决方案。此外,介绍了该实验所使用的数据集,参数,损失函数,基线系统,
评估指标等。并对所提出的基于交叉注意力机制引导的 WaveNet 模型做出了全面的评估,验
证了该方法的有效性。
第四章:基于离散扩散模型说话脑电信号语音转换算法。在本章中首先介绍了研究问题
以及所提出的解决方法。此外,介绍了该实验所使用的数据集,参数,损失函数,基线系统,


内蒙古大学硕士学位论文
6
评估指标等。并对所提出的基于离散扩散模型说话脑电信号语音转换算法做出了全面的评估,
验证了该方法的有效性。
第五章:总结与展望。本章对整篇论文进行了总结概括,分析并指出未来的几个可能研
究方向。


内蒙古大学硕士学位论文
7
第二章 脑电信号语音合成相关概念描述
脑电图语音合成旨在从脑电图信号中提取语音相关特征,并生成对应的语音信号,从而
为语言障碍患者提供新的沟通方式,同时也为脑机接口领域的语音交互提供潜在的技术支持。
然而,与传统的语音信号不同,脑电图信号在生理和统计特性上存在显著差异,这使得语音
合成任务面临诸多挑战,例如模态不匹配、数据量受限、个体差异大等。脑电图语音合成中
从去除无关信息(例如生理噪声、环境噪声等),并提取与语音生成相关的有效特征,尤其
重要。在基于深度学习的脑电语音合成总体框架如图 2.1 所示,分为训练阶段以及测试阶段。
在训练阶段,经过预处理后的脑电图信号作为模型的输入,对应的语音信号作为目标。通过
监督学习的方式优化网络的参数使之可以合成目标语音。在测试阶段,将采集到的脑电图信
号经过预处理得到输入特征,作为网络的输入,通过网络的预测得到对应语音的频域特征,
最后还原为时域波形。
图 2.1 脑电图语音合成任务示意图
Figure 2.1 Diagram of EEG-to-Speech Synthesis Task
2.1 脑电图信号预处理
脑电图信号是一种记录大脑神经元活动的电生理信号,目前,在非侵入式脑活动测量方
法中,脑电图是应用最广泛的一种。这主要归因于其高时间分辨率、易用性以及安全性。脑
电图主要通过在头皮放置电极记录大脑皮层的电位变化,以时间序列的形式反映大脑的动态
活动,脑细胞之间通过离子通道和突触传递电信号,这些信号可以通过头皮、骨骼和脑脊液
等组织层传输到头皮表面并形成信号。脑电图信号反映了大量神经元的同步和异步放电活动。
由于其高时间分辨率和非侵入性,脑电图在神经科学、脑机接口以及认知神经科学领域得到
了广泛应用。由于脑电图信号的信噪比较低,使其解读变得困难。
在脑电图到语音信号的转换过程中,干扰信号是影响解码精度和语音合成质量的重要因
素。由于脑电图信号本身的信噪比较低,容易受到多种生理和环境因素的干扰。其中,最常


内蒙古大学硕士学位论文
8
见的干扰包括眼电和肌电伪迹,前者主要来源于眨眼和眼球运动,后者则来自面部肌肉活动,
如咀嚼或说话时的肌肉收缩。此外,心电信号的低频干扰也可能混入脑电图记录,影响信号
的稳定性。环境因素方面,电磁干扰是脑电图采集中的常见问题,电源噪声、实验设备和无
线信号都会对脑电图数据质量产生负面影响。运动伪迹也是不可忽视的干扰源,受试者的头
部或身体移动可能导致电极接触不稳定,引发信号漂移。另一方面,脑电图采集设备本身的
局限性,如电极阻抗变化、信号放大器噪声以及电极接触不良,也会影响数据的质量。因此,
在脑电图语音合成任务中,如何有效去除这些干扰,提高脑电图信号的纯净度,是提升模型
性能和语音合成质量的关键。常见的降噪和去伪迹方法包括滤波、独立成分分析(Independent
Component Analysis,ICA)、伪迹去除算法以及深度学习降噪技术等[42-46],旨在提取更可靠
的脑信号,以增强脑电图与语音信号之间的映射能力。
2.1.1 脑电图信号采样和量化
脑电图信号的采样是模拟脑电图信号转化为数字信号的关键步骤,它通过设置一定的时
间间隔来对脑电图信号进行抽样,得到一系列离散的数值。采样频率,即单位时间内进行采
样的次数,用赫兹(Hz)表示,决定了数字信号的清晰度和所需的存储容量。根据奈奎斯特
采样定理,为了无损的重建原始模拟信号,所需的最小采样频率应为信号中最高频率成分的
两倍,称为奈奎斯特频率。脑电图不同频率波段和不同信息相关,其中包括:δ 波(Delta, <4
Hz):主要出现在深度睡眠或昏迷状态,与脑部修复和恢复有关。θ 波(Theta, 4–7 Hz):
常见于轻度睡眠、冥想或放松状态,与情绪调节和学习记忆相关。 α 波(Alpha, 8–13 Hz):
出现在放松但清醒的状态,有助于注意力调节。β 波(Beta, 14–30 Hz):与认知、思考、
注意力集中相关,通常在清醒和工作状态下占主导。γ 波(Gamma, >30 Hz):与高级认知功
能、信息处理和意识活动相关。在实际应用中,为了更好地适应不同的任务需求,脑电图信
号采集设备通常采用 1024Hz 的采样率。
采样后的脑电图信号虽然在时间域上是离散的形式,但在幅度上依旧保持着连续的特点,
需通过量化进一步离散。在量化过程中,每个采样值都被分配到最近的预定义的量化级别,
并获得一个数字表示。由于实际幅值与固定量化级别之间的近似,会引入量化误差或噪声。
量化的精度由量化位数决定,即用于表示每个采样值的比特数,常使用的量化位数包括 8、
16、24 位等。更高的量化位数意味着更多的量化级别,从而量化误差更小,信号更加精确。
但同时也会增加所需的存储容量。
另外,为了在模型训练中尽可能的使模型学习到脑电图信号到语音信号的转换,需要尽


内蒙古大学硕士学位论文
9
可能多且干净的数据。脑电图与语音的对应数据通常通过实际录制获得。实验过程中,被试
需佩戴电极帽,以同步采集其进行语音任务时的脑电信号。为确保数据质量,被试需端坐于
实验桌前,保持身体静止、双手自然放置,并始终保持注意力集中,以尽量减少运动伪差和
其他电生理干扰。语音任务的指令以语音或图像形式呈现,指导被试完成听觉或发音操作,
从而获取相应的脑电图与语音数据。
2.1.2 生物电信号的过滤
在脑电信号处理中,眼电和肌电伪迹去除是关键的预处理步骤,以提高信号质量和解码
准确性。眼电主要源于眼球运动和眨眼,会在前额及邻近电极处引起低频干扰,而肌电主要
由面部和头部肌肉活动引起,通常表现为高频噪声。常见的去伪迹方法包括独立成分分析,
它可以分离出脑电图信号中的独立源信号,并通过识别伪迹分量进行去除;主成分分析,用
于降维并滤除特定伪迹成分;回归分析,通过建立伪迹信号与脑电图之间的线性回归模型,
减去相关干扰分量。这些方法能够有效提高脑电图信号的纯净度,为后续的特征提取和分类
提供更可靠的基础。
2.1.3 重采样
在预处理之前,一些研究人员会使用降采样(DownSampling)技术,将脑电图数据重新
采样到较低的采样率。这样做的目的是降低数据处理的计算复杂度,使后续分析更加高效。
此外,由于 γ 频段(Gamma, >30 Hz)被认为与语音处理密切相关,因此采样率的选择需要
足以覆盖该频段。例如,言语相关的频段通常集中在 30 Hz 到 100 Hz 之间,并在某些研究中
扩展至 150 Hz 甚至更高,特别是在脑电图语音合成这类,高级语音感知和语言处理任务中。
2.1.4 时域滤波
在脑机接口研究中,研究人员普遍认同特定的频率范围可以获得最佳分类性能。目前在
脑电图语音合成任务中,关于最佳频段的共识仍然缺乏。通常,所选择的频率范围取决于脑
电图信号记录过程中出现的伪迹类型及其去除方式。大多数研究采用的频率范围是 8 至 20Hz,
此外,一些研究也使用 2 至 50Hz 的频带。高 γ频段被认为与语音相关,但由于其信号微弱
且易受噪声影响,在脑电图信号任务中使用较少。Synigal 在 2020 年的研究[47]表明,脑电图 γ
频段的包络与语音信号的相关性较高,而不是原始脑电图信号 γ频段本身,这进一步表明该
频段可能存在低信噪比的问题。在脑电图语音合成任务中,如何准确地去除肌电伪迹,同时
有效利用高 γ频段仍然是一个亟待解决的问题。


内蒙古大学硕士学位论文
10
2.1.5 空间信息提取
大多数研究在预处理过程中不使用任何空间滤波或是双极导联。唯一的例外是 Zhao 在
2015 年的研究[48]以及 Cooney 2018 年的研究[49],他们在预处理中使用了这些方法。这些通常
在分类任务中会带来效果提升,但是对于脑电图和语音多模态任务中,空间滤波通常被避免,
主要原因是可能会减少信号中的有用信息 Saha 2019 的研究[50]。因此,尽管空间滤波可以增
强局部信号并减少全局噪声,但在脑电图和语音多模态任务中,研究人员通常选择直接处理
原始脑电图信号,以尽可能保留全部信息。
2.2 训练目标
在脑电图语音合成任务中,网络的训练目标是关键因素,它直接影响到模型的性能。因
此,一些研究提出了不同的训练目标,可以分为以下两类:基于梅尔谱图,以及基于时域语
音。
基于梅尔谱图的方法,这一方法的核心思想是将脑电图信号转换为梅尔谱图,再通过神
经网络声码器(如 WaveNet[29]、WaveGlow[51]、HiFi-GAN[52])将频谱转换为时域语音信号。
梅尔谱图是一种常用于语音处理的表示方式,它保留了语音的时频结构,同时降低了数据的
维度,使神经网络更易于学习。因此,许多研究选择梅尔谱图作为中间目标,以减少脑电图
到语音直接映射的复杂性。常见的损失函数包括平均绝对误差(Mean Absolute Error,MAE)、
均方误差(Mean Squared Error,MSE)损失。基于时域语音的训练目标,这一方法直接将脑
电图信号映射到时域语音信号,跳过频谱转换过程,尝试构建端到端的脑电图语音合成模型。
直接使用时域语音作为目标的优势在于,它避免了梅尔谱图重建带来的信息损失,同时能够
利用神经网络学习脑电图和语音之间的复杂映射关系。然而,由于脑电图信号本身的模态与
语音信号差异较大,直接训练时域模型往往面临更大的挑战,并且对数据量和模型容量要求
更高。
无论采用哪种训练目标,选择合适的损失函数和优化策略至关重要。基于梅尔谱图的方
法通常更稳定,并在数据量较小时表现较好,而基于时域语音的方法则更具潜力,能够生成
更加自然的语音信号。因此,在脑电图语音合成任务中,不同研究会根据具体应用场景、数
据规模和计算资源选择适合的训练目标。


内蒙古大学硕士学位论文
11
2.3 脑电图信号与语音多模态任务
2.3.1 听觉注意力解码
对于给定的脑电图信号 A 和多声源语音信号 A,方向性解码任务旨在从脑电图信号记录
的神经响应中检测受试者关注的语音信号。其目标可以表示为:
A = AA(A, A) (2.1)
其中 AA 是由参数 A 定义的神经网络模型。通常,该任务涉及音频和脑电图信号的时间对齐,
并通过时间窗滑动处理脑电图片段。给定脑电图片段矩阵 A ∈ AA∗A,其中 A 为时间步, A 为
脑电图信号的通道数,方向性解码的目标是在每个时间步预测受试者的听觉注意力状态:
AA = AA(AA) (2.2)
其中,AA 表示脑电图在时间 A 的输入特征,AA 表示对应的听觉注意力分类标签。模型的优化
目标通常基于交叉熵损失:
A=−
A
AA A𦐀A AA(AA) (2.3)
2.3.2 听觉脑电图语音合成
在听觉脑电图语音合成任务中,目标是从听语音的对应脑电图信号中重建对应的语音频
谱或波形。给定脑电图信号 A 和对应的语音目标 A,该任务可以表示为:
A = AA(A) (2.4)
其中 A ∈ AA×A是脑电图输入,C 是脑电图信号的通道数, A 是时序脑电信号长度,A ∈ AA'×A
是目标语音信号, A' 是语音信号的帧数, A 是语音信号特征维度,AA 是神经网络脑电图语音
合成模型。
在时域中,算法旨在直接从脑电图信号中预测目标语音信号,而不是从其时频表示中预
测。基于 DNN 的时域脑电图语音合成过程可以表示为:AA = AA(AA) 其中,AA 为语音信号的
第 A 帧,AA 为脑电图对应的特征表示。通常,语音信号先分割成重叠的帧,再通过深度神经
网络进行处理。设 A 由语音信号的帧组成的矩阵:
A = A1, A2, ⋯, AA (2.5)


内蒙古大学硕士学位论文
12
2.3.3 说话、想象脑电图语音合成
对于说话、想象脑电图语音合成任务,其目标是在受试者说话或想象说话的情况下,从
脑电图信号生成对应的语音信号。说话、想象说话脑电图语音合成任务与听觉脑电图语音合
成在整体流程上相似,主要区别在于脑电图信号的来源不同。听觉脑电图语音合成任务利用
听觉皮层对外部语音信号的神经响应,而说话、想象说话脑电图语音合成则依赖于大脑在语
言产生过程中的内部神经活动。由于输入脑电图模态的差异,两者在脑区激活模式、信号特
征以及解码策略上存在明显区别。听觉脑电图主要涉及颞叶的听觉皮层活动,信号较稳定且
与语音特征的时间对齐较强;而说话、想象语音脑电图主要涉及额叶、运动皮层和语言相关
区域(如 Broca 区),信号较弱且个体差异较大,因此解码难度更高。此外,应用场景也有
所不同,听觉脑电图语音合成可用于语音增强、助听设备优化等,而说话、想象语音脑电图
语音合成则更适用于脑机接口辅助沟通,如帮助渐冻症患者实现“思维到语音”的转换。
2.4 本章小节
本章旨在介绍脑电信号转换语音相关的一些概念和方法。首先详细介绍了脑电信号基本
概念,以及脑电图信号处理中的基本操作:去干扰信号、重采样、时间滤波。另外,详细的
介绍了脑电信号转换语音中常见的基于梅尔谱图方法与基于时域语音方法。最后讨论了不同
脑电信号与语音多模态任务的区别及作用。


内蒙古大学硕士学位论文
13
第三章 基于交叉注意力机制听觉脑电信号语音转换算法
3.1 引言
目前,基于深度学习的听觉脑电图语音合成受到学术研究人员的广泛的关注,越来越多
的研究正在尝试解码大脑的听觉信息,这项研究可以应用于智能助听器。这些领域中越来越
流行的方法是将人的脑电图与他们正在聆听的自然语音信号的特征联系起来。通常使用线性
回归来预测解码脑电图。鉴于脑电图的信噪比非常低,这是一个具有挑战性的问题,本文提
出了基于交叉注意力机制听觉脑电信号语音转换算法,将脑电图信号到语音信号的转换为了
多个从粗粒度到细粒度的子任务,有效提高了模型的性能。
具体来说,为了解决脑电图到语音信号转换任务中的挑战,本文提出了基于交叉注意力
机制听觉脑电信号语音转换算法。该算法采用多级粒度预测策略、扩张卷积时序建模、交叉
注意力机制融合特征,同时,结合多种损失函数优化模型训练的稳定性,并引入 Mixup 数据
增强技术。通过这些优化策略,本文提出的基于交叉注意力机制听觉脑电信号语音转换算法
在脑电图语音转换任务中取得了显著的性能提升,为脑电图语音解码的实际应用提供了新的
可能性。
3.2 网络介绍
本节将详细介绍基于交叉注意力机制听觉脑电信号语音转换算法。网络结构如图 3.1 所
示,该网络由以下两个模块构成:脑电图解码模块,特征融合模块,网络由多个脑电图解码
模块,特征融合模块重复得到。网络的输入为脑电图信号,输出为语音包络、低维和高维梅
尔谱图以及幅度谱。
该算法采用多级粒度预测策略,从粗粒度到细粒度逐步重建语音信号,以提高最终的合
成质量。该算法采用扩张卷积进行时序建模,通过 WaveNet 结构的网络得到更优的表现。此
外,通过交叉注意力机制融合脑电图和语音特征,有效增强跨模态信息的关联性,提高语音
重建的准确性。为了进一步优化模型效果,采用多级预测策略,依次重建语音频谱包络、低
维和高维梅尔谱图以及幅度谱,从而增强模型根据脑电徐信息对语音信号梅尔谱图的重建性
能。同时,结合多种损失函数优化模型训练的稳定性,并引入 Mixup 数据增强技术,以缓解
过拟合问题,提升泛化能力。
由于说话脑电图数据量较小,直接训练深度模型可能导致过拟合,为了更高效地利用脑
电图数据,本研究引入了多级粒度预测策略,从不同粒度的特征上学习脑信号到语音信号的


内蒙古大学硕士学位论文
14
转换,更高效的利用有限的数据提高语音生成的连贯性和稳定性。在下面的小节中,将分别
介绍网络模块,实验设置,实验结果。
图 3.1 基于交叉注意力机制听觉脑电信号语音转换算法网络结构图
Figure 3.1 network structure diagram of Cross-Attention-Based Auditory EEG-to-Speech Conversion Model
3.2.1 WaveNet 模块
在 WaveNet 网络中,采用扩张卷积对时序信号进行建模,该结构在语音生成、时序预测
等多个领域取得了成功。扩张卷积通过指数增长的扩张率扩大感受野,使模型能够捕捉长时
依赖关系,同时保持较低的计算成本。此外,与 RNN 等循环结构不同,扩张卷积可以并行计
算,提高训练效率,并避免梯度消失问题,使得模型更加稳定。该方法尤其适用于处理具有
稀疏特征的信号,例如脑电图数据以及语音数据。
图 3.2 扩张卷积网络结构图
Figure 3.2 Dilated convolutional network structure diagram
扩张卷积常用于增加卷积神经网络的感受野,在保证输入维度不变的情况下,可以有效
的学习长距离的依赖关系。在扩张率为 A 的卷积中,卷积核之间会进行A − 1 次零填充。一
个大小为 A 的卷积核,设置其扩展率为 A ,可以将感受野从 A 增加到 A − 1 × A − 1 + A。
通过在网络中使用基于指数增长的扩张率(如:2,4,8,16),可以将感受野逐步增大。图


内蒙古大学硕士学位论文
15
3.2 是以卷积核大小为 2,扩张率分别分 1,2,3 时卷积核的视野变化。
另外 WaveNet 还采用门控激活单元来优化信息流控制,提高模型的稳定性和表达能力。
通过门控机制,模型能够动态调节不同时间步的信息流动,增强非线性建模能力,并减少梯
度消失问题,从而提升深层网络的训练效率。该设计还能有效筛选特征,提高语音信号的动
态建模能力。得益于这些优势,WaveNet 在脑电图语音解码任务中展现出强大的特征建模能
力,为脑电图到语音信号的转换提供了高效、稳定的解决方案。
如图 3.3 所示,WaveNet 网络由多个扩张卷积模块构成。将输入的时域脑电信号 A 作为
输入,并生成对应的语音特征。扩张卷积模块由扩张卷积层、门控激活单元以及 1*1 卷积组
成且带有残差链接。本文采用的 WaveNet 由 36 层卷积层组成,并使用 12 轮扩张周期进行建
模,同时设定 128 个通道以增强特征表达能力。卷积层的卷积核大小为 3,步长为 1,以确保
足够的特征捕获能力,同时减少计算开销。通过这种方式,WaveNet 从输入的时域脑电图信
号中提取有意义的语音相关特征 A 。
图 3.3 WaveNet 网络结构图
Figure 3.3 Block Diagram of WaveNet
3.2.2 多层交叉注意力模块
在听觉脑电图语音合成任务中,脑电图信号和语音信号属于不同模态,具有不同的时间
动态特性和信息分布。因此,如何有效融合两种模态的信息对于提升语音重建质量至关重要。
在本研究中,本研究提出了多层交叉注意力模块,用于高效整合脑电图和语音特征,并提升
模型对语音内容的感知能力。多层交叉注意力模块结构如图 3.4 所示。
具体来说,本研究的多层交叉注意力模块通过多层交叉注意力机制在脑电图和语音模态


内蒙古大学硕士学位论文
16
之间进行特征对齐和增强。该模块的核心思想是利用交叉注意力机制学习脑电图与语音特征
之间的映射关系,使模型能够更好地捕捉脑电图信号中与语音相关的关键信息。在结构上,
该模块包含多个交叉注意力层,每一层都在不同的特征层级融合两个模态的信息。
交叉注意力机制是近年来深度学习模型中的关键组件,尤其在多模态学习和序列到序列
建模任务中得到了广泛应用,与自注意力不同,自注意力在同一输入序列中提取查询向量、
键向量和值向量信息,而交叉注意力则在两个不同来源的输入之间建立联系,其中一组输入
作为查询向量,另一组输入则作为键向量和值向量,从而实现跨模态或跨表示空间的信息交
互与对齐。形式上,给定查询向量A ∈ AAA×A、键向量 A ∈ AAA×A、和值向量 A ∈ AAA×A、交叉
注意力计算过程如下:
A𝠀𦐀AA𧀀AAAAA𝀀𦐀A(A,A,A) = A𦐀AAA𤰀A( AA⏉
A )A (3.1)
其中 A 表示特征维度。该机制能够引导查询序列在键值对中动态聚焦于最相关的信息,从而
增强模型的表达能力与建模效果。
图 3.4 多层交叉注意力结构图
Figure 3.4 Detailed architecture of Multi-Layer Cross-Attention


内蒙古大学硕士学位论文
17
交叉注意力广泛应用于神经机器翻译、图文检索、语音合成等任务中,特别适用于需要
对不同模态或不同来源信号进行对齐与融合的场景。需要注意的是,在本研究中,本研究仅
使用与目标语音相关的脑电图特征,并通过多层交叉注意力网络进行多层级的特征融合,使
得最终模型能够充分利用脑电图信息,提高语音解码的准确性和可懂度。
多层交叉注意力模块,由语音特征输入卷积,脑电特征输入卷积,交叉注意力模块,特
征融合输出投影层构成。对于脑电图及语音两个模态脑电图 A 及语音特征 A ,通过输入卷积
层获得特征图特征,将不同维度的特征压缩至 128 维以降低计算复杂度,分别可以表示为:
AA = A𝀀AA𤰀𝠀 A ∈ AA×128 (3.2)
A𤰀 = A𝀀AA𤰀𝠀 A ∈ AA×128 (3.3)
多层交叉注意力模块由两层交叉注意力组成,每个交叉注意力模块分别包含全连接,层
归一化,最后两个模态输出通过全连接进行压缩。
AA = A𝀀AA𤰀𝠀(A𧀀A(AAA𧀀(AA, A𤰀) + AAA𧀀(A𤰀, AA))) ∈ AA×128' (3.4)
3.2.3 多级粒度预测策略
在脑电图语音合成任务中,由于脑电图信号与语音信号在模态上的巨大差异,直接从脑
电图信号预测高维语音特征往往面临建模复杂度高、优化难度大、语音质量较差等挑战。为
了解决这一问题,本文提出了一种多级粒度预测策略(Coarse-to-Fine Granularity Strategy),
通过逐步增加预测目标的复杂性,使得模型能够在稳定学习脑电图与语音特征映射关系的同
时,逐步提升语音信号的质量和可懂度。
该方法的核心思想是按照粗粒度到细粒度的层次递进,让模型先学习低维的、全局性强
的语音特征,如语音信号的频谱包络,然后逐步预测更精细的语音特征,如十个子带梅尔谱
图、八十个子带梅尔谱图再到幅度谱。这种方式不仅有助于降低模型的学习难度,还能在预
测过程中有效利用上下文信息,提高语音信号的清晰度和自然度。其中,主要关注第二阶段
十个子带梅尔谱图的输出,因为它已经包含了足够的语音信息,并且相对低维,便于进一步
处理。然而,在实验过程中,增加后两个更精细阶段的训练目标,能够提升模型的整体性能,
即使在推理时不使用这两个阶段的输出,模型在十个子带梅尔谱图的预测上变得更加精确和
稳定。
具体来说,在模型的第一阶段,首先预测语音信号的频谱包络特征。频谱包络特征反映
了语音信号的整体能量变化模式,是语音感知过程中最重要的信息之一。研究表明,人耳对
语音的可懂度高度依赖于频谱包络信息,即使去除了语音的频谱细节,仅保留包络信息,人


内蒙古大学硕士学位论文
18
类仍然能够在一定程度上识别语音内容。因此,在脑电图语音合成任务中,首先学习语音频
谱包络可以帮助模型捕捉脑电图与语音之间的基础映射关系,并建立稳定的初步预测。频谱
包络预测可定义为:
AAAA = A𤰀AAAAA1(x) (3.5)
其中 x 为脑电图信号,A𤰀AAAAA1代表第一个 WaveNet 模块,AAAA代表预测的频谱包络。
在第二阶段,模型在语音包络的基础上,进一步预测十个子带的梅尔谱图。梅尔谱图是
一种常用于语音处理的时频表示,它将语音信号的频谱特征映射到梅尔频率尺度,以更符合
人耳的听觉感知特性。由于梅尔谱图的维度较高,直接从脑电图信号预测完整的梅尔谱图可
能会导致学习难度过大,因此本研究采用了分层预测的方法,先预测低维梅尔谱图,使模型
能够在较低维度下学习脑电图到语音频谱的映射关系,并逐步优化语音特征的细节。十个子
带的梅尔谱图预测可表示为:
AAAA10 = AAA𦐀A𝀀AAAA𦐀AA1( x, AAAA ) (3.6)
其中 x 为脑电图信号, AAAA为前一阶段预测的语音频谱包络,AAA𦐀A𝀀AAAA𦐀AA1代表第一个解
码模块,AAAA10代表预测的十个子带的梅尔谱图。预测阶段不仅依赖于输入的脑电图信号,还
参考了第一阶段预测的频谱包络信息 AAAA。
在第三阶段,模型进一步预测八十个子带的梅尔谱图,即完整的梅尔谱图表示。相比于
十个子带的梅尔谱图,八十个子带的梅尔谱图能够提供更丰富的频谱信息,增强语音信号的
音质和清晰度。然而,由于脑电图信号本身的模态特性,直接从脑电图预测八十个子带的梅
尔谱图可能会导致语音质量下降。因此,在本文提出的方法中,对完整梅尔谱图预测是在已
经学会十个子带的梅尔谱图的基础上进行的,这样可以充分利用模型在前一阶段已经学习到
的特征,使高维预测更加稳定,并确保语音信号的完整性。八十个子带的梅尔谱图预测可表
示为:
AAAA80 = AAA𦐀A𝀀AAAA𦐀AA2( x, AAAA10 ) (3.7)
其中 x 为脑电图信号, AAAA10为上阶段预测的十个子带梅尔谱图, AAA𦐀A𝀀AAAA𦐀AA2 代表第二
个解码模块,AAAA80代表预测的八十个子带的梅尔谱图。该过程建立再十个子带梅尔谱图预测
的基础上,使后续更高维、精细的语音特征预测更加稳定。
在第四阶段,模型预测幅度谱,进一步增强语音信号的复杂度。幅度谱包含了语音信号
的重要频率成分信息,是语音合成过程中必不可少的一部分。由于幅度谱相比于梅尔谱图更
加精细,因此直接从脑电图信号预测幅度谱可能会导致训练不稳定。而在本文提出的方法中,


内蒙古大学硕士学位论文
19
幅度谱的预测是建立在已经学习到的八十个子带的梅尔谱图之上的,这样可以使模型在逐步
优化的过程中稳定学习频谱信息,确保语音信号的最终质量。幅度谱预测可表示为:
AA𤰀A = AAA𦐀A𝀀AAAA𦐀AA3( x, AAAA80 ) (3.8)
其中AAA𦐀A𝀀AAAA𦐀AA3 代表第三个解码模块,x 为脑电图信号, AAAA80为上阶段预测的八十个子
带梅尔谱图,AA𤰀A代表预测的幅度图。该过程依赖于上一阶段预测的八十个子带梅尔谱图,
使模型能进一步优化语音信号的细节。
多级粒度预测策略的优势在于,它能够让模型从低维到高维、从粗粒度到细粒度逐步学
习脑电图到语音的映射关系,避免了直接预测高维语音特征所带来的优化困难。通过分阶段
学习,每个阶段的预测都建立在前一阶段的基础上,使得模型能够稳定地优化,并最终获得
高质量的语音输出。此外,这种策略还能够提升模型的泛化能力,使其在不同被试数据上均
表现良好。
实验结果表明,相较于直接预测梅尔谱图的方法,多级粒度预测策略在脑电图语音合成
任务中取得了更优的表现。具体而言,该策略能够显著提升语音信号的清晰度、自然度和可
懂度,同时提高模型的收敛速度和训练稳定性。这一策略的成功应用,进一步证明了在脑电
图语音合成任务中,采用由粗粒度到细粒度的预测方式是提升模型性能的有效方法。未来,
该策略还可以推广到其他脑机接口任务,如脑电图语音增强、脑电图语音识别等,为脑机接
口领域的研究提供更多的可能性。
3.2.4 Mixup 数据增强
不同被试之间的脑电图信号存在显著差异,这种个体间的区别主要体现在神经活动模式、
信号幅值、频率分布以及电极信号质量等方面。即使是在相同的任务条件下,不同个体的大
脑对外部刺激的反应也可能存在较大变异。此外,被试的生理状态(如注意力水平、疲劳程
度)、头皮与电极的接触情况以及大脑皮层结构的个体差异,都会影响脑电图信号的稳定性
和一致性。这些因素使得跨被试脑电图语音合成任务具有较大的挑战,为了提高模型在跨被
试脑电图语音解码任务中的泛化能力,本研究引入了 Mixup 数据增强策略[30],通过混合不同
被试的脑电图信号和对应的语音特征,使模型能够学习更具鲁棒性的映射关系,从而缓解个
体差异对模型性能的影响。
具体来说,Mixup 通过线性插值的方式,将两个不同被试的脑电图信号及其对应的语音
特征进行加权混合,生成新的训练样本。定义两个来自不同被试的脑电图语音样本为:


内蒙古大学硕士学位论文
20
(A𝀀 , A𝀀) ,(AA, AA) (3.9)
其中A𝀀, AA分别表示来自不同被试的脑电图特征,A𝀀, AA分别表示对应的语音特征。
本研究使用 Mixup 进行混合,生成新的脑电图-语音对:
A = λA𝀀+(1-λ)AA (3.10)
A = λA𝀀+(1-λ)AA (3.11)
其中,A ∽ (0,1)其中,A(0,1)表示 λ服从均匀分布,在区间 [0,1] 内随机采样。
通过这种方式,Mixup 使得模型能够在训练过程中接触到更多样化的脑电图-语音映射关
系,从而提高对不同个体脑电图信号的适应能力,增强模型的跨被试泛化能力,同时降低过
拟合风险。
3.2.5 损失函数
在本实验中,采用的损失函数由两个主要部分组成:Mel10 负皮尔逊相关性损失和多级
粒度特征的 L1 损失(Env、Mel10、Mel80、Mag),以确保模型能够在不同层次上优化脑
电图到语音的映射关系。
皮尔逊相关系数(Pearson Correlation Coefficient)用于衡量两个变量之间的线性相关性,
定义如下:
AAA(AAAA10, AAAA10) = A (AAAA10,A − AAAA10)(AAAA10,A − AAAA10)
A (AAAA10,A − AAAA10)2
A (AAAA10,A − AAAA10)2
(3.12)
AAAA10和 AAAA10分别表示模型预测的十个子带梅尔谱图和真实十个子带梅尔谱图,AAAA10
和 AAAA10分别表示它们的均值,t 表示时间帧索引。为了使十个子带梅尔谱图预测结果与真
实值高度相关,最小化其负皮尔逊相关性作为损失:
AAAA = − AAA(AAAA10, AAAA10) (3.13)
在AAAA的基础上,加入AA𧀀A,为了保证模型能够逐步优化从脑电图到语音的不同特征,
在语音包络、十个子带梅尔谱图、八十个子带梅尔谱图、幅度谱这四个层级上计算平均绝对
误差损失该损失函数可以表示为:


内蒙古大学硕士学位论文
21
AAAA = ∥AAAA − AAAA∥1
AAAA10 = ∥AAAA10 − AAAA10∥1
AAAA80 = ∥AAAA80 − AAAA80∥1
AA𤰀A = ∥AA𤰀A − AA𤰀A∥1
AA𧀀A = AAAA+AAAA10+ AAAA80+AA𤰀A
(3.14)
其中,AAAA是真实的语音包络(Envelope)特征。AAAA是模型预测的语音包络。AAAA10是
真实的 10 维 Mel 频谱(低维频谱特征)。SMel10是模型预测的 10 维 Mel 频谱。AAAA80
是真实的 80 维 Mel 频谱(完整的 Mel 频谱)。AAAA80是模型预测的 80 维 Mel 频谱。AA𤰀A
是真实的语音幅度谱(Magnitude Spectrum)。AA𤰀A是模型预测的语音幅度谱。
最终的损失函数,采用以上两种损失函数的组合,并设置了不同的权重,可以表示为:
AAAA𝠀𤰀AA = A×AAAA + β×AA𧀀A (3.15)
3.3 实验设置
3.3.1 数据集介绍
在本研究中,采用 Auditory EEG Challenge 2024 数据集[53]评估所提出网络的性能。该数
据集是一个中等规模的多被试听觉脑电图数据集。Auditory EEG Challenge 2024 数据集采用
高品质的 Biosemi ActiveTwo 系统进行脑电信号采集,该系统配备 64 个电极,采样率为 8192
Hz。该数据包含 105 名年轻、听力正常的受试者,母语为荷兰语。每个受试者聆听 8 到 10
次试验,每次试验大约 15 分钟。试验的顺序在参与者之间是随机的。语音内容包括播客和声
读物。超过 15 分钟的有声读物被分成两次试验,连续呈现给受试者。训练集包含来自 85 名
受试者的 655 次试验(每次 15 分钟),使用 72 种不同的语音内容,总计 9420 分钟(157
小时)。测试集共包含 20 名受试者、15 种不同的刺激物,共计 2315 分钟数据(38 小时)。
脑电信号使用多通道维纳滤波器去除伪影。归一化后被重采样为 64Hz,音频信号被重新采样
为 16kHz。
3.3.2 参数设置
本实验采用 PyTorch 实现,使用 Adam[55]优化器优化网络参数,学习率设置为 0.0005。并
使用 StepLR 学习率调度器,其衰减因子设为 0.9。首先,模型在未使用 Mixup 数据增强的
情况下进行了 300 轮训练,以在验证集上找到表现最佳的模型。随后,引入 Mixup 数据增
强技术,并在此基础上继续训练 2000 轮,进一步优化模型性能。本文使用的 WaveNet 由
四个阶段组成,每个阶段包含 36 层,并采用 12 轮扩张周期。前三个阶段的通道数为 128,


内蒙古大学硕士学位论文
22
而最后一个阶段的通道数为 512。对于最终的损失函数,其中的损失权重 α 为 1,β 为 0.2。
3.3.3 基线系统
为了验证所提出的基于交叉注意力机制听觉脑电信号语音转换算法的性能,采用以下几
个基线系统进行比较:分别为 DPRNN[54]、VLAAI[18]、CONVCONCATNET[22]。下面将给出
每个基线系统的具体配置信息。
图 3.5 DPRNN 网络结构图[54]
Figure 3.5 Detailed architecture of DPRNN[54]
DPRNN 是一种专门用于序列建模的双路径循环神经网络,其核心思想是在时间和通道
维度上交替使用循环神经网络结构,以更有效地捕捉序列的长期依赖关系。基于原论文的最
优网络配置进行了复现,其具体网络结构如图 3.5 所示。
DPRNN 采用帧级(intra-chunk)和全局级(inter-chunk)交替计算的策略。在时间和通
道两个方向分别采用双向长短期记忆网络(Long Short-Term Memory,LSTM)进行特征处理,
使其能够更好地建模长期依赖关系。能够有效建模长时依赖,适用于 EEG 语音任务等涉及
长时序列建模的问题。适用于低资源 EEG 任务,因为其参数量相对适中,并能在小规模数
据集上取得较好的泛化能力。
VLAAI 是 2024 Auditory EEG Challenge 提供的基线模型,专为听觉脑电图语音解码任
务设计。该模型采用卷积网络结构,结合全连接层进行特征映射,并通过上下文卷积层建模
时间依赖关系。该模型适用于低计算资源环境。但其解码精度仍然有限,在语音包络预测任
务上未能取得更大的突破。VLAAI 的网络结构如图 3.6 所示,基于原文的配置进行复现。


内蒙古大学硕士学位论文
23
图 3.6 VLAAI 网络结构图[18]
Figure 3.6 Detailed architecture of VLAAI[18]
ConvConcatNet 是基于卷积和注意力机制的语音解码网络,网络结构如图 3.7 所示,基于原
文的配置进行复现。该网络在预测梅尔频谱时,引入语音包络信息作为辅助特征,以增强脑
电图到语音的映射能力。模型首先采用卷积层对脑电图进行特征提取,并融合包络信息后进
入注意力模块。随后,模型通过级联策略,在不同计算层级聚合多尺度特征,以强化全局上
下文信息。最终,解码层基于提取的特征生成梅尔谱图表示,并结合语音重建策略还原目标
语音。
3.3.4 评估指标
为了有效的评估提出的基于交叉注意力机制听觉脑电信号语音转换算法的性能,采用以
下指标:皮尔逊相关系数。在脑电图语音合成任务中,皮尔逊相关系数通常用于衡量预测的
语音特征(如梅尔谱图或频谱包络)与真实语音特征之间的相似性。皮尔逊相关系数是一种
度量两个变量之间线性相关性的统计指标,其取值范围为 [−1,1],
皮尔逊相关系数为一表示完全正相关,即当一个变量增加时,另一个变量也严格增加。
皮尔逊相关系数为负一表示完全负相关,即当一个变量增加时,另一个变量严格减少。皮尔
逊相关系数为零表示两个变量之间不存在线性相关性。


内蒙古大学硕士学位论文
24
图 3.7 ConvConcatNet 网络结构图[22]
Figure 3.7 Detailed architecture of ConvConcatNet[22]
3.4 实验结果分析
3.4.1 基线方法对比与分析
在本节中,分别将提出的基于交叉注意力机制听觉脑电信号语音转换算法(CAGWaveNet)
与当前主流的听觉脑电图解码模型进行了对比,并在 Auditory EEG Challenge 2024 数据集上
进行了全面的评估。以皮尔逊相关性系数作为评价指标。表 3.1 展示了在验证集和测试集上
三个基线系统与基于交叉注意力机制听觉脑电信号语音转换算法的 PCC 比较,且最好的结果
采用粗体标记。可以得出,CAGWaveNet 相比于基线系统具有更优异的性能,这证明了本研
究提出的多级粒度预测策略的有效性。在测试集上皮尔逊相关性系数相较于基线模型 VLAAI
提高了 0.0218,在验证集高相较于 VLAAI 提高了 0.0181,特别是在语音和被试都未见过的测
试集上,本研究提出的 CAGWaveNet 更加鲁棒,展现了其在听觉脑电图解码任务上拥有更强
的跨被试泛化能力。
CAGWaveNet 与 VLAAI、DPRNN 和 ConvConcatNet 进行对比,所提出的模型远远优于
其他听觉脑电图解码模型,完成了大幅度的性能提升。ConvConcatNet 作为基线系统中性能
最佳的模型,实现了在验证集上平均 0.0622 的 PCC 得分,在测试集上平均 0.0420 的 PCC 得
分。而 CAGWaveNet 实现了性能上的最大提升,分别在验证集和测试集上提升 0.0029 和 0.0012。
这种性能的提升可以归因于多个因素。首先,不同粒度语音信号的加入提供了额外的不
同粒度语音信息(频谱包络、八十个子带的梅尔谱图、幅度谱)引导模型的解码结果,帮助
模型更准确理解脑电图到语音信号的转换过程,从而更好地分解码听觉脑电图信号。其次,


内蒙古大学硕士学位论文
25
通过交叉注意力机制融合脑电信号与语音信号两种不同模态特征,进一步增强了模型性能。
此外,CAGWaveNet 通过扩张卷积模块,有效的对脑电图信号进行时序建模。因此,这些方
法不仅提高了听觉脑电图的整体性能,还改善了模型的跨被试泛化能力提高了鲁棒性和适应
性。
表 3.1 各模型合成语音信号的 PCC 得分
Table 3.1 PCC score of each model’s enhanced speech
评价指标 皮尔逊相关性(PCC)
验证集 测试集
VLAAI 0.0470 0.0214
DPRNN 0.0554 0.0368
ConvConcatNet 0.0622 0.0420
CAGWaveNet 0.0651 0.0432
3.4.2 消融实验对比与分析
在本节中,对所提出的基于交叉注意力机制听觉脑电信号语音转换算法进行了消融实验,
来评估各个注意力模块的必要性。设计了以下几种变体,分别为:消融实验 1,该方法仅使
用 WaveNet 模块来重建梅尔谱图,旨在探究多级粒度预测策略对模型性能的影响;消融实
验 2,该方法移除了最后两个解码块,以研究多级粒度预测策略中后两个更细粒度特征对模
型性能的作用;消融实验 3,该方法去除了 Mixup 数据增强技术,旨在评估数据增强操作对
模型性能的影响。
消融实验的结果如表 3.2 所示。可以观察到,相较于其他配置,所提出的模型仍然表现
出更优的性能。此外,模型的每个模块对整体性能均具有显著贡献。其中,提出的模型与消
融实验一对比,对于多级粒度预测策略,其促进作用为 0.0071,该方法在整个预测过程中缺
失了中间阶段的多尺度语音特征指导,导致模型只能从全局层面粗略拟合最终梅尔谱,缺乏
渐进式特征精炼过程,从而影响语音细节质量和还原精度。此外,WaveNet 虽然在单阶段建
模上表现良好,但在跨模态任务中缺乏多粒度监督会限制其泛化能力。提出的模型与消融实
验二对比,多了后两个更细粒度特征对模型引导促进皮尔逊相关性系数提升 0.002。在训练阶
段,相比于消融实验 2,所提出的模型具有更高的计算需求,但在测试阶段,两者的计算量
相同。这是因为最终的目标是重建十个子带梅尔谱图,因此在推理过程中不需要预测十个子
带维梅尔谱图之后更细粒度的语音特征。分析原因为后两个解码块主要捕捉梅尔谱中高频细


内蒙古大学硕士学位论文
26
节及上下文边界的微变特征,对于还原脑电中细微语义变化尤为重要。这些细粒度特征提升
了模型对语音动态变化的刻画能力,从而有效提高了梅尔频谱与目标语音的相关性。尽管这
些细粒度模块在训练阶段计算代价较高,但它们增强了模型对复杂脑电模式的适应性。提出
的模型与消融实验二对比,得到数据增强 Mixup 的促进作用为 0.0039,Mixup 策略通过在样
本间进行线性插值增强了脑电输入的表示多样性,有效缓解了数据稀疏与过拟合问题。对于
脑电信号这类高噪低维的生理数据,引入 Mixup 可帮助模型学到更稳健的语义映射函数,从
而提升了模型的泛化能力,尤其是在样本较少的神经解码任务中效果更为显著。上述消融实
验验证了所提出基于交叉注意力机制听觉脑电信号语音转换算法的有效性和合理性。
表 3.2 消融实验的 PCC 得分
Table 3.2 PCC for ablation experiments
评价指标 皮尔逊相关性(PCC)
验证集 测试集
消融实验 1 0.0580 0.0379
消融实验 2 0.0631 0.0415
消融实验 3 0.0612 0.0407
CAGWaveNet 0.0651 0.0432
3.5 本章小结
在本章中介绍了基于交叉注意力机制听觉脑电信号语音转换算法,该算法由 WaveNet 模
块、解码模块构成。通过 WaveNet 将脑电图信号解码为不同粒度的语音信号。多层注意力模
块则完成前一阶段解码的语音特征与脑电图信号的融合。为了降低学习难度,提升模型稳定
性,使用多级粒度预测策略,从低维到高维、从粗粒度到细粒度逐步优化预测目标,通过逐
级预测,使得每一级特征的预测都建立在前一级已经学习到的信息之上,从而让模型在更稳
定的基础上优化复杂的语音特征,提高语音合成的细节质量。由于脑电图信号数据集中被试
数量较少,并且不同被试之间的个体差异较大,直接训练的模型容易过拟合特定被试的数据
分布,导致泛化能力受限。为了提升模型的跨被试泛化能力,使其在未见过的被试上仍能保
持较好的解码效果,本文采用 Mixup 数据增强方法,通过在脑电图和语音特征层面进行加权
混合,生成更多样化的训练样本,从而增强数据的多样性。
此外,介绍了该实验所使用的数据集、参数、损失函数、基线系统、以及评价指标等。
并对所提出的基于交叉注意力机制听觉脑电信号语音转换算法做出了全面的评估。首先,将


内蒙古大学硕士学位论文
27
该模型基线系统进行了对比。本文中提出的模型比基线系统有着显著的提升。最后,进行了
消融实验,验证了多级粒度预测策略,Mixup 数据增强以及后两个更细粒度的八十个子带维
梅尔谱图和幅度谱引导的重要性。
综上所述,本章提出了基于交叉注意力机制听觉脑电信号语音转换算法,介绍了其组成
部分,实验设置,并进行了全面的实验结果评估和分析,验证了该方法的有效性。


内蒙古大学硕士学位论文
28
第四章 基于离散扩散模型说话脑电信号语音转换算法
4.1 引言
在上一章节中介绍了基于交叉注意力机制说话脑电信号语音转换算法,并分析了其实验
结果。本章将介绍基于离散扩散模型说话脑电信号语音转换算法。
尽管说话脑电图语音合成和听觉脑电图语音合成任务在形式上相似,二者仍然存在显著
区别,使得解码难度更高,相比听觉脑电图解码,说话脑电图数据更难获取,数据集规模更
小,且干扰信号更多,由于录制过程中涉及面部和咽喉运动,容易引入肌电和眼电噪声。此
外,大脑在主动说话和被动听觉感知时的神经活动模式存在显著差异,使得直接迁移听觉脑
电图语音合成任务的方法难以取得理想效果。为此,本研究提出了基于离散扩散模型说话脑
电信号语音转换算法。
本研究采用离散扩散模型,在离散隐空间中进行语音信号的合成。相较于直接生成波形
或频谱,该方法通过在低维离散空间中建模,有效简化了生成空间的复杂度,从而降低了任
务难度,并提升了语音合成的稳定性。此外,为了充分利用脑电图数据,本研究引入了脑电
图预训练模型 LaBram 来提取输入的脑电图特征。通过利用预训练模型的知识迁移能力,在
训练样本有限场景下,有效提升了模型的特征表达能力,从而增强脑电图语音解码任务的性
能和泛化能力。
4.2 网络介绍
本节将详细介绍基于离散扩散模型说话脑电信号语音转换算法。网络总体结构如图 4.1
所示,所提出的网络分别由三个子模块构成:语音编码器 WavTokenizer,脑电图预训练模型
LaBram,以及离散扩散模型 VQ-Diffusion。本章采用离散扩散策略,通过神经网络音频编码
器将音频信号压缩为离散 Token,避免了直接在连续空间建模带来的优化难度,同时充分利
用离散空间的表示优势,使得语音合成任务更加稳定,并提升了重建性能。
由于说话脑电图数据量较小,直接训练深度模型可能导致过拟合,为了更高效地利用脑
电图数据,本研究引入了脑电图预训练模型 LaBram 作为特征提取器,利用先验信息提升脑
电图语音映射关系的建模能力。LaBram 模块提取的特征在扩散模型中作为条件引导,优化离
散扩散过程中的特征生成。此外,为了进一步增强扩散过程的时序建模能力,本章在
VQ-Diffusion 结构中采用了 Mamba 模型进行长时序依赖建模,提高语音生成的连贯性和稳定
性。在下面的小节中,将分别介绍网络模块,实验设置,实验结果。


内蒙古大学硕士学位论文
29
图 4.1 基于离散扩散模型说话脑电信号语音转换算法结构图
Figure 4.1 Architecture of Discrete Diffusion-Based Spoken EEG-to-Speech Conversion Model
4.2.1 音频编码器 WavTokenizer
WavTokenizer 是一种高效的离散语音编码模型,专为音频语言建模设计。该模型的主要
目标是将高维音频信号压缩为低维离散 Token,同时保持较高的重建质量和丰富的语义信息,
与大多数 NAC 相似,其架构由三个主要组件组成:编码器(Encoder)、向量量化模块(Vector
Quantization, VQ)和解码器(Decoder)。WavTokenizer 是目前性能较好的单码本离散语音编
码模型。尽管 Token 数量减少,该模型仍能实现 SOTA 的音频重建质量,UTMOS 评分(与
人类主观评分高度相关)表现优异,且保持较丰富的语义信息。输入为时域语音信号,A ∈ AA ∙AA
(其中 A 为语音时长,AA为采样率),WavTokenizer 通过向量量化将其压缩为:
A ∈ {1, . . . , A}A ∙A𝠀 × A (4.1)
其中,A𝠀 为帧率(Frame Rate),D 为码本大小(Codebook Size)。
编码器的主要任务是将原始时域音频 A ∈ AA 转换为低维潜在表示 A ∈ AA' × A,其中 A' 是
下采样后的时间步长,A 是通道数。
首先,输入音频 A 经过多个卷积进行特征提取和下采样:


内蒙古大学硕士学位论文
30
A1 = Conv1D(X, A1, s) (4.2)
其中,A1 为卷积核,s 为步长(Stride),用于控制时间维度的缩减。然后,采用残差卷积块
进一步提取深层特征:
AA+1 = AA +ReLu(Conv1D( AA, A2)) (4.3)
其中,A2 为残差块的卷积核,ReLu 是激活函数,为模型引入非线性。此外,为了增强时间
建模能力,编码器在最后加入双向 LSTM:
A = BiLSTM(Z) (4.4)
在编码器提取的连续特征 A 之后,WavTokenizer 采用向量量化进行离散化。即将连续特
征映射到一个固定大小的离散空间,以便进行离散表示学习和高效音频建模。对于每个时间
步 A 的特征向量 AA ∈ AA,WavTokenizer 在码本中选择最接近的离散码向量 AA:
AA = 𤰀𝠀A A𝀀A
A∈A
∥ AA − A ∥2 (4.5)
其中,A = {A1, A2, . . . , AA} 是码本,包含 A 个离散向量 A𝀀 ∈ AA,𤰀𝠀A A𝀀A操作选择最接近输入
特征AA的码向量 A,通过将每个连续特征映射到离散索引 AA,模型在训练时能够学习更紧致
的离散表示。通过这种方式,WavTokenizer 将连续特征离散化为有限的 Token,从而减少信
息冗余,提高模型的生成稳定性。经过量化后,整个特征序列 A 被映射为离散 Token 序列:
A = {A1,A2, . . . AA} , AA ∈ {1,2, . . . , A} (4.6)
其中,每个 AA 表示音频在该时间步的离散编码,从而形成一个可用于语音合成或建模的紧凑
表示。
解码器的任务是将离散 Token A 转换回连续的音频波形 A。首先,解码器通过一维反卷
积(Transposed Convolution)恢复时间维度:
A' = ConvTranspose1D(Q, A3) (4.7)
然后,采用多头注意力机制(Multi-Head Self-Attention, MHSA)进行上下文信息建模:
A'' = MHSA(A') (4.8)
最后,应用逆短时快速傅立叶变换(ISTFT)还原时域波形:
A = A−1(A'') (4.9)
其中A−1代表逆傅立叶变换。


内蒙古大学硕士学位论文
31
4.2.2 脑电图预训练模型 LaBram
如图 4.2 所示,LaBraM(Large Brain Model)是一种大规模的脑电预训练模型,旨在学
习通用的脑电图信号表示。该模型采用自监督学习方法,在超过 2500 小时的多种类型脑电图
数据上进行预训练,以提升脑电图任务的泛化能力。LaBraM 通过将脑电图信号分割成脑电
图切片(EEG patches),并训练一个编码器,将连续脑电图信号转换为紧凑的离散 Token。
随后,该模型采用 Transformer encoder 结构,采用类似 BERT[56]的预训练方式,通过预测被
掩码的脑电图信号片段来进行自监督预训练,使其能够适应不同脑电图数据集的变换格式和
下游任务。
图 4.2 LaBram 网络结构图
Figure 4.2 Detailed structure of LaBram
在本研究中,脑电图信号通过 LaBraM 网络进行特征提取,以获得稳定且具有泛化能力
的脑电图信号表示。给定输入的脑电图信号信号序列 AAAA ∈ AA×A,其中 A 为时间步数,A 为
脑电图信号通道数,首先经过 LaBraM 的编码器 AAAA 提取特征表示:
AAAA = AAAA(AAAA) (4.10)
随后,LaBraM 采用向量量化(Vector Quantization, VQ)机制,将连续特征映射到离散空间
中的 EEG Token:


内蒙古大学硕士学位论文
32
AAAA = 𤰀𝠀A A𝀀A
A∈A
∥ AAAA − A ∥ (4.11)
其中,A 为预训练码本中的向量集合,AAAA 代表量化后的脑电图特征。最终,LaBraM 采用
Mamba 结构 AA𤰀A𣠀𤰀 进一步建模时序关系,获得最终的脑电图语义特征:
AAAA = AA𤰀A𣠀𤰀(AAAA) (4.12)
特征 AAAA 作为脑电图到语音转换模型的输入,提供更稳定的脑电图表示,提高语音生成的
质量和泛化能力。
通过这种方式,LaBraM 提供了丰富的脑电图表示能力,有助于在数据量较小的任务中提
高模型性能。LaBraM 的优势在于其跨数据集的学习能力,使得脑电图语音合成任务能够充分
利用大规模预训练的知识,提高在应用中的表现。
4.2.3 离散扩散模型
所提出的基于离散扩散模型说话脑电信号语音转换算法结合了 LaBram、Wavtokenizer
和离散扩散模型的优势。首先,通过 LaBram 提取脑电图特征作为生成的条件,然后通过
Wavtokenizer 将语音数据转换为离散 Token,这些 Token 随后由离散扩散模型进行处理。离散
扩散模型通过将去噪扩散概率模型中的高斯噪声加成操作替换为状态转换方法,使其更适用
于离散 Token,从而将去噪扩散概率模型框架扩展到离散数据建模。
尽管离散扩散模型处理的是离散数据,而去噪扩散概率模型处理的是连续数据,但二者
具有相似的结构,都包含一个前向过程和一个逆向过程。图 4.1 (b) 展示了离散扩散模型的
结构,其中说明了用于处理离散 Token 的状态转换方法所涉及的关键组件。
在前向过程中,离散扩散模型通过状态转换逐步对离散 Token A0 进行加噪,直到第 A
步,使其演变为一个高度破坏的状态 AA。在中间步骤 A 之后,数据的状态被表示为 AA 。
离散扩散模型采用状态转换矩阵(State Transition Matrix)来将 Token 转换为其他 Token
或掩码 Token,转换基于一定的概率进行。掩码 Token 是一个特殊定义的 Token,用于表示信
息的缺失。
每个 Token 具有三种转换概率,以概率 AA 变为掩码 Token;以概率 AAA 在所有 A 个类别
上均匀重采样;以概率 AA = 1 − AA − AAA 保持不变。
值得注意的是,掩码 Token 在整个过程中始终保持固定,不会再进一步变化。状态转换
矩阵AA ∈ A(A+1)×(A+1)定义如下:


内蒙古大学硕士学位论文
33
AA =
AA + AA
AA
⋮
AA
AA AA + AA ⋮
AA
AA
AA
⋮
AA
...0
...0 ⋱⋮ ...1
(4.13)
从状态 AA−1过渡到 xt的概率 A(AA|AA−1)由下式给出:
A(AA|AA−1) = A⊤(AA)AAA(AA−1) (4.14)
其中,A(A)是一个长度为 A 的 one-hot 列向量,在位置 A 处的元素为 1,其余为 0。类别分布 AA
由向量 AAA(AA−1) 确定。该过程描述了如何通过状态转换矩阵 AA 逐步对 Token 进行扰动,使
其从初始状态逐步演化为高度破坏的状态 AA ,从而适应离散数据建模的需求。
在逆向过程中,通过 Mamba-DPM 逆转扩散过程。Mamba-DPM 经过训练,使用 LaBram
提取的脑电图特征作为条件,恢复被破坏的 Token 序列,从而逆转扩散过程,从一个不包含
先验知识的序列恢复出语音序列。神经网络 AA(AA−1| AA, A) 被训练来估计后验状态转换分布
A(AA−1|AA, A0),其中 A 表示输入条件,也就时 LaBram 提取的脑电图特征。该扩散模型通过最
小化变分下界(Variational Lower Bound, VLB)作为其优化目标,以增强生成质量并提高训练
过程的稳定性。
在所提出的基于基于离散扩散模型说话脑电信号语音转换算法中,Mamba-DPM 作为关
键组件之一,被用于对去噪过程(Reverse Process)中的时序建模。该模块由多层 Mamba-DPM
堆叠而成,其核心思想是利用 LaBram 提取的特征作为条件,引导扩散过程的逆向去噪,以
提高重建质量。
在特征处理的第一步,该模块采用自适应层归一化(Adaptive Layer Normalization, AdaLN)
来结合不同时间步的特征 AA :
𧀀A𤰀AA( AA, A) = 𤰀A ∙ A𤰀AA𝠀A𦐀𝠀A(AA) + 𣠀A (4.15)
其中, 𤰀A 和 𣠀A 由时间步嵌入(Timestep Embedding)线性投影得到,从而对不同时间步的特
征进行归一化。
随后,经过归一化的特征 AA 和条件特征(脑电图提取的特征)在特征维度上拼接,并输
入 ConMamba 块进行处理。ConMamba 块由多个子层组成,包括:前馈层、双向 Mamba 层
(Bi-Mamba Layer)、卷积层、层归一化,这些组件协同工作,使得模型能够有效建模短期
和长期的时间依赖关系,并保持特征的完整性。同时,双向 Mamba 层进一步增强了模型对过
去和未来信息的捕捉能力,使得重建过程更加稳定。
Mamba 是一种基于状态空间模型的新型序列建模架构,旨在提升长序列处理中的效率与
建模能力。与传统的循环神经网络和基于注意力机制的 Transformer 模型相比,Mamba 通过


内蒙古大学硕士学位论文
34
引入选择性状态空间机制,在保持长距离依赖建模能力的同时,实现了线性时间复杂度的训
练与推理过程。Mamba 的关键特性在于,其以状态表征过去的序列信息,而非每个时间步都
重新关注整个序列,从而有效降低计算复杂度,同时仍然具备较强的全局建模能力。这种机
制不仅提高了推理效率,同时保证了良好的鲁棒性,使本课题提出的模型在语音重建和脑电
图语音合成任务中均表现出色。
4.2.4 损失函数
在所提出的基于离散扩散模型说话脑电信号语音转换算法中,模型的训练目标是最小化
变分下界,以确保扩散去噪过程的稳定性和最终生成结果的质量,该训练目标常用于生成模
型的训练。损失函数由三个主要部分组成,分别针对最终重建误差、去噪过程的匹配误差。
(1)最终重建误差
为了确保从扩散模型生成的最终结果 A0 逼近真实语音离散序列,模型最大化 A0 在给定
去噪预测结果 A1 和脑电图输入 A 下的概率:
A0 =− A𦐀A AA ( A0| A1, A) (4.16)
该项损失相当于最大化对数似然,使得模型能够在去噪序列的最后一步成功重建原始语音离
散序列。
(2)去噪匹配损失
在扩散逆向过程的每个时间步 A,模型需要学习如何从当前状态 AA预测前一状态 AA−1,
其目标是让模型学习真实的后验状态转换分布 A(AA−1|AA, A0):
AA−1 = AAA(A(AA−1|AA, A0) ∥ AA(AA−1| AA, A)) (4.17)
其中A(AA−1|AA, A0) 是基于真实数据的后验分布;AA(AA−1| AA, A) 是模型学习的近似分布;AAA
表示 Kullback-Leibler(KL)散度,用于衡量模型预测分布与真实分布的差异。
这一损失项确保了模型能够在每个时间步正确地逆转扩散过程,逐步去噪并恢复真实语
音离散序列。
最终,模型的总损失为上述两部分的和:
AAA𣠀 = A0 + A=1
A−1 AA (4.18)
其中,A0负责最终的语音重建,AA确保扩散去噪过程的正确性。


内蒙古大学硕士学位论文
35
4.3 实验设置
4.3.1 数据集介绍
在本章实验中,采用单人闭集数字说话脑电图数据集来评估提出的框架。该数据集采用
BioSemi Activte Two 进行录制,该设备的采样率为 1024,该设备共有 64 个电极片用于采取
头皮电信号,音频采样率为 16K,包含来自 1 位男性被试的 2200 条数字语音及同时录制的脑
电图信号。脑电图信号经过 ICA 去除掉眼电肌电的预处理,得到的脑电图信号作为训练和测
试的数据。其中训练集、验证集、测试集分别包含 1800、200 以及 200 条对应语音及脑电图。
图 4.3 ARSS 网络结构图[8]
Figure 4.3 Detailed structure of ARSS network[8]
4.3.2 参数设置
本实验采用 PyTorch 实现,使用 Adam[55]优化器优化网络参数,学习率为 0.001。如果模
型在验证集上损失函数连续两轮不下降,则学习率按 0.8 倍衰减,以提高训练稳定性。Batch
数量设置为 4。该网络训练共 200 个 Epoch。VQDiffusion 采用 10 层 Mamba-DPM 模块,每
个模块的维度设置为 512。此外,模型使用 LaBram 预训练模型来提取脑电图信号特征。在
音频离散化方面,模型采用 16 kHz 预训练 WavTokenizer,其码本大小 K 为 4096。对于
VQDiffusion 过程,设定时间步长 T=100。在前向扩散过程中,状态转换矩阵的参数设定如


内蒙古大学硕士学位论文
36
下:AA(转换为掩码 Token 的概率)线性增长,从 0 增加到 0.9。AA(均匀重采样的概率)线
性增长,从 0 增加到 0.1。
4.3.3 基线系统
为了验证所提出的基于离散扩散模型说话脑电信号语音转换算法的性能,采用以下几种
基线系统进行比较。ARSS[8],VCA[57],以及 DIFFWAVE[58]作为基线系统。下面将给出每个
基线系统的具体配置信息。
ARSS 模型采用了一种 GRU 和 Attention 组合的架构,用于根据脑电图中预测目标语音
的梅尔谱图,然后使用 Hifigan 合成语音。该架构直接预测目标梅尔谱图,该模型包括音频
GRU 层、Attention 层、和 Time Distributed Dense 层,该模型原文是预测梅尔频率倒谱系数,
本研究做了部分修改,使其预测梅尔谱图,最后通过声码器合成语音。ARSS 的网络结构如
图 4.3 所示,并基于原文的配置进行了模型复现。
图 4.4 VCA 网络结构图[57]
Figure 4.4 Detailed structure of VCA network[57]
VCA 提出了一个基于 Attention 的 GAN 网络,旨在通过给定嘴唇图像合成语音,本研
究将网络修改为脑电图语音合成。该网络包括生成器、判别器。生成器分为三个阶段分别预
测不同精度的梅尔谱图,然后使用交叉注意力机制完成两个模态数据的混合,确保了脑电图
数据不缺失,更好的融合两个模态数据,从而提高了语音合成的效果。此外,判别器由多个
卷积层和残差块组成,通过分别计算无条件和有条件的输出,判别器帮助模型在训练过程中
更好地学习语音和视觉之间的关联。该网络结构如图 4.4 所示,基于原文的配置进行了模型


内蒙古大学硕士学位论文
37
复现。
DiffWave 提出了一种基于 DDPM 的时域语音合成方法,旨在通过利用 Diffusion 的强大
生成能力完成语音合成,本研究将输入修改为了脑电图。该网络结构如图 4.5 所示,并基于
原文的配置进行了模型复现。
考虑到脑电图信号结构复杂,直接建模语音波形可能增加任务难度。为与提出模型在梅
尔谱域进行公平对比,本研究在原 DiffWave 框架基础上,将语音目标由波形形式替换为梅尔
谱图表示,并将该调整后模型作为基线系统,命名为 DiffWave+。
图 4.5 DiffWave 网络结构图[58]
Figure 4.5 Detailed structure of DiffWave network[58]
4.3.4 评价指标
为了有效的评估所提出的基于离散扩散模型说话脑电信号语音转换算法的性能,采用以
下几种指标,分别为:梅尔频谱倒谱失真率(Mel Cepstral Distortion,MCD)、梅尔谱相关
性(Mel-Correlation, Mel-Corr)以及准确率(Accuracy,ACC)。
MCD 是一种衡量梅尔频谱倒谱系数之间差异的指标。它常用于评估语音合成的质量,特
别是在与人类语音的对比中。MCD 计算的是生成语音与目标语音在梅尔频谱倒谱域上的距离,


内蒙古大学硕士学位论文
38
值越小表示生成语音的质量越好。其计算公式如下:
AAA = 1
A A=1
A
A=1
A ( A𦐀A AA,A − A𦐀A AA,A )2 (4.19)
其中,AA,A 和 AA,A 分别是目标语音和生成语音的梅尔频谱倒谱系数,A 是语音帧的总数,
A是倒谱系数的数量。
Mel-Corr 是一种衡量生成语音与目标语音之间的梅尔谱图相似度的指标。它用于评估语
音合成模型在频谱域上的保真度,特别是在语音内容匹配方面。相比于其他失真度度量
Mel-Corr 直接计算生成语音和目标语音在梅尔谱图上的相关性,其值越高,表示生成语音与
真实语音的频谱特性越接近。
Mel-Corr 的计算公式如下:
AAA − A𦐀𝠀𝠀 = A=1
A
A=1
A (AA,A − AA) (AA,A − AA)
A=1
A
A=1
A (AA,A − AA)2 ∙ A=1
A
A=1
A (AA,A − AA)2 (4.20)
其中,AA,A 和 AA,A 分别是目标语音和生成语音的梅尔频谱值。AA 和 AA 是目标语音和生成语音
在第 A 维上的均值。A𠐀为语音帧数,A 为梅尔频谱的维度。Mel-Corr 取值范围在 −1,1 之
间,值越接近 1 则表示生成语音的频谱结构与目标语音越一致,Mel-Corr 越高,语音合成的
质量越好。
ACC 是分类任务中常用的评价指标,用于衡量预测结果的准确度,ACC 的计算公式如下:
𧀀AA = 正确预测的数量
总预测的数量 × 100% (4.21)
4.4 实验结果分析
4.4.1 基线方法对比与分析
在本节中,将提出的基于离散扩散模型说话脑电信号语音转换算法(VQDES)与基线模
型进行了对比,并在自采数据集测试集上测试了其性能。分别以 MCD、LSD 和 ACC 作为评
价指标进行了比较,分别记录在表中。
表 4.1 展示了所提出的模型与基线系统的对比。其中,提出的采用离散语音 Token 语音
增强的方法,明显优于采用 Mel 特征的方法,如:VCA、DiffWave+以及 ARSS,展示了在空
间更加压缩的离散数据中完成语音生成任务的优越性。实验结果表明,所提出模型相较于基
于梅尔谱图的 DiffWave+,在 MCD、LSD 和 ACC 指标上分别取得了 102、0.03 和 5%的性能
提升;相较于时域实现的 DiffWave,提升幅度进一步扩大,分别达到了 166、0.11 和 6%。
此外,相比与基于 GAN 和直接预测的方式,三个使用 Diffusion 的模型均实现了更优的性能,


内蒙古大学硕士学位论文
39
这是因为扩散模型通过多次迭代不断修正推理结果,通过显式的学习分布转换能力为模型提
供了更强的生成能力。
所提出的基于离散扩散模型说话脑电信号语音转换算法在所有指标上均超越基线系统。
该模型的优异性能主要来源于以下两点:(1)离散数据空间的有效建模:相较于连续数据建
模,离散表示空间更具结构化特性,减少了数据分布的复杂性,从而降低了模型学习的难度,
提高了训练稳定性。通过将语音信号映射到离散 Token,模型能够更高效地学习脑电图与语
音之间的映射关系,避免了连续空间中优化的不稳定性。(2)LaBraM 预训练模型的高效特
征利用:LaBraM 作为大规模脑电图预训练模型,能够从有限的脑电图训练数据中提取更具
泛化性的高层语义特征,从而缓解脑电图数据量较小带来的模型学习瓶颈。该预训练模型利
用自监督学习方法构建脑电图表示,使得解码网络能够更充分地利用数据。综上所述,该实
验充分证明了所提出的网络在说话脑电图解码方面有着更加优异的性能。
表 4.1 说话脑电图语音合成模型性能比较
Table 4.1 Performance comparison of spoken EEG to speech models
模型 语音特征 MCD LSD ACC(%)
DiffWave 时域语音 454 0.70 70
ARSS 梅尔谱图 498 0.65 65
VCA 梅尔谱图 469 0.75 70
DiffWave+ 梅尔谱图 390 0.78 71
VQDES 语音离散 Token 288 0.81 76
表 4.2 VQDES 消融实验性能比较
Table4.2 Ablation Experiment Performance Comparison of VQDES
模型 MCD Mel-Corr ACC(%)
消融实验 1 350 0.73 70
消融实验 2 512 0.54 40
消融实验 3 295 0.79 74
VQDES 288 0.81 76
4.4.2 消融实验对比与分析
在本节中,对提出的基于离散扩散模型说话脑电信号语音转换算法进行了消融实验,来


内蒙古大学硕士学位论文
40
评估各个模块的必要性。采用 MCD、Mel-Corr 以及 ACC 作为评估指标,实验结果如表 4.2
所示。
根据表 4.2,其中消融实验 1 去除了预训练模型 LaBram,该方法会导致 MCD 下降 62,
Mel-Corr 下降 0.08,ACC 下降 6%,这表明 LaBram 的引入显著增强了脑电信号的表征能力。
本质上,LaBram 通过跨模态预训练建立了语音与脑电之间的对齐映射,使模型能更好地捕捉
语义相关的神经特征,从而提升了生成语音的内容一致性和可懂度。消融实验 2 将离散扩散
去掉,使用直接预测的方式,该方法对模型性能影响更为显著,MCD 下降 224,Mel-Corr 下
降 0.27,ACC 下降 36%。这说明扩散过程在建模复杂映射空间中具有显著优势。扩散模型通
过引入多步反演过程逐步精细化语音重建,相比直接回归更能避免过拟合与语音模糊现象,
提升了生成质量与稳定性。尤其对于高维模态间映射(如脑电至语音),离散扩散机制可有
效分解任务复杂性,降低学习难度。消融实验 3 去掉扩散模型中双向 Mamba,将其替换为单
向 Mamba,该方法会导致模型合成结果的 MCD 下降 7,Mel-Corr 下降 0.02,ACC 下降 2%。
尽管性能下降不如前两项显著,但仍体现出双向 Mamba 在解码阶段的建模优势。由于脑电信
号存在前后文耦合与语义拖延现象,单向建模难以捕捉完整上下文,而双向 Mamba 可同时感
知历史与未来状态,增强了特征的时序一致性与预测准确度。以上的三个消融实验结果表明,
引入 LaBraM 预训练模型有助于更有效地提取脑电图特征,提高模型对脑电信号的表征能力。
同时,VQDiffusion 提供了更强的生成能力,确保了语音合成的稳定性。此外,双向 Mamba
结构使得模型能够在时序建模中捕获更全面的全局特征,从而进一步提升解码性能。这些结
果验证了所提出模型在脑电图语音解码任务中的有效性,并进一步证明了不同模块在优化脑
电图到语音转换中的重要作用。
4.5 本章小结
在本章中介绍了基于离散扩散模型说话脑电信号语音转换算法,该网络由音频编码器
Wavtokenizer,脑电图预训练模型 LaBram,以及离散扩散模型成。通过 Wavtokenizer 将音频
转为离散 Token,将数据压缩到一个离散紧致的低维空间,有效简化了任务难度。此外,通
过 LaBram 预训练模型引入先验知识,更好的提取脑电图信号中得到特征,最后通过离散扩
散模型进行生成得到的语音,用到了扩散模型的强大生成能力。
此外,介绍了该实验所使用的数据集、参数、损失函数、基线系统、以及评价指标等。
并对所提出的基于离散扩散模型说话脑电信号语音转换算法做出了全面的评估。首先,将该
模型与基于基线系统进行了对比。其次,进行了消融实验,通过去除 LaBram、离散扩散、双


内蒙古大学硕士学位论文
41
向 Mamba 验证了该方法的有效性。
综上所述,本章提出基于离散扩散模型说话脑电信号语音转换算法,介绍了其组成部分,
实验设置,并进行了全面的实验结果评估和分析,验证了该方法的有效性。


内蒙古大学硕士学位论文
42
第五章 总结与展望
5.1 工作总结
在这项研究中,提出了基于交叉注意力机制听觉脑电信号语音转换算法和基于离散扩散
模型说话脑电信号语音转换算法,在脑电图转换语音的两个分支分别做出了贡献。具体而言,
两个分支都有效提高了脑电图转换语音算法的性能。其中基于交叉注意力机制听觉脑电信号
语音转换算法更关注提高模型的可解释性从而提高模型性能,此外还采用了使用扩张卷积结
构的 WaveNet 模型对时序信号进行建模,通过多级粒度更好的引导模型的学习,通过 Mixup
方法有效的提升了脑电图数据的多样性,从而达到了当前最优秀的听觉脑电图解码性能。而
基于离散扩散模型说话脑电信号语音转换算法更关注提高模型生成能力和使用预训练模型的
先验知识,此外通过使用神经网络编码器将语音信号压缩至离散空间有效的提升了模型的性
能。这两项工作都有效提高了模型的性能,推动了电图转换语音算法的发展。
本研究提出的两个脑电图语音合成算法,分别在 2024 Auditory EEG Challenge 数据集上
和单人数字说话脑电图数据上进行训练,并在相应测试集上超过了现有模型。在实验方面,
分别进行了和基线系统的对比与分析,证明了该模型的先进性与优越性。提出的模型相比于
基线系统都得到了更好的效果,基于交叉注意力机制听觉脑电信号语音转换算法在 PCC 上超
过了最优的基线系统,基于离散扩散模型说话脑电信号语音转换算法分别在 MCD、Mel-Corr
和 ACC 上超过了最优的基线系统。其次,进行了消融实验,分别对基于交叉注意力机制听觉
脑电信号语音转换算法提出的多级粒度策略、Mixup 数据增强等各个模块进行了评估,以及
对基于离散扩散模型说话脑电信号语音转换算法的 LaBram 预训练模型以及离散扩散模型等
各个模块进行了评估,验证了它们的有效性并分析了各自的贡献。其中,多级粒度策略、Mixup
数据增强和 LaBram 预训练模型、离散扩散模型这些方法的使用都对更高效使用现有脑电数
据提供帮助,而且为性能提升做出了较大贡献。
5.2 未来展望
本文围绕脑电图到语音转换算法展开研究,尽管取得了一些成果,但是仍然存在一些问
题需要进一步研究,可以从以下几个方面展开未来工作:
(1)所提出的基于交叉注意力机制听觉脑电信号语音转换算法使用扩张卷积对脑电信号
建模,扩张卷积的方式虽然有效,但是在近期大语言模型潮流中如何能使用 Transformer 结构
模型对脑电更高效学习是一个很重要的研究。但是目前的脑电图转换语音数据量来说还具有


内蒙古大学硕士学位论文
43
很大的挑战。在未来的研究中,需要进行更多尝试进行数据增强,扩充数据集,是一项很重
要的任务。
(2)基于离散扩散模型说话脑电信号语音转换算法使用的离散扩散方式,需要迭代一百
次,从一个不包含先验知识的纯 Mask 序列不断去噪,最终预测出语音,计算复杂度较高,
当前已经有研究提出了离散流匹配的方式[59],可以加速离散扩散模型的推理速度,在后续的
工作中本人将继续尝试这一方式,提高模型的效率。
(3)目前在脑电图到语音转换中,本研究尝试的方法中 Mixup 的数据增强虽然是最有
效的,可以有效提升模型的跨被试泛化能力,提高数据多样性,但是在使用 Mixup 后模型的
训练轮数需要提高,增加了模型的训练成本,将考虑在未来的工作研究如何在提高数据,后模
型训练效率探索更优的混合策略。
(4)目前,在预训练模型的选择上本研究使用的 LaBram 是一个通用的脑电图表示的预
训练模型,为本研究的方案带来了很大的提升,但是本研究是一个脑电图到语音转换任务,
在脑电图中有特定频率和语音是关联更加密切的,在后续的研究中,考虑训练一个脑电图语
音转换特定任务上的脑电图表示预训练模型来提高模型的性能。


内蒙古大学硕士学位论文
44
参考文献
[1] Vaid S, Singh P, Kaur C. EEG signal analysis for BCI interface: A review[C]//2015 fifth
international conference on advanced computing & communication technologies. IEEE, 2015:
143-147.
[2] D ’ Zmura M, Deng S, Lappas T, et al. Toward EEG sensing of imagined speech[C]//
Human-Computer Interaction. New Trends: 13th International Conference, HCI International
2009, San Diego, CA, USA, July 19-24, 2009, Proceedings, Part I 13. Springer Berlin
Heidelberg, 2009: 40-48.
[3] DaSalla C S, Kambara H, Sato M, et al. Single-trial classification of vowel speech imagery
using common spatial patterns[J]. Neural networks, 2009, 22(9): 1334-1339.
[4] Panachakel J T, Ramakrishnan A G. Decoding covert speech from EEG-a comprehensive
review[J]. Frontiers in Neuroscience, 2021, 15: 392.
[5] Piao Z, Kim M, Yoon H, et al. Happyquokka System For Icassp 2023 Auditory Eeg
Challenge[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2023: 1-2.
[6] Van Dyck B, Yang L, Van Hulle M M. Decoding Auditory EEG Responses Using an Adapted
Wavenet[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2023: 1-2.
[7] Puffay C, Accou B, Bollens L, et al. Relating EEG to continuous speech using deep neural
networks: a review[J]. arXiv preprint arXiv:2302.01736, 2023.
[8] Krishna G, Tran C, Han Y, et al. Speech synthesis using EEG[C]//ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020:
1235-1238.
[9] Lee Y E, Kim S H, Lee S H, et al. Speech Synthesis from Brain Signals Based on Generative
Model[C]//2023 11th International Winter Conference on Brain-Computer Interface (BCI).
IEEE, 2023: 1-4.
[10]Lee Y E, Lee S H. EEG-transformer: Self-attention from transformer architecture for decoding
EEG of imagined speech[C]//2022 10th International winter conference on brain-computer
interface (BCI). IEEE, 2022: 1-4.


内蒙古大学硕士学位论文
45
[11] Lee Y E, Lee S H, Kim S H, et al. Towards voice reconstruction from EEG during imagined
speech[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(5):
6030-6038.
[12]Kim S, Lee Y E, Lee S H, et al. Diff-E: Diffusion-based learning for decoding imagined speech
EEG[J]. arXiv preprint arXiv:2307.14389, 2023.
[13]Krishna G, Tran C, Carnahan M, et al. Generating EEG features from acoustic
features[C]//2020 28th European Signal Processing Conference (EUSIPCO). IEEE, 2021:
1100-1104.
[14]Thornton M, Mandic D, Reichenbach T. Robust decoding of the speech envelope from EEG
recordings through deep neural networks[J]. Journal of neural engineering, 2022, 19(4):
046007.
[15]Krishna G, Tran C, Carnahan M, et al. Advancing speech synthesis using EEG[C]//2021 10th
International IEEE/EMBS Conference on Neural Engineering (NER). IEEE, 2021: 199-204.
[16]Sakthi M, Tewfik A, Chandrasekaran B. Native language and stimuli signal prediction from
eeg[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP). IEEE, 2019: 3902-3906.
[17]Duda R O, Hart P E, Stork D G. Chapter 8 pattern classification 2nd edition[J]. A
Wiley-Interscience Publication, 2001.
[18]Accou B, Vanthornhout J, hamme H V, et al. Decoding of the speech envelope from EEG using
the VLAAI deep neural network[J]. Scientific Reports, 2023, 13(1): 812.
[19]Katthi J R, Ganapathy S. Deep correlation analysis for audio-EEG decoding[J]. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 2021, 29: 2742-2753.
[20]Fu Z, Wang B, Wu X, et al. Auditory attention decoding from eeg using convolutional recurrent
neural network[C]//2021 29th European Signal Processing Conference (EUSIPCO). IEEE,
2021: 970-974.
[21]Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural
information processing systems, 2017, 30.
[22]Xu X, Wang B, Yan Y, et al. ConvConcatNet: a deep convolutional neural network to
reconstruct mel spectrogram from the EEG[C]//2024 IEEE International Conference on


内蒙古大学硕士学位论文
46
Acoustics, Speech, and Signal Processing Workshops (ICASSPW). IEEE, 2024: 113-114.
[23]Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[J]. Advances in
neural information processing systems, 2014, 27.
[24]Sohl-Dickstein J, Weiss E, Maheswaranathan N, et al. Deep unsupervised learning using
nonequilibrium thermodynamics[C]//International conference on machine learning. pmlr, 2015:
2256-2265.
[25]Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural
information processing systems, 2020, 33: 6840-6851.
[26]Song Y, Sohl-Dickstein J, Kingma D P, et al. Score-based generative modeling through
stochastic differential equations[J]. arXiv preprint arXiv:2011.13456, 2020.
[27]Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint
arXiv:2010.02502, 2020.
[28]Qi D, Kong L, Yang L, et al. AudioDiffusion: Generating High-Quality Audios from EEG
Signals: Reconstructing Audio from EEG Signals[C]//2023 4th International Symposium on
Computer Engineering and Intelligent Communications (ISCEIC). IEEE, 2023: 344-348
[29]Oord A, Dieleman S, Zen H, et al. Wavenet: A generative model for raw audio[J]. arXiv
preprint arXiv:1609.03499, 2016.
[30]Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond empirical risk minimization[J]. arXiv
preprint arXiv:1710.09412, 2017.
[31]Gu A, Goel K, Ré C. Efficiently modeling long sequences with structured state spaces[J]. arXiv
preprint arXiv:2111.00396, 2021.
[32]Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces[J]. arXiv
preprint arXiv:2312.00752, 2023.
[33]Dao T, Gu A. Transformers are ssms: Generalized models and efficient algorithms through
structured state space duality[J]. arXiv preprint arXiv:2405.21060, 2024.
[34]Jiang W B, Zhao L M, Lu B L. Large brain model for learning generic representations with
tremendous EEG data in BCI[J]. arXiv preprint arXiv:2405.18765, 2024.
[35]Van Den Oord A, Vinyals O. Neural discrete representation learning[J]. Advances in neural
information processing systems, 2017, 30.


内蒙古大学硕士学位论文
47
[36]Gu S, Chen D, Bao J, et al. Vector quantized diffusion model for text-to-image
synthesis[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 2022: 10696-10706.
[37]Tang Z, Gu S, Bao J, et al. Improved vector quantized diffusion models[J]. arXiv preprint
arXiv:2205.16007, 2022.
[38]Austin J, Johnson D D, Ho J, et al. Structured denoising diffusion models in discrete
state-spaces[J]. Advances in neural information processing systems, 2021, 34: 17981-17993.
[39]Ji S, Jiang Z, Wang W, et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for
audio language modeling[J]. arXiv preprint arXiv:2408.16532, 2024.
[40]Défossez A, Copet J, Synnaeve G, et al. High fidelity neural audio compression[J]. arXiv
preprint arXiv:2210.13438, 2022.
[41]Yang D, Liu S, Huang R, et al. Hifi-codec: Group-residual vector quantization for high fidelity
audio codec[J]. arXiv preprint arXiv:2305.02765, 2023.
[42]Delorme A, Makeig S. EEGLAB: an open source toolbox for analysis of single-trial EEG
dynamics including independent component analysis[J]. Journal of neuroscience methods, 2004,
134(1): 9-21.
[43]Makeig S, Bell A, Jung T P, et al. Independent component analysis of electroencephalographic
data[J]. Advances in neural information processing systems, 1995, 8.
[44]Jung T P, Makeig S, Humphries C, et al. Removing electroencephalographic artifacts by blind
source separation[J]. Psychophysiology, 2000, 37(2): 163-178.
[45]Roy Y, Banville H, Albuquerque I, et al. Deep learning-based electroencephalography analysis:
a systematic review[J]. Journal of neural engineering, 2019, 16(5): 051001.
[46]Islam M K, Rastegarnia A, Yang Z. Methods for artifact detection and removal from scalp EEG:
A review[J]. Neurophysiologie Clinique/Clinical Neurophysiology, 2016, 46(4-5): 287-305.
[47]Synigal S R, Teoh E S, Lalor E C. Including measures of high gamma power can improve the
decoding of natural speech from EEG[J]. Frontiers in human neuroscience, 2020, 14: 130.
[48]Zhao S, Rudzicz F. Classifying phonological categories in imagined and articulated
speech[C]//2015 IEEE international conference on acoustics, speech and signal processing
(ICASSP). IEEE, 2015: 992-996.


内蒙古大学硕士学位论文
48
[49]Cooney C, Folli R, Coyle D. Mel frequency cepstral coefficients enhance imagined speech
decoding accuracy from EEG[C]//2018 29th Irish Signals and Systems Conference (ISSC).
IEEE, 2018: 1-7.
[50]Saha P K, Rahman M A, Mollah M N. Frequency domain approach in CSP based feature
extraction for EEG signal classification[C]//2019 International Conference on Electrical,
Computer and Communication Engineering (ECCE). IEEE, 2019: 1-6.
[51]Prenger R, Valle R, Catanzaro B. Waveglow: A flow-based generative network for speech
synthesis[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2019: 3617-3621.
[52]Kong J, Kim J, Bae J. Hifi-gan: Generative adversarial networks for efficient and high fidelity
speech synthesis[J]. Advances in neural information processing systems, 2020, 33:
17022-17033.
[53]Bollens L, Puffay C, Accou B, et al. Auditory EEG decoding challenge for ICASSP 2024[J].
IEEE Open Journal of Signal Processing, 2025.
[54]Luo Y, Chen Z, Yoshioka T. Dual-path rnn: efficient long sequence modeling for time-domain
single-channel speech separation[C]//ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 46-50.
[55]Kingma D P, Ba J. Adam: A method for stochastic optimization[J]. arXiv preprint
arXiv:1412.6980, 2014.
[56]Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for
language understanding[C]//Proceedings of the 2019 conference of the North American chapter
of the association for computational linguistics: human language technologies, volume 1 (long
and short papers). 2019: 4171-4186.
[57]Kim M, Hong J, Ro Y M. Lip to speech synthesis with visual context attentional GAN[J].
Advances in Neural Information Processing Systems, 2021, 34: 2758-2770.
[58]Kong Z, Ping W, Huang J, et al. Diffwave: A versatile diffusion model for audio synthesis[J].
arXiv preprint arXiv:2009.09761, 2020.
[59]Gat I, Remez T, Shaul N, et al. Discrete flow matching[J]. Advances in Neural Information
Processing Systems, 2024, 37: 133345-133385.