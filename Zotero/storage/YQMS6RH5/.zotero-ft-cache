290 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
Semi-Supervised Dual-Stream Self-Attentive
Adversarial Graph Contrastive Learning for
Cross-Subject EEG-Based Emotion Recognition
Weishan Ye , Zhiguo Zhang , Fei Teng, Min Zhang, Jianhong Wang, Dong Ni , Fali Li , Peng Xu , and Zhen Liang
Abstract—Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in crosssubject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream
Manuscript received 18 October 2023; revised 11 July 2024; accepted 12 July 2024. Date of publication 25 July 2024; date of current version 7 March 2025. This work was supported in part by the National Natural Science Foundation of China under Grant 62276169, Grant 62071310, and Grant 82272114, in part by the Medical-Engineering Interdisciplinary Research Foundation of Shenzhen University under Grant 2024YG008, in part by Shenzhen University-Lingnan University Joint Research Programme, in part by the Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions under Grant 2022SHIBS0003, in part by the Shenzhen Science and Technology Research and Development Fund for Sustainable Development Project under Grant KCXFZ20201221173613036. Recommended for acceptance by D. Zhang. (Weishan Ye and Zhiguo Zhang contributed equally to this work.) (Corresponding authors: Peng Xu; Zhen Liang.)
This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by Shanghai Jiao Tong University (SEED, SEED-IV, and SEED-V) and Tsinghua University (FACED). Weishan Ye, Fei Teng, Dong Ni, and Zhen Liang are with the School of Biomedical Engineering, Medical School, Shenzhen University, Shenzhen 518060, China, and also with the Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging, Shenzhen 518060, China (e-mail: 2110246024@email.szu.edu.cn; 2021222013@email.szu.edu.cn; nidong@szu.edu.cn; janezliang@szu.edu.cn). Zhiguo Zhang is with the Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen 518000, China, also with the Marshall Laboratory of Biomedical Engineering, Shenzhen 518060, China, and also with Peng Cheng Laboratory, Shenzhen 518055, China (e-mail: zhiguozhang@hit.edu.cn). Min Zhang is with the Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen 518000, China (e-mail: zhangmin2021@hit.edu.cn). Jianhong Wang is with Shenzhen Mental Health Center, Shenzhen Kangning Hospital, Shenzhen 518020, China (e-mail: wangjianhong0755@163.com). Fali Li and Peng Xu are with the Clinical Hospital of Chengdu Brain Science Institute, MOE Key Lab for Neuroinformation, University of Electronic Science and Technology of China, Chengdu 611731, China, and also with the School of Life Science and Technology, Center for Information in Medicine, University of Electronic Science and Technology of China, Chengdu 611731, China (e-mail: fali.li@uestc.edu.cn; xupeng@uestc.edu.cn). The source code is available at https://github.com/Vesan-yws/DS-AGC. Digital Object Identifier 10.1109/TAFFC.2024.3433470
develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments are conducted on four benchmark databases (SEED, SEED-IV, SEED-V, and FACED) using a semi-supervised cross-subject leave-one-subject-out crossvalidation evaluation protocol. The results show that the proposed model outperforms existing methods under different incomplete label conditions with an average improvement of 2.17%, which demonstrates its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.
Index Terms—EEG, emotion recognition, graph contrastive learning, domain adaption, semi-supervised learning.
I. INTRODUCTION
T
HE field of affective computing is experiencing rapid growth in Electroencephalography (EEG)-based emotion recognition. However, the predominant focus in current research lies on supervised learning approaches that heavily rely on high-quality labeled data for model training. This process can be labor-intensive, expensive, and complicated to acquire. In contrast, a vast amount of readily available unlabeled data presents an opportunity for semi-supervised learning (SSL), where a small amount of labeled data can be combined with a large amount of unlabeled data to construct models. The generalization capacity could be enhanced, while the burden of extensive labeling efforts could be reduced [1]. Currently, one of the critical challenges in semi-supervised EEG-based emotion recognition is to develop algorithms that can efficiently utilize both labeled and unlabeled data to improve model learning. For example, Jia et al. [2] developed a generative restricted Boltzmann machine (RBM) model and incorporated a regularization of the supervised training process with unlabeled data for model variance reduction. In the model, the unsupervised information was adopted for initial feature selection and treated as learning constraints. In the experiments, the data was randomly divided into two groups: a supervised group and an unsupervised group. This division may result in both groups containing samples from the same subject, potentially affecting the generalization of the model. In a more recent study, Zhang et al. [3] experimented with a variety of SSL methods on
1949-3045 © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 291
two public emotion EEG databases, demonstrating that effective utilization of unlabeled data can lead to improved model performance. Specifically, they explored three state-of-the-art methods (MixMatch [4], FixMatch [5], and AdaMatch [6]) as well as five classical semi-supervised methods. Their results underscored the importance of leveraging unlabeled data to improve the performance of EEG-based emotion recognition models. However, their evaluation protocol focused exclusively on within-subject assessments. Zhang et al. [7] introduced a domain adaptation method called PARSE that combined domain adversarial neural networks (DANN) and pairwise representation alignment. The results showed that PARSE achieved similar performance to fully-supervised models with substantially fewer labeled samples. Nevertheless, PARSE constrains its utilization to solely labeled data from the source domain for model training, without taking into account the unlabeled data present in the source domain. The previous semi-supervised EEG-based emotion recognition methods have exhibited limitations in their ability to construct robust domain classifiers, which failed to distinguish among the labeled source domain, the unlabeled source domain, and the unknown target domain. This deficiency has led to suboptimal adaptation to the unlabeled source domain, especially in scenarios where the source data predominantly consists of unlabeled samples. Consequently, these methods have not achieved the desired level of performance in such challenging situations. Additionally, these methods extracted features from isolated EEG channels, which disregard the complex feature representation among different EEG channels. This limitation results in a lack of rich structural information representation in modeling. To address the limitations in the literature, this paper proposes a novel semi-supervised Dual-stream Self-attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) for cross-subject EEG-based emotion recognition. A dual-stream SSL framework is designed, comprising a non-structural stream and a structural stream. Within this framework, two types of feature representations are generated, ensuring the preservation of both non-structural and structural feature information. In the non-structural stream, differential entropy (DE) features are extracted from labeled source data, unlabeled source data, and unknown target data, and a semi-supervised multi-domain adaptation method is developed to jointly minimize distribution discrepancy among the three domains. This approach helps to address the limitations of previous methods by leveraging unlabeled source data to improve model performance. In the structural stream, a graph contrastive learning method is proposed to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. This approach can capture the complex relationships between features across different channels and address the limitation of previous methods in overlooking structural information. Finally, to efficiently and effectively fuse the dual-stream feature representation in a delicate and intelligent manner, a self-attentive fusion module is introduced, leveraging the strengths of both streams and highlighting the EEG features and samples that are more discriminative for emotions. Overall, the proposed DS-AGC can
overcome the limitations of previous methods in adapting to the unlabeled source domain and capturing rich non-structural and structural information. The main contributions of this study can be summarized as follows. We propose a novel semi-supervised learning framework designed to efficiently leverage a limited set of labeled data in conjunction with a large quantity of unlabeled data, ultimately enhancing the performance of semi-supervised cross-subject EEG emotion recognition. A dual-stream architecture is designed to capture and extract a comprehensive array of EEG features, including both non-structural and structural elements. Both streams are intelligently united through a self-attentive module, ensuring the preservation of the intricate blend of nonstructural and structural information inherent within EEG signals. Extensive experiments are conducted on benchmark databases, covering various data scarcity conditions. The effectiveness and reliability of the model’s performance are validated in a cross-subject scenario, while thorough analyses of model components and parameters, along with feature visualization, are performed to enhance our understanding of the results.
II. RELATED WORK
A. EEG-Based Emotion Recognition
In recent years, there has been an increasing interest in the potential of EEG signals to identify emotional states of individuals [8], [9], [10], [11], [12], [13], [14], [15]. These studies have yielded valuable insights and paved the way for the development of affective brain-computer interface (aBCI) systems. Among the early contributions to this field, Duan et al. extracted emotion-related EEG features and employed support vector machines (SVM) for emotion classification[8]. Since then, a number of studies based on various machine learning and deep learning methods have been developed to enhance the accuracy and usability of aBCI systems. The recent advancements in deep learning have led to a surge of research on EEG-based emotion recognition using various neural network architectures. For example, Zhang et al. proposed a cascaded and parallel convolutional recursive neural network that can effectively learn discriminative EEG features [12]. Song et al. introduced a dynamic graph convolutional neural network to dynamically capture intrinsic relationships among different EEG channels [16]. Niu et al. developed a novel deep residual neural network by combining brain network analysis and channel-spatial attention mechanism [17]. The above-mentioned models were robust in within-subject emotion recognition tasks, where the training and testing data come from the same subject. However, due to individual differences in EEG signals collected from different subjects, the model performance would significantly decrease in cross-subject emotion recognition tasks, when the training and testing data come from different subjects [18], [19], [20].
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


292 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
B. Cross-Subject EEG-Based Emotion Recognition
To improve the generalizability of cross-subject EEG-based emotion recognition models, transfer learning is employed to mitigate individual differences by harmonizing the feature distribution across different subjects [18], [21], [22], [23], [24]. Transfer learning methods, such as transfer component analysis (TCA) and transductive parameter transfer (TPT), have been incorporated in Zheng et al.’s work [18] to improve cross-subject emotion recognition performance from 56.73% to 76.31%, demonstrating the effectiveness of transfer learning in improving the generalizability of EEG-based emotion recognition models in the presence of individual differences among subjects. In recent years, deep transfer learning methods have been proposed to further improve the model performance in crosssubject EEG-based emotion recognition tasks. In 2017, Jin et al. [24] introduced a deep transfer learning framework with DANN [25] to further improve the emotion recognition accuracy from 76.31% (non-deep transfer learning) to 79.19% (deep transfer learning). Based on Jin et al.’s work, Li et al. proposed a domain adaptive method to minimize distribution shift and generalize the emotion recognition model to different individuals [26]. He et al. combined adversarial discriminators and time convolutional networks (TCNs) to further enhance distribution matching in EEG-based emotion recognition tasks [23]. To preserve the neural information during feature alignment, Huang et al. developed a generator-based ignorant mechanism domain adaptation model [22]. Xu et al. proposed a domain adversarial graph attention model (DAGAM) that utilized graph attention adversarial training and biological topology information [21]. Additionally, Peng et al. proposed a joint feature adaptation and graph-adaptive label propagation method (JAGP) [27]. Compared to the original DANN structure, these enhanced deep transfer learning frameworks have achieved better performance on the cross-subject EEG-based emotion recognition tasks. Recent deep transfer learning-based emotion recognition models have shown promising results in handling individual differences. Nonetheless, these models often necessitate a significant volume of labeled source data to establish consistent performance. Acquiring a sufficient amount of high-quality labeled EEG data is a challenging and time-consuming task in aBCI systems. In light of this challenge, the development of a novel SSL framework for cross-subject EEG-based emotion recognition becomes important. Such a framework should effectively leverage a small quantity of labeled source data and a large amount of unlabeled source data. This represents a crucial research direction with the potential to mitigate the constraints imposed by the limited availability of labeled data in EEG-based emotion recognition.
C. Contrastive Learning
Contrastive learning offers a promising alternative for feature learning, as it enables the extraction of meaningful representations directly from data without the need for manual annotation. This approach has found widespread application in various fields, including computer vision [28], natural language processing (NLP) [29], and bioinformatics [30], [31]. In recent
years, researchers have also started exploring the potential of contrastive learning for EEG-based emotion recognition. For example, Mohsenvand et al. [32] utilized the SimCLR framework [28] to learn the similarity between augmented EEG samples from the same input, thereby enhancing the model’s ability to capture distinctive features. Similarly, Shen et al. [33] proposed the CLISA model, which maximized feature representation similarity across subjects experiencing the same emotional stimuli, leading to improved emotion recognition performance. These studies highlight the efficacy of contrastive learning in the context of EEG-based emotion recognition. However, there are still challenges that need to be addressed in contrastive learning based EEG modeling, particularly with regard to effectively integrating structural information from EEG signals into the contrastive learning framework. Exploring the structural information present in EEG signals is vital for capturing the complex connectivity patterns and topographical relationships within the brain. EEG signals exhibit natural spatial and temporal dependencies among different electrodes and brain regions, which can provide valuable insights into brain dynamics and emotional processes. Graph contrastive learning (GCL) has emerged as a powerful technique that leverages structured information in data, expanding the capabilities of contrastive learning to incorporate rich contextual information in graph data [34]. Integrating GCL into EEG analysis holds the potential for facilitating a more comprehensive analysis of brain function and its implications, specifically by considering the interplay between different EEG channels within the complex and dynamic realm of emotional processing. The intricate connectivity patterns within the brain could be uncovered and the understanding of how different channels interact and contribute to the overall emotional experience could be deepened.
III. METHODOLOGY
An overview of the proposed DS-AGC is shown in Fig. 1, which consists of three main parts. (1) Non-Structural Stream: extracting non-structural EEG features from the labeled source S, unlabeled source U , and unknown target T domains. Based on the extracted DE features at five different frequency bands, a non-structural feature extractor FNS(·) is defined to extract the features from each domain. A gradient reversal layer is used to reverse the gradients during the feature extraction process, allowing the model to learn domain-invariant features and ensuring that the extracted features from the three domains are indistinguishable. (2) Structural Stream: extracting structural EEG features from S, U , and T . Based on the extracted DE features at five different frequency bands, a graph convolution network (GCN) is constructed for spatial feature representation, and the corresponding positive samples are generated by data augmentation. Then, a structural feature extractor FS(·) is defined to characterize structural feature representation from the input, with a contrastive loss ensuring that the structural features extracted from the positive samples are consistent with each other. (3) Self-Attentive Fusion: fusing the extracted non-structural and structural features from the above two parallel streams. A concatenation of the extracted non-structural and structural
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 293
Fig. 1. An overview of the proposed DS-AGC. It consists of three parts: non-structural stream, structural stream, and self-attentive fusion. S, U , and T refer to the labeled source domain, unlabeled source domain, and unknown target domain.
features is fed into a multi-head self-attention mechanism. This fusion process generates a new feature representation that emphasizes the most discriminative features related to emotions and suppresses irrelevant information. To ensure the decodability of the features, we train a classifier on the labeled source data (S) using the fused features, with a classification loss. The feature representation and the classifier are optimized jointly, ensuring the final feature representation is effective enough for emotion recognition.
A. Non-Structural Stream
Traditional EEG-based supervised domain adaptation methods primarily utilize DANN [25], aiming to align feature distributions between source and target domains [35], [36], [37], [38], [39], [40]. However, in semi-supervised learning, treating labeled and unlabeled source data as a single domain complicates adaptation and adversely affects performance of downstream tasks [41], [42]. To address distribution mismatches among different domains, this paper proposes a novel semi-supervised multi-domain adaptation method, which aligns feature representations among the labeled source domain S, the unlabeled source domain U , and the target domain T . The feature distribution discrepancies across these domains could be mitigated and the model’s generalization capabilities could be further enhanced.
In the proposed model, three domains are defined below. The labeled source domain S (Ds = {Xs, Ys} = {(xis, yis)}Ns
i=1),
which contains labeled samples with their corresponding emotion labels; the unlabeled source domain U (Du = {Xu} =
{(xiu)}Nu
i=1), which contains unlabeled samples without emo
tion labels; and the unknown target domain T (Dt = {Xt} =
{(xit)}Nt
i=1), which contains samples from an unseen domain that
needs to be classified. Here, xis, xiu, and xit are the EEG data from
the three domains, and yis is the given emotion label of xis in the labeled source domain. Ns, Nu, and Nt are the corresponding sample sizes. It is noted that the emotion label information in the unlabeled source domain and the unknown target domain are not available during model training. Finally, the three domains, S, U , and T , constitute a multi-domain. For the existing domain adaptation approaches involving two domains (the labeled source domain S and the target domain T ), the corresponding target error T (h) is constrained by the following inequality [43]
T (h) ≤ δ + S(h) + dH (DS, DT ) , (1)
where δ represents the difference in labeling functions across the two domains, typically assumed to be small under the covariate shift assumption [44]. S(h) denotes the source error for a classification hypothesis h as determined by the source classifier.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


294 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
dH(DS, DT ) measures the divergence between the source domain distribution DS and the target domain distribution DT , estimated directly from the error of a trained binary classifier [45]. In the semi-supervised learning, accurately estimating source error and H-divergence with limited labeled source data is highly challenging. To address this, our method utilizes both labeled and unlabeled source samples for estimating source error and H-divergence. The source domain is defined as S∗ = {S, U }, combining labeled source data (S) and unlabeled source data (U ). The convex hull ΛS∗π of S∗π is a set of mixed distributions, defined as
ΛS∗π = {DS∗π : DS∗π (·) = πS DS (·) + πU DU (·)}, (2)
where DS∗π is a distribution calculated by the weighted sum of the labeled source domain distribution DS and the unlabeled source domain distribution DU . πS and πU are the respective weights, belonging to the simplex Δ1. For the target domain T , DT is the nearest element to DT within ΛS∗π , given as
argminπS,πU dH [DT , πS DS + πU DU ] . (3)
Then, based on prior domain adaptation methods [45], [46], [47], a generalization upper-bound for the target error T (h) can be derived as
T (h) ≤ S(h) + min {EDT [|FT − FT |] , EDT [|FT − FT |]}
+ πU (dH (DS, DU )+dH (DU , DT ))+πSdH (DS, DT )
+ πU min {EDS [|FS − FU |] , EDU [|FU − FS|]} , (4)
where S(h) is the error of the labeled source domain, and dH(·) denotes the H-divergence between the specified domains. FT ̄(x) = πSFS(x) + πU FU (x) is the labeling function for any
x ∈ Supp(D ̄T ), representing the target domain’s labeling function. In extreme cases where all samples in the source domain are labeled (i.e., U is empty), the upper bound in (4) equals the bound given in the traditional supervised domain adaptation method. Next, we will demonstrate how to empirically optimize the target error in the downstream tasks. For the non-structural stream, we focus on extracting nonstructural EEG features. To address the distribution shift among the three domains, a multi-domain adversarial neural network is incorporated for feature adaptation. It aligns the feature distributions, making them more consistent and reliable across the three different domains. Specifically, we first flatten the extracted DE features into one-dimensional feature vectors (termed as {f1, f2, . . ., fm} in Fig. 1, m is the feature dimensionality) and input them into a feature extractor FNS(·) for sample feature extraction. This produces the corresponding features as
{f ̃1, f ̃2, . . ., f ̃m ̇ }, where  ̇m is the obtained feature dimensionality after FNS(·). To align the distribution shift among the extracted FNS(Xs), FNS(Xu), and FNS(Xt) for the labeled source domain, unlabeled source domain, and unknown target domain, we introduce a discriminator d(·) with parameters θd to distinguish the domain from which the sample features originate. The distribution discrepancies among the three domains are minimized by optimizing the discriminator loss function. For an ideal joint hypothesis across the S, U , and T domains, the second term min{EDT [|FT − FT |], EDT [|FT − FT |]} and the
last term πU min{EDS [|FS − FU |], EDU [|FU − FS|]} in (4) are assumed to be small under the covariate shift assumption [43], [44], [46], [48]. Previous research has demonstrated that minimizing the Hdivergence can be approximated by maximizing the classification error of the domain discriminator through adversarial training [25], [45]. Given the distribution differences among the three domains, the domain discriminator loss is redefined in this study as
Ldisc (θf , θd) = − ∑
xi
l (xi) log d (FNS (xi)) , (5)
where l(xi) is a one-hot domain label of the input sample data xi, and FNS(xi) is the corresponding extracted non-structural features. The domain adversarial training aligns the feature representation distribution across the three domains, making the non-structural features more robust to domain changes and more effective for downstream tasks.
B. Structural Stream
We capture the structural information of EEG signals collected from multiple EEG channels and express the inherent relationships among the channels by extracting the structural features. It enables us to gain more valuable insights into the complex interconnections and dependencies within the EEG network. An undirected graph G = (V G, EG, AG) is defined, where V G, EG, AG represent the nodes, edges, and adjacency matrix, respectively. |V G| is equal to the number of the EEG channels, given as NG. For a sample data xi, the input is denoted as Ψi = (ψi
1, ψi
2, . . . , ψi
NG )T ∈ RNG×Cde , where
ψi
k ∈ RCde (k ∈ 1, 2, . . . , NG) is the extracted DE features of the kth node.
1) Graph Convolution Network: A GCN is developed to aggregate the neighbor information of the feature matrix Ψi for spatial feature extraction, resulting in G. Specifically, we construct a channel-based graph, where the nodes V G correspond to EEG channels, and the node features are the extracted DE features at each channel termed as Ψi = (ψi
1, ψi
2, . . . , ψi
NG )T ∈
RNG×Cde .
Different from the traditional GCNs that use a fixed adjacency matrix (such as the k-nearest neighbor graph [49]), we define a dynamic adjacency matrix AG as
AG
jk = exp (− ReLU (wT ||ψi
j − ψi
k ||))
∑NG
k=1 exp (− ReLU (wT ||ψi
j − ψi
k||)) , (6)
where ψi
j and ψi
k are the extracted DE features at the jth and kth channels. The linear rectification function (ReLU) is employed as an activation function here to ensure that the output of the linear operation (i.e., the dot product between the weight vector w and the node distance ||ψi
j − ψi
k||) is non-negative, which introduces non-linearity into the model and improves its capacity to learn complex patterns. The weight vector w is learned by minimizing the GCN loss function as
Lgcn = λ
N ∑G
j,k=1
∥∥ψi
j − ψi
k
∥∥2 AG
jk + ‖AG‖2
F . (7)
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 295
In (7), the first term quantifies the similarities between any two nodes (ψi
j and ψi
k). When the nodes exhibit dissimilar characteristics, it leads to a smaller adjacency value (AG
jk). As the brain network is known to exhibit sparse connectivity, we incorporate sparsity constraint into the graph learning process. A Frobenius norm term of the adjacency matrix AG is included in (7), which encourages the learned adjacency matrix to be sparse. λ ≥ 0 is a regularization parameter that controls the trade-off between graph learning and sparsity degree. Based on the learned adjacency matrix AG, the node representation could be characterized on the basis of the Chebyshev expansion of the graph Laplacian. The Chebyshev graph convolution [50] is defined as a Φ − 1 degree polynomial, given as
G=
Φ−1
∑
φ=0
θφTφ(L ̃)xi, (8)
where θ ∈ RΦ is a Chebyshev coefficient vector, and xi is the in
put sample data. Tφ(L ̃) ∈ RNG×NG is the φth order Chebyshev polynomial with the variable L ̃ given as
L ̃ = 2
λmax
L − ING , (9)
where λmax is the maximum eigenvalue of the Laplacian matrix. ING is the identity matrix, and L is the Laplacian matrix calculated as
L = D − AG, (10)
where D is the degree matrix. The Chebyshev polynomials are given as
Tφ(L ̃) = 2L ̃Tφ−1(L ̃) − Tφ−2(L ̃), (11)
where T0(L ̃) = 1 and T1(L ̃) = L ̃. In the final obtained graph G, the corresponding node representation could well capture information about the φth order nodes of the graph and provide a richer and more comprehensive view of the graph.
2) Graph Contrastive Learning: To further enhance feature representation, GCL is introduced to learn representations that are robust to certain transformations or augmentations of the data. This process ensures that similar instances are brought closer together in the representation space, while dissimilar instances are pushed apart. Building upon the G obtained in Section III-B1, two aug
mented graphs, denoted as Gˆi and Gˆj, are generated as positive samples. We randomly drop ζ% of the nodes from G, following a uniform dropout probability distribution, similar to [34]. The node features of the augmented graphs are then flattened into one-dimensional feature vectors, denoted as {gˆ1, . . ., gˆn}
and {gˆ′1, . . ., gˆ′n}, respectively. n is the corresponding feature dimensionality. A feature extractor FS(·) is applied to generate high-level feature representation, resulting as {g ̃1, . . ., g ̃ ̇n} and
{g ̃′1, . . ., g ̃′n ̇ }. n ̇ is the obtained feature dimensionality after the feature extraction. Then, a projector P (·) is used to further reduce the feature dimensionality, producing zi and zj for each augmented graph. To ensure consistency between the feature representation of the two augmented graphs generated from
the same input, a contrastive learning loss Lgcl is defined as a normalized temperature-scaled cross-entropy loss, given as
Lgcl = − log exp (Sim (zi, zj) /τ )
∑B
k=1,k=i exp (Sim (zi, zk) /τ ) , (12)
where Sim refers to cosine similarity and τ is a temperature parameter to adjust the feature learning performance. Lgcl encourages the similarity between zi and zj (positive samples) to be maximized, while pushing away the similarity between zi and zk (negative samples). B is the batch size.
C. Self-Attentive Fusion
Self-attentive fusion is introduced to effectively highlight important features and assign higher weights to the source data that is in closer proximity to the target data, resulting in more informative feature representation. Furthermore, to ensure that the extracted features are discriminant for emotion recognition, a supervised classification part is incorporated into the model learning process as well.
1) Informative Feature Fusion: The extracted non-structural features by FNS(·) and structural features by FS(·) are concatenated into a new feature representation, denoted as
{f ̃1, . . . , f ̃m ̇ , g ̃1, . . . , g ̃ ̇n}. The multi-head self-attention mechanism is incorporated to emphasize the most crucial features relevant to emotions while reducing the influence of irrelevant information. It adapts to various viewpoints or focal points, autonomously determining the focus of feature extraction based on input data and task-specific requirements. As shown in Fig. 1, we generate three matrices Q, K, and V from the input using linear transformations. Inspired from [51], the attention weights are given as
AT T (Q, K, V ) = Softmax
( QKT
√d
)
V (13)
Then, we further extend the attention mechanism to H heads over the three matrices. Each matrix is divided into H subspaces, termed as Q = {Q1, . . . , QH }, K = {K1, . . . , KH }, V = {V 1, . . . , V H }. In each subspace h ∈ H, we calculate Ah using the attention formula, given as
Ah = AT T (Qh, Kh, V h) (14)
Finally, all H representations are concatenated together to obtain the final output M HA(X) = M HA(Q, K, V ) for classification as
M HA(Q, K, V ) = Concat (A1, . . . , AH ) . (15)
2) Informative Sample Selection: During the model training process, we place additional attention on weighing the contribution of each labeled source data. We assign higher weights to those labeled source data that offer more valuable information for effective emotion recognition. This process helps prioritize and focus on the most informative data during the optimization process. Specifically, based on the feature representation M HA(·), a fully connected layer φ(·) is designed as
{R(Xs) = φ(M HA(Xs)) = {rs1, . . . , rsB}
R(Xt) = φ(M HA(Xt)) = {rt1, . . . , rtB}, (16)
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


296 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
where B represents the batch size. For each labeled source data rsb (1 ≤ b ≤ B), we calculate the corresponding cosine similarity with all unknown target data as
Sim(rb
s) = 1
B
B ∑
k=1
rsb · rtk
∥∥rsb
∥∥2
∥∥rtk
∥∥2
. (17)
Softmax is then applied on {Sim(rs1), . . . , Sim(rsB)} for nor
malization. Finally, the normalized similarity weight Sim(rsb) is used to adjust the sample contribution in the multi-class cross-entropy loss function as
Lce = − 1
B
B ∑
b=1
C ∑
c=1
yc
b log (Sim(rb
s) · yˆc
b
) , (18)
where yc
b and yˆc
b denote the actual emotion label and the predicted emotion label of b-th labeled source data, respectively. C is the total number of emotion categories.
IV. EXPERIMENTAL RESULTS
A. Benchmark Databases
To assess the efficacy of the proposed DS-AGC model, comprehensive experiments are carried out on four benchmark EEG databases: SEED [9], SEED-IV [52], SEED-V [53] and FACED [54]. In the SEED database, three emotional states (negative, neutral, and positive) were involved, and 15 subjects (7 males and 8 females) were recruited. In the SEED-IV database, four emotional states (happiness, sadness, fear, and neutral) were selected, and 15 subjects (7 males and 8 females) were recruited. In the SEED-V database, it included five emotional states (happiness, sadness, fear, disgust, and neutral), with EEG recordings from 16 subjects (6 males and 10 females). In the FACED database, it included nine emotional states (amusement, inspiration, joy, tenderness, anger, fear, disgust, sadness, and neutral), with EEG recordings from 123 subjects. To maintain consistency with previous research on the three benchmark databases and ensure fair comparisons, our study also utilizes the pre-computed DE features [9], [55].
B. Implementation Details and Model Setting
In the implementation, the feature extractors FNS(·) and FS(·), as well as the domain discriminator d(·), are composed of fully connected layers with the ReLU activation function. Given the input DE features with a small feature dimensionality, a lightweight network architecture would be more appropriate as discussed in prior works [56], [57]. Specifically, FNS(·) is designed with 310 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-64 neurons (hidden layer 2)-ReLU activation-64 neurons (output feature layer). The probability of node dropout ζ% is set to 0.2, resulting in 49 remaining channels. FS(·) is designed with 245 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-64 neurons (hidden layer 2)-ReLU activation-64 neurons (output feature layer). The domain discriminator d(·) is designed with 64 neurons (input layer)-64 neurons (hidden layer 1)-ReLU activation-dropout
Fig. 2. The cross-subject leave-one-subject-out cross-validation experimental protocol with incomplete labels. S, U , and T represent the labeled source domain, unlabeled source domain, and unknown target domain. For the SEED and SEED-IV databases, which contain a total of 15 subjects, 14 − N subjects are allocated to S, and N subjects to U . Similarly, for the SEED-V database with 16 subjects, 15 − N subjects are assigned to S, and N subjects to U . M denotes the total number of trials for each subject. Lce, Lgcn, Lgcl, and Ldisc are the classification loss, GCN loss, GCL loss, and discriminator loss, given in (18), (7), (12), and (5). In the implementation, Lce is calculated using only S (represented by blue stars), as the label information for U and T is unknown. Lgcn, Lgcl, and Ldisc, which do not depend on label information, are calculated using data from S (blue stars), U (pink circles), and T (gray triangles).
layer-64 neurons (hidden layer 2)-2 neurons (output layer) / 3 neurons (output layer)-Softmax activation. Gradient descent and parameter optimization are carried out using the RMSprop optimizer, with a learning rate set to 1e-3 and a batch size of 48. In the GCN architecture, we set φ in (8) as 3 to ensure the extraction of stable graph network features. This choice strikes a balance by incorporating relevant channel features while minimizing the introduction of noise. To further fine-tune the model, we set the balancing parameter λ in (7) to 0.01. For the multi-head self-attention mechanism, we utilize H = 64 in (15). This configuration allows the model to capture multiple perspectives and enhance its ability to input data. All the models are trained using the PyTorch API on an NVIDIA GeForce RTX 1080 GPU, with CUDA version 11.7. During the model training process, we solely utilize the raw target data without any label information. This approach aligns with previous EEG-based emotion recognition methods that employ a transfer learning framework, as demonstrated in studies such as [40], [41], [58], [59].
C. Experimental Protocol With Incomplete Labels
As illustrated in Fig. 2, we implement a strict leave-onesubject-out cross-validation experimental protocol. For the SEED and SEED-IV databases (each containing 15 subjects), we sequentially select 14 subjects as the source domain and the remaining one as the target domain for model testing. Following the same strategy, in the SEED-V database (containing 16 subjects), 15 subjects are used as the source domain, and the remaining subject is used as the target domain. In order to ensure each subject is rotated as the target domain, undergoing a total of 15 rounds (SEED and SEED-IV) or 16 rounds (SEED-V) of model training and testing. The final reported classification results are the average accuracy and standard deviation across 15 (SEED and SEED-IV) or 16 (SEED-V) rounds. Within the source domain, a subset of subjects (N subjects) is classified as
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 297
TABLE I THE EMOTION RECOGNITION RESULTS ON SEED DATABASE WITH INCOMPLETE LABEL CONDITIONS, IN TERMS OF THE AVERAGE ACCURACY (%) AND STANDARD DEVIATION (%)
the unlabeled source domain U , and the remaining 14 − N subjects (SEED and SEED-IV) or 15 − N (SEED-V) are classified as the labeled source domain S. For example, if the first subject is assigned to T , then subjects 2 to 2 + N − 1 are assigned to U , and subjects 2 + N to 15 (SEED and SEED-IV) or 2 + N to 16 (SEED-V) belong to S. To fully evaluate the model’s stability in various situations of label scarcity, we vary the N value from 1 to 13 for SEED and SEED-IV, and from 1 to 14 for SEED-V in the implementation. During the training process of the model, the unlabeled source domain U is excluded from the beginning and only the labeled source domain S and the unknown target domain T participate in the training for the first Et iterations. The output layer of the domain discriminator d(·) has two neurons at this point, performing binary classification between the S and T domains. After the model reaches a certain stability, the unlabeled source domain U is added to the training, and the output layer of the domain discriminator d(·) has three neurons, performing ternary
classification among the S, U , and T domains. Notably, only the data from the labeled source domain S is used for calculating the cross-entropy loss Lce in the classifier throughout the entire process.
D. Emotion Recognition Performance With Incomplete Labels
As shown in Table I, the proposed DS-AGC model consistently outperforms existing machine learning and deep learning models on the SEED database, with an average improvement of 5.83%. Furthermore, even with an increasing number of incomplete labels, the model maintains relatively stable performance. Under the most challenging condition of extreme label scarcity (N = 13), the DS-AGC model achieves an improvement of 7.56%, demonstrating robust stability in situations with minimal labeled data. Table II shows the experimental comparison results on the SEED-IV database. Similarly, we vary the N value from 1 to 13. The corresponding average accuracy of DS-AGC on
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


298 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
TABLE II THE EMOTION RECOGNITION RESULTS ON SEED-IV DATABASE WITH INCOMPLETE LABEL CONDITIONS, IN TERMS OF THE AVERAGE ACCURACY (%) AND STANDARD DEVIATION (%)
SEED-IV is 61.32 ± 10.41, with an average improvement of 0.19%. It shows DS-AGC exhibits particularly strong adaptability and efficiency with larger N value (less labeled source data). The evaluation performance on the SEED-V database is presented in Table III. The DS-AGC model achieves an average accuracy of 53.87 ± 11.14, marking an average improvement of 0.48% over existing methods. Similar to the model’s performance on SEED-IV, the superiority of the DS-AGC is particularly evident in situations of acute label scarcity (when the N value is larger than 7). These experimental results demonstrate that the proposed DS-AGC outperforms existing methods on average across different N values and exhibits particular superiority when labeled source data is limited.
E. Further Validation on the FACED Dataset
To further assess the effectiveness of the proposed DS-AGC semi-supervised learning model in handling large amounts of unlabeled data, we also validate it with the FACED dataset [54].
For a total of 123 subjects, each subject underwent 28 trials corresponding to 9 different emotions: amusement, inspiration, joy, tenderness (categorized as positive emotions), anger, fear, disgust, sadness (categorized as negative emotions), and a neutral state. Following standard EEG preprocessing, similar to the SEED series database, DE features were extracted. Here, we evaluate the FACED database under a ten-fold cross-validation method. Specifically, the total of 123 subjects is randomly divided into ten groups. In each iteration, one group serves as the unknown target domain for testing, while the remaining nine groups serve as the source domain, and this process is repeated for ten rounds. This ensures that each subset is used as the test set once. In the source domain, the training data is further divided into labeled (S) and unlabeled (U ) domains according to a specific ratio. To fully verify the model stability under different conditions of label scarcity, three semi-supervised cases are considered. (1) 66% of the data is assigned as labeled source data and 33% as unlabeled source
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 299
TABLE III THE EMOTION RECOGNITION RESULTS ON SEED-V DATABASE WITH INCOMPLETE LABEL CONDITIONS, IN TERMS OF THE AVERAGE ACCURACY (%) AND STANDARD DEVIATION (%)
TABLE IV EXPERIMENTAL RESULTS ON THE FACED DATABASE USING CROSS-SUBJECT TEN-FOLD CROSS-VALIDATION PROTOCOL WITH INCOMPLETE LABELS
data (S : U = 2 : 1). (2) 50% of the data is assigned as labeled source data and 50% as unlabeled source data (S : U = 1 : 1). (3) 33% of the data is assigned as labeled source data and 66% as unlabeled source data (S : U = 1 : 2). The third case is the most challenging, as it has the smallest amount of labeled
source data. Additionally, two types of emotional classification tasks are conducted: a three-category classification (positive, neutral, and negative emotions) and a nine-category classification (amusement, inspiration, joy, tenderness, anger, fear, disgust, sadness, and neutral), denoted as Label3 and Label9,
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


300 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
TABLE V THE ABLATION RESULTS USING CROSS-SUBJECT LEAVE-ONE-SUBJECT-OUT CROSS-VALIDATION UNDER N = 2
respectively. As shown in Table IV, the experimental results indicate that, compared to the existing methods, the proposed DS-AGC model achieves superior performance, with an average improvement of 0.43% in three-class classification results and 0.64% in nine-class classification results.
V. DISCUSSION AND CONCLUSION
In order to thoroughly assess the performance of the proposed model, we conduct a series of experiments to evaluate the contribution of each module in the proposed model and also explore the effect of various hyperparameter settings.
A. Ablation Study
To assess the contribution of each component in the proposed model, a series of ablation studies are conducted. Specifically, we systematically removed different components to observe their impact on the overall performance. Table V reports the ablation results using cross-subject leave-one-subject-out crossvalidation under N = 2. First, we remove the discriminator from the model and find that the performance significantly drops when neglecting the distributional differences among domains. The model’s performance decrease from 87.37 ± 06.19 to 78.71 ± 06.52 on SEED, from 66.00 ± 07.93 to 64.03 ± 08.60 on SEED-IV, and from 59.40 ± 09.99 to 53.32 ± 10.95 on SEED-V. Second, we evaluate the benefits of considering three domains (labeled source domain, unlabeled source domain, and target domain) and compare the proposed semi-supervised multi-domain adaptation method with traditional domain adaptation methods, which treat the labeled and unlabeled source domains as a single domain. The results demonstrate that considering feature alignment between both labeled and unlabeled source data achieves higher performance, leading to improvements of 2.44%, 1.50%, and 5.06% on the SEED, SEED-IV, and SEED-V databases, respectively. Third, we evaluate the model without contrastive learning, directly extracting structural features from the constructed GCN. The model performance falls from 87.37 ± 06.19 to 86.48 ± 05.07 for SEED, from 66.00 ± 07.93 to 62.98 ± 07.29 for SEED-IV, and from 59.40 ± 09.99 to 57.33 ± 11.17 for SEED-V. It shows the significance of contrastive learning in improving feature discrimination and enhancing overall model performance. Fourth, the importance of attentive feature fusion is evaluated. When the classifier is developed based on a simple concatenation of the extracted non-structural and structural features, a performance decline is observed across all three databases: from 87.37 ± 06.19 to 84.23 ± 05.47 for SEED, from 66.00 ± 07.93 to 64.39 ± 07.83
Fig. 3. Experimental results under different settings on the SEED databases. Red line with triangle marker: the proposed DS-AGC; pink line with circle marker: single structural stream; green line with square marker: single nonstructural stream; blue line with diamond marker: without unlabeled source data.
for SEED-IV, and from 59.40 ± 09.99 to 57.25 ± 13.95 for SEED-V. Lastly, the effect of attentive sample selection is examined. Equal weights (Sim(rsb) = 1) in (18) are assigned to all samples. The results show that considering sample importance could be beneficial to the performance, bringing an average improvement in model performance by 0.45%, 0.57% and 0.97% on SEED, SEED-IV, and SEED-V databases, respectively. The above results provide a detailed breakdown of the performance of the model with and without specific components, indicating that each component in the proposed model plays an important role in improving the overall performance and addressing the challenges of emotion recognition in real-world scenarios with incomplete label information. Additionally, we further assess the contribution of the dualstream design and the incorporation of unlabeled data in the model learning process. To evaluate the dual-stream design, we analyze the model’s performance with each stream separately under various N values. As shown in Fig. 3, whether using a single structural stream (pink line with circle marker) or a single non- structural stream (green line with square marker), their performance across all settings (N = 1 : 13) is inferior to that of the complete dual-stream design (red line with triangle marker). In evaluating the contribution of the unlabeled source data in modeling, we compare the model’s performance both with and without the inclusion of the unlabeled source data. As shown in Fig. 3, the model performance without unlabeled source data (blue line with diamond marker) shows poorer results compared
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 301
TABLE VI MODEL PERFORMANCE ON UNSEEN SEED DATA, WHERE THE TOTAL NUMBER OF TRIALS OF EACH SUBJECT IS 15
TABLE VII MODEL PERFORMANCE ON UNSEEN SEED-IV DATA, WHERE THE TOTAL NUMBER OF TRIALS OF EACH SUBJECT IS 24
to the performance with unlabeled source data. It demonstrates that including the unlabeled source data plays an important role in the model learning process. Additionally, we conduct supervised learning for comparison, where all the source data are labeled. Following the same cross-validation protocol, the supervised learning results on SEED, SEED-IV, and SEED-V are 87.72 ± 06.54, 67.95 ± 07.84, and 60.27 ± 11.92, respectively. These results indicate that the proposed DS-AGC model can achieve relatively stable results similar to supervised learning in a semi-supervised manner, even when labeled data is limited.
B. Model Performance on Unseen Target Data
We further explore the model stability on completely unseen target data. For this purpose, we divide the target domain data into two parts: one as a validation set (visible in the model learning phase) and the other as an unseen test set (completely invisible in the model learning phase). The ratio of the validation set to the unseen set is controlled by the parameter Z. When Z = 0, it indicates that all the target domain data are assigned as the unseen test set. When Z = 15 (SEED and SEED-V) or Z = 24 (SEED-IV), it indicates that all the target domain data are assigned as the validation set, as utilized in Section IV. The corresponding results under various Z values on the SEED, SEED-IV, and SEED-V databases are reported in Tables VI, VII, and VIII, respectively. It shows that an increase in the Z value, which brings more information during the model learning phase, could be helpful in improving the model performance. Even in the extreme case with Z = 0 (all the target domain data is completely unseen during model learning), it still manages to exhibit acceptable performance. It demonstrates the model’s good generalization capability and accuracy in predicting unknown data, highlighting its ability to handle incomplete label problems in real-world application scenarios.
TABLE VIII MODEL PERFORMANCE ON UNSEEN SEED-V DATA, WHERE THE TOTAL NUMBER OF TRIALS OF EACH SUBJECT IS 15
Fig. 4. Model performance under various Et values on the SEED database.
C. The Effect of the Hyperparameter Et
During the training process, the unlabeled source domain U is incorporated when the model has undergone an initial warmup period of Et epochs. We evaluate how different Et values influence model performance, with Et ranging from 0 (start of training) to 100 (maximum number of epochs). As shown in Fig. 4, optimal results are obtained when U is incorporated once the model has achieved a certain degree of stability, rather than from the very beginning (Et = 0). Introducing U at the onset can inject noise, potentially disrupting the model’s initial learning phase.
D. Data Visualization
To delve deeper into the learning process, we employ the t-distributed stochastic neighbor embedding (t-SNE) algorithm [73] to visually compare the acquired feature representation at different stages. This analysis allows us to gain valuable insights into the model’s learning dynamics. Specifically, we visualize the fused features obtained through M HA(·) (Fig. 5) and the final classification results (Fig. 6) at various learning stages: before training, at the 30th training epoch, and in the final trained model. By examining these visualizations at different learning stages, we obtain vivid depictions of the evolution and enhancement of both the feature representation and the classification performance. These visualizations showcase a notable expansion of inter-class separability, meaning that the distinctions between different classes become more pronounced. Simultaneously, the intra-class variability is minimized, resulting in tighter clustering of samples belonging to the same class.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


302 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
Fig. 5. A visualization of the obtained informative feature {ξ1, . . . , ξ  ̇m+n ̇ } by M HA(·) at three different stages: (a) before training, (b) at the 30th training epoch, and (c) in the final trained model. In this visualization, the circle, asterisk, and triangle represent the labeled source domain (S), unlabeled source domain (U ), and the unknown target domain (T ), respectively. The red, purple, and blue colors correspond to negative, neutral, and positive emotions, respectively.
Fig. 6. A visualization of the classification results at three different stages: (a) before training, (b) at the 30th training epoch, and (c) in the final trained model. In this visualization, the circle, asterisk, and triangle represent the labeled source domain (S), unlabeled source domain (U ), and the unknown target domain (T ), respectively. The red, purple, and blue colors correspond to negative, neutral, and positive emotions, respectively.
Through this visual examination, we observe a clear trend of the model’s ability to discriminate between different classes, with the learned feature representation becoming increasingly distinct and discriminative. The reduction in intra-class variability ensures that samples within the same class are closer to each other, reinforcing the model’s ability to accurately classify them. This visualization evidence underscores the model’s capability to learn meaningful and discriminative features, enabling it to make refined distinctions between classes and achieve enhanced classification performance throughout the learning process.
E. Conclusion
This paper proposes a novel semi-supervised dual-stream self-attentive adversarial graph contrastive learning model (DS-AGC) for cross-subject EEG-based emotion recognition. Through an intelligent dual-stream framework, both nonstructural and structural information from EEG signals are well established and further fused through a self-attentive mechanism. Based on comprehensive experimental validations on the well-known databases, the proposed DS-AGC shows the ability to effectively leverage both labeled and unlabeled data and also
makes it a promising solution for situations where labeled data is scarce or expensive to obtain.
REFERENCES
[1] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised learning,” Mach. Learn., vol. 109, no. 2, pp. 373–440, 2020. [2] X. Jia, K. Li, X. Li, and A. Zhang, “A novel semi-supervised deep learning framework for affective state recognition on EEG signals,” in Proc. IEEE Int. Conf. Bioinf. Bioeng., 2014, pp. 30–37.
[3] G. Zhang and A. Etemad, “Holistic semi-supervised approaches for EEG representation learning,” in Proc. IEEE Int. Conf. Acoust. Speech Signal Process., 2022, pp. 1241–1245. [4] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, “MixMatch: A holistic approach to semi-supervised learning,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2019, pp. 1–11.
[5] K. Sohn et al., “FixMatch: Simplifying semi-supervised learning with consistency and confidence,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2020, pp. 596–608. [6] D. Berthelot, R. Roelofs, K. Sohn, N. Carlini, and A. Kurakin, “AdaMatch: A unified approach to semi-supervised learning and domain adaptation,” 2021, arXiv:2106.04732.
[7] G. Zhang, V. Davoodnia, and A. Etemad, “PARSE: Pairwise alignment of representations in semi-supervised EEG learning for emotion recognition,” IEEE Trans. Affective Comput., vol. 13, no. 4, pp. 2185–2200, Fourth Quarter, 2022. [8] R.-N. Duan, J.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for EEG-based emotion classification,” in Proc. 6th Int. IEEE/EMBS Conf. Neural Eng., 2013, pp. 81–84.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 303
[9] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks,” IEEE Trans. Auton. Mental Develop., vol. 7, no. 3, pp. 162–175, Sep. 2015. [10] X. Li, B. Hu, S. Sun, and H. Cai, “EEG-based mild depressive detection using feature selection methods and classifiers,” Comput. Methods Prog. Biomed., vol. 136, pp. 151–161, 2016. [11] Y.-J. Liu, M. Yu, G. Zhao, J. Song, Y. Ge, and Y. Shi, “Real-time movie-induced discrete emotion recognition from EEG signals,” IEEE Trans. Affective Comput., vol. 9, no. 4, pp. 550–562, Fourth Quarter, 2018. [12] D. Zhang et al., “Cascade and parallel convolutional recurrent neural networks on EEG-based intention recognition for brain computer interface,” in Proc. AAAI Conf. Artif. Intell., 2018, pp. 1–8.
[13] F. Yang, X. Zhao, W. Jiang, P. Gao, and G. Liu, “Multi-method fusion of cross-subject emotion recognition based on high-dimensional EEG features,” Front. Comput. Neurosci., vol. 13, 2019, Art. no. 53. [14] S. K. Khare, V. Bajaj, and G. R. Sinha, “Adaptive tunable Q wavelet transform-based emotion identification,” IEEE Trans. Instrum. Meas., vol. 69, no. 12, pp. 9609–9617, Dec. 2020. [15] A. Anuragi, D. S. Sisodia, and R. B. Pachori, “EEG-based cross-subject emotion recognition using fourier-bessel series expansion based empirical wavelet transform and NCA feature selection method,” Inf. Sci., vol. 610, pp. 508–524, 2022. [16] T. Song, W. Zheng, P. Song, and Z. Cui, “EEG emotion recognition using dynamical graph convolutional neural networks,” IEEE Trans. Affective Comput., vol. 11, no. 3, pp. 532–541, Third Quarter, 2020. [17] W. Niu, C. Ma, X. Sun, M. Li, and Z. Gao, “A brain network analysis-based double way deep neural network for emotion recognition,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 31, pp. 917–925, 2023.
[18] W.-L. Zheng and B.-L. Lu, “Personalizing EEG-based affective models with transfer learning,” in Proc. 25th Int. Joint Conf. Artif. Intell., 2016, pp. 2732–2738. [19] W. Samek, F. C. Meinecke, and K.-R. Müller, “Transferring subspaces between subjects in brain–computer interfacing,” IEEE Trans. Biomed. Eng., vol. 60, no. 8, pp. 2289–2298, Aug. 2013. [20] H. Morioka et al., “Learning a common dictionary for subject-transfer decoding with resting calibration,” NeuroImage, vol. 111, pp. 167–178, 2015. [21] T. Xu, W. Dang, J. Wang, and Y. Zhou, “DAGAM: A domain adversarial graph attention model for subject independent EEG-based emotion recognition,” J. Neural Eng., vol. 20, no. 1, pp. 016 022–016 032, 2023. [22] D. Huang, S. Zhou, and D. Jiang, “Generator-based domain adaptation method with knowledge free for cross-subject EEG emotion recognition,” Cogn. Comput., vol. 14, no. 4, pp. 1316–1327, 2022. [23] Z. He, Y. Zhong, and J. Pan, “An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition,” Comput. Biol. Med., vol. 141, pp. 105 048–105 060, 2022. [24] Y.-M. Jin, Y.-D. Luo, W.-L. Zheng, and B.-L. Lu, “EEG-based emotion recognition using domain adaptation network,” in Proc. Int. Conf. Orange Technol., 2017, pp. 222–225. [25] Y. Ganin et al., “Domain-adversarial training of neural networks,” J. Mach. Learn. Res., vol. 17, no. 1, pp. 2096–2030, 2016. [26] J. Li, S. Qiu, C. Du, Y. Wang, and H. He, “Domain adaptation for EEG emotion recognition based on latent representation similarity,” IEEE Trans. Cogn. Devel. Syst., vol. 12, no. 2, pp. 344–353, Jun. 2020. [27] Y. Peng, W. Wang, W. Kong, F. Nie, B.-L. Lu, and A. Cichocki, “Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals,” IEEE Trans. Affective Comput., vol. 13, no. 4, pp. 1941–1958, Fourth Quarter, 2022. [28] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, “Simple and deep graph convolutional networks,” in Proc. Int. Conf. Mach. Learn., 2020, pp. 1725–1735. [29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” 2018, arXiv: 1810.04805.
[30] P. Li et al., “An effective self-supervised framework for learning expressive molecular global representations to drug discovery,” Brief. Bioinf., vol. 22, no. 6, 2021, Art. no. bbab109. [31] X. Liu, Y. Luo, P. Li, S. Song, and J. Peng, “Deep geometric representations for modeling effects of mutations on protein-protein binding affinity,” PLoS Comput. Biol., vol. 17, no. 8, 2021, Art. no. e1009284. [32] M. N. Mohsenvand, M. R. Izadi, and P. Maes, “Contrastive representation learning for electroencephalogram classification,” Mach. Learn. Health, vol. 136, pp. 238–253, 2020.
[33] X. Shen, X. Liu, X. Hu, D. Zhang, and S. Song, “Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition,” IEEE Trans. Affective Comput., vol. 14, no. 3, pp. 2496–2511, Third Quarter, 2023. [34] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph contrastive learning with augmentations,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2020, pp. 5812–5823. [35] X. Du et al., “An efficient LSTM network for emotion recognition from multichannel EEG signals,” IEEE Trans. Affective Comput., vol. 13, no. 3, pp. 1528–1540, Third Quarter, 2022. [36] Y. Luo, S. Y. Zhang, W. L. Zheng, and B. L. Lu, “WGAN domain adaptation for EEG-Based emotion recognition,” in Proc. Int. Conf. Neural Inf. Process., 2018, pp. 275–286. [37] Y. Li et al., “A novel bi-hemispheric discrepancy model for EEG emotion recognition,” IEEE Trans. Cogn. Devel. Syst., vol. 13, no. 2, pp. 354–367, Jun. 2021. [38] H. Li, Y.-M. Jin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emotion recognition using deep adaptation networks,” in Proc. Int. Conf. Neural Inf. Process., 2018, pp. 403–413. [39] L. Zhao, X. Yan, and B. Lu, “Plug-and-play domain adaptation for crosssubject EEG-based emotion recognition,” in Proc. 35th AAAI Conf. Artif. Intell. 33rd Conf. Innov. Appl. Artif. Intell. 11th Symp. Educ. Adv. Artif. Intell., 2021, pp. 863–870. [40] P. Zhong, D. Wang, and C. Miao, “EEG-based emotion recognition using regularized graph neural networks,” IEEE Trans. Affective Comput., vol. 13, no. 3, pp. 1290–1301, Third Quarter, 2022. [41] J. Li, S. Qiu, Y.-Y. Shen, C.-L. Liu, and H. He, “Multisource transfer learning for cross-subject EEG emotion recognition,” IEEE Trans. Cybern., vol. 50, no. 7, pp. 3281–3293, Jul. 2020. [42] H. Chen, M. Jin, Z. Li, C. Fan, J. Li, and H. He, “MS-MDA: Multisource marginal distribution adaptation for cross-subject and cross-session EEG emotion recognition,” Front. Neurosci., vol. 15, pp. 778 488–778 501, 2021. [43] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan, “A theory of learning from different domains,” Mach. Learn., vol. 79, no. 1, pp. 151–175, 2010. [44] S. B. David, T. Lu, T. Luu, and D. Pál, “Impossibility theorems for domain adaptation,” in Proc. 13th Int. Conf. Artif. Intell. Statist., 2010, pp. 129–136. [45] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, “Analysis of representations for domain adaptation,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2006, pp. 1–8. [46] I. Albuquerque, J. Monteiro, M. Darvishi, T. H. Falk, and I. Mitliagkas, “Generalizing to unseen domains via distribution matching,” 2019, arXiv: 1911.00804.
[47] H. Zhao, S. Zhang, G. Wu, J. M. Moura, J. P. Costeira, and G. J. Gordon, “Adversarial multiple source domain adaptation,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2018, pp. 1–12.
[48] A. Sicilia, X. Zhao, and S. J. Hwang, “Domain adversarial neural networks for domain generalization: When it works and how to improve,” Mach. Learn., vol. 112, no. 7, pp. 2685–2721, 2023. [49] B. Jiang, C. Ding, B. Luo, and J. Tang, “Graph-Laplacian PCA: Closedform solution and robustness,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2013, pp. 3492–3498. [50] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural networks on graphs with fast localized spectral filtering,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2016, pp. 1–9.
[51] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 1–11.
[52] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “EmotionMeter: A multimodal framework for recognizing human emotions,” IEEE Trans. Cybern., vol. 49, no. 3, pp. 1110–1122, Mar. 2019. [53] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition,” IEEE Trans. Cogn. Devel. Syst., vol. 14, no. 2, pp. 715–729, Jun. 2022. [54] J. Chen, X. Wang, C. Huang, X. Hu, X. Shen, and D. Zhang, “A large finer-grained affective computing EEG dataset,” Sci. Data, vol. 10, no. 1, pp. 740–749, 2023. [55] L.-C. Shi and B.-L. Lu, “Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning,” in Proc. Annu. Int. Conf. IEEE Eng. Med. Biol., 2010, pp. 6587–6590.
[56] R. Zhou et al., “PR-PL: A novel prototypical representation based pairwise learning framework for emotion recognition using EEG signals,” IEEE Trans. Affective Comput., vol. 15, no. 2, pp. 657–670, Second Quarter, 2024.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


304 IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 16, NO. 1, JANUARY-MARCH 2025
[57] Z. Liang et al., “EEGFuseNet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional EEG with an application to emotion recognition,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 29, pp. 1913–1925, 2021. [58] D. Wu, Y. Xu, and B.-L. Lu, “Transfer learning for EEG-based braincomputer interfaces: A review of progress made since 2016,” IEEE Trans. Cogn. Devel. Syst., vol. 14, no. 1, pp. 4–19, Mar. 2022. [59] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, and A. Song, “Can emotion be transferred?—A review on transfer learning for EEG-based emotion recognition,” IEEE Trans. Cogn. Devel. Syst., vol. 14, no. 3, pp. 833–846, Sep. 2022. [60] J. Suykens and J. Vandewalle, “Least squares support vector machine classifiers,” Neural Process. Lett., vol. 9, no. 3, pp. 293–300, 1999. [61] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via transfer component analysis,” IEEE Trans. Neural Netw., vol. 22, no. 2, pp. 199–210, Feb. 2011. [62] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, “Unsupervised visual domain adaptation using subspace alignment,” in Proc. IEEE Int. Conf. Comput. Vis., 2013, pp. 2960–2967. [63] S. Mika, B. Schölkopf, A. Smola, K.-R. Müller, M. Scholz, and G. Rätsch, “Kernel PCA and de-noising in feature spaces,” in Proc. Int. Conf. Neural Inf. Process. Syst., 1999, pp. 1–7.
[64] L. Breiman, “Random forests,” Mach. Learn., vol. 45, pp. 5–32, 2001. [65] J. Zhu, A. Arbor, and T. Hastie, “Multi-class adaboost,” Statist. Interface, vol. 2, no. 3, pp. 349–360, 2006. [66] B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain adaptation,” in Proc. 13th AAAI Conf. Artif. Intell., 2016, pp. 2058–2065. [67] B. Gong, Y. Shi, F. Sha, and K. Grauman, “Geodesic flow kernel for unsupervised domain adaptation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 2066–2073. [68] D. Coomans and D. L. Massart, “Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. K-nearest neighbour classification by using alternative voting rules,” Analytica Chimica Acta, vol. 136, pp. 15–27, 1982. [69] B. Sun and K. Saenko, “Deep coral: Correlation alignment for deep domain adaptation,” in Proc. Eur. Conf. Comput. Vis. Workshops, 2016, pp. 443–450. [70] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, “Deep domain confusion: Maximizing for domain invariance,” 2014, arXiv:1412.3474. [71] B. Zhang et al., “FlexMatch: Boosting semi-supervised learning with curriculum pseudo labeling,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2021, pp. 18 408–18 419. [72] H. Chen et al., “SoftMatch: Addressing the quantity-quality trade-off in semi-supervised learning,” 2023, arXiv:2301.10921. [73] L. Van der Maaten and G. Hinton, “Visualizing data using t-SNE,” J. Mach. Learn. Res., vol. 9, no. 11, pp. 1–27, 2008.
Weishan Ye is currently working toward the master’s degree with the Department of Biomedical Engineering, Shenzhen University, China. Her research interests include affective computing, semi-supervised learning, and transfer learning.
Zhiguo Zhang received the BEng degree from Tianjin University, in 2000, the MPhil degree from the University of Science and Technology of China, in 2003, and the PhD degree from the University of Hong Kong, in 2008. He is now a professor with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. He has authored 130 journal papers. His research interests include biomedical signal processing, neural engineering, and brain-inspired computation.
Fei Teng is currently working toward the bachelor degree with the Department of Biomedical Engineering, Shenzhen University, China. His research interests include transfer learning and affective brain-computer interface.
Min Zhang received the bachelor’s and PhD degrees in computer science from the Harbin Institute of Technology, Harbin, China, in 1991 and 1997, respectively. He is currently a professor with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. He has authored 180 papers in leading journals and conferences. His research interests include machine translation, natural language processing, and artificial intelligence. He is the vice president of COLIPS, Steering Committee member of PACLIC, executive member of AFNLP, and member of ACL.
Jianhong Wang received the MSHA degree from Nankai University-Flinders University in Australia, in 2008. From 2016 to 2019, she was vice president of Shenzhen People’s Hospital. From 2019 to now, she has served as secretary of the Party Committee of Shenzhen Kangning Hospital. She is also currently a master tutor of Southern University of Science and Technology. She has been engaged in hospital management for nearly 20 years, accumulated a lot of hospital management experience and achieved many achievements. Her current research interests include hospital management, public health, and psychologic medicine.
Dong Ni received the bachelor’s and master’s degrees in biomedical engineering from Southeast University, Nanjing, China, in 2000 and 2003, respectively, and the PhD degree in computer science and engineering from the Chinese University of Hong Kong, Hong Kong, China, in 2009. From 2009 to 2010, he was a postdoctoral fellow with the School of Medicine, University of North Carolina at Chapel Hill, USA. He has joined Shenzhen University since 2010. In 2013, he became associate professor in biomedical engineering with Shenzhen University. Due to outstanding achievements, he was promoted to professor in biomedical engineering with Shenzhen University in 2017. He has been the vice dean with the School of Biomedical Engineering, Shenzhen University since 2016. He founded the Medical Ultrasound Image Computing (MUSIC) Laboratory, Shenzhen University, in 2018. His research interests include ultrasound image analysis, image guided surgery, and machine/deep learning. He has published more than 200 peer-reviewed journal/conference articles. He is also committed to translate multiple research findings into products to help increase the efficiency and standardization of ultrasound diagnostics. He is a long-standing member of the MICCAI Society. He is one of the founders of the Medical Image Computing Seminar (MICS) in China. He served as the chairs of both Local Organization committee and Finance committee in MICCAI 2019 and endeavored to make this conference the most successful in the history of MICCAI. In January 2020, he was elected as member of MICCAI Board.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.


YE et al.: SEMI-SUPERVISED DUAL-STREAM SELF-ATTENTIVE ADVERSARIAL GRAPH CONTRASTIVE LEARNING FOR CROSS-SUBJECT 305
Fali Li received the PhD degree from the University of Electronic Science and Technology of China, in 2020. He is now an associate researcher with the School of Life Science and Technology, Center for Information in Medicine, University of Electronic Science and Technology of China, China. His research interests include brain-computer interface, bio-informatics, and neural engineering.
Peng Xu received the PhD degree in biomedical engineering from the School of Life Science and Technology, University of Electronic Science and Technology of China, in 2006. He was a postdoctoral researcher with UCLA from 2007-02 to 2009-04. He is currently a full professor with the School of Life Science and Technology, University of Electronic Science and Technology of China, China. His research interests include brain–computer interface, brain inspired intelligence, machine learning, and brain network analysis, etc.
Zhen Liang received the PhD degree from the Hong Kong Polytechnic University, Hong Kong, in 2013. From 2012 to 2017, she was an algorithm development scientist with NeuroSky, Inc., Hong Kong. From 2018 to 2019, she was a specially-appointed assistant professor with the Graduate School of Informatics, Kyoto University, Japan. She is currently an associate professor with the School of Biomedical Engineering, Health Science Center, Shenzhen University, China. Her current research interests include brain encoding and decoding systems, affective computing, visual attention, and neural engineering.
Authorized licensed use limited to: University of Electronic Science and Tech of China. Downloaded on October 15,2025 at 07:03:37 UTC from IEEE Xplore. Restrictions apply.