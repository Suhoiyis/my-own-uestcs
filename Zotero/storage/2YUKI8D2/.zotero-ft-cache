分 类 号:TP391.4 单位代码:10183 研究生学号:2022522063 密 级:公 开
吉林大学
硕士学位论文
(学术学位)
基于深度学习的视觉刺激脑电信号增强-识别算法研究
Research on Deep Learning based Visual Stimulus EEG
Augmentation and Recognition
作者姓名:黄俊洁
专 业:控制科学与工程
研究方向:模式识别与智能系统
指导教师:陈万忠 教授
培养单位:通信工程学院
2025 年 5 月


————————————————————
基于深度学习的视觉刺激脑电信号增强-识别算法研究
————————————————————
Research on Deep Learning based Visual Stimulus EEG
Augmentation and Recognition
作 者 姓 名:黄俊洁
专 业 名 称:控制科学与工程
指 导 教 师:陈万忠 教授
学 位 类 别:工学硕士
答 辩 日 期:2025 年 5 月 22 日


摘要
视觉感知过程的识别是一项在脑科学领域和人工智能领域都十分具有挑战
性的研究课题,也是实现人机交互系统中各种视觉相关任务的前提。然而,基于
脑电信号的视觉刺激解码研究存在数据稀缺性的问题,难以建立有效、高精度和
稳定的机器学习模型;同时,如何高效地从脑电信号中提取出有效特征,以获得
更好的解码性能仍然是一项十分具有挑战性的工作。针对以上问题,本文提出了
一种基于深度学习的视觉刺激脑电信号增强-识别算法,它由以下两个部分组成:
增强算法是基于深度生成模型的脑电信号数据增强框架,用于生成高质量的脑电
信号数据以提高深度学习模型的分类性能;识别算法则是结合线性注意力机制与
卷积神经网络的深度学习解码模型,可以在保持较少的模型参数量下取得最先进
的视觉刺激脑电分类性能。本文的主要研究工作总结如下:
(1)针对视觉刺激脑电解码研究中存在脑电信号数据稀缺性的问题,本文
提出了一种基于扩散概率模型的深度脑电信号生成模型,对视觉刺激脑电信号进
行高质量、多样化的生成。在斯坦福大学公开的视觉刺激脑电数据集上的实验结
果显示,使用本文提出的数据增强方法后,6 分类和 HF-IO 分类任务下识别准确
率相较于无增强方法平均提升了 9.56%和 2.57%。
(2)为了使生成的视觉刺激脑电信号尽可能接近真实,本文设计了一个基
于贝叶斯方法的超参数优化模块,对脑电信号的生成过程进行自我优化,使生成
的脑电信号的质量达到最优。
(3)针对现有解码模型参数量大,分类准确率较低的问题,本文提出了一
种结合线性注意力机制和卷积神经网络的深度学习模型。本文提出的方法在视觉
刺激脑电 6 分类、72 分类、HF-IO 分类任务下识别准确率分别为 54.13%,29.83%,
89.69%,优于现有方法的同时所需的模型参数量不到现有方法的 10%。
总的来说,本文提出的增强算法可以有效地生成高质量的脑电信号,并且明
显提升了脑电解码模型的性能;识别算法在多项分类任务中达成了最先进的分类
性能,同时模型参数量和网络计算复杂度相比于现有的方法具有极其明显的优势。
关键词:
脑机接口,视觉刺激脑电信号,深度学习模型,卷积神经网络,线性注意力
机制


Abstract
The EEG-based visual stimuli recognition is a highly challenging research topic
in both neuroscience and artificial intelligence, it’s a prerequisite for various vision
related tasks in human-computer interaction systems. However, the scarcity of EEG
data in visual stimulus decoding poses a significant challenge, making it difficult to
establish effective, high-accuracy, and stable machine learning models. Additionally,
efficiently extracting meaningful features from EEG signals to improve decoding
performance remains a formidable task. To address these issues, this paper proposes a
Deep Learning based algorithm for Visual Stimulus EEG Augmentation and
Recognition. The algorithm consists of two components: the Augment algorithm is
composed of a deep generative model-based framework for EEG data augmentation
that generates high-quality EEG signals to improve the classification performance of
deep learning models. The Recognition algorithm consists of a deep learning decoding
model that integrates a linear attention mechanism with convolutional neural networks
(CNNs), which achieves state-of-the-art classification performance for visual stimulus
EEG decoding tasks while maintaining a reduced number of model parameters. The
main contributions of this paper are summarized as follows:
(1) To address the issue of data scarcity in EEG signals for visual stimulus
decoding research, this paper proposes a deep generation model based on diffusion
probabilistic models, which enables high-quality and diverse generation of visual
stimulus EEG signals. Experimental results on the publicly available visual stimulus
EEG dataset from Stanford University show that, after applying the data augmentation
method proposed in this paper, the recognition accuracy was improved by an average
of 9.56% for the 6-classification task and 2.57% for the HF-IO classification task
compared with methods without data augmentation.
(2) To ensure the generated EEG signals closely resemble real signals, we designed
a Bayesian Method-based Hyperparameter Optimizer that self-optimizes the EEG
signal generation process, achieving optimal quality for the generated EEG signals.
(3) To overcome the challenges of large model parameters and low classification
accuracy in existing decoding models, we propose a novel deep learning model that
combines linear attention mechanism and convolutional neural network. Compared
with the existing best methods, the proposed method in this paper achieves decoding
accuracy of 54.13%, 29.83%, and 89.69% in the 6-classification task, 72-classification


task, and HF-IO classification task respectively. It outperforms existing methods while
requiring less than 10% of the model parameters of current approaches.
In general, our proposed Augment algorithm effectively generates high-quality
EEG signals and significantly improves the performance of various open-source deep
learning EEG decoding models. Additionally, the Recognition algorithm achieves state
of-the-art performance in multiple classification tasks while maintaining significant
advantages in terms of parameter count and network complexity compared with
existing methods.
Key Words:
Brain-computer interface, EEG-based visual stimuli, deep learning models,
convolutional neural networks, linear attention mechanism


目录
第 1 章 绪论...............................................................................................1
1.1 课题研究背景及意义.......................................................................1
1.2 脑电信号的特征和分类...................................................................3
1.3 研究现状及存在问题.......................................................................6
1.3.1 脑电分类研究现状及存在问题.................................................6
1.3.2 脑电增强研究现状及存在问题.................................................8
1.4 论文研究内容与结构安排 ..............................................................9
第 2 章 相关深度学习模型及实验数据 ................................................13
2.1 深度解码模型.................................................................................13
2.1.1 卷积神经网络...........................................................................13
2.1.2 Transformer ................................................................................17
2.2 深度生成模型.................................................................................20
2.2.1 生成对抗网络...........................................................................20
2.2.2 变分自动编码器.......................................................................21
2.2.3 扩散模型...................................................................................23
2.3 数据集介绍.....................................................................................25
2.4 本章小结.........................................................................................26 第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法 ....27
3.1 整体方法框架.................................................................................27
3.2 生成过程.........................................................................................28
3.2.1 AV-DPM 的结构 ........................................................................28
3.2.2 AV-DPM 的前向扩散过程 ........................................................29
3.2.3 反向生成过程中的最优方差上界...........................................29
3.2.4 反向生成过程中的自适应方差...............................................30
3.2.5 AV-DPM 的反向生成过程 ........................................................31
3.3 优化过程.........................................................................................32
3.3.1 相似性度量网络.......................................................................32
3.3.2 基于贝叶斯方法的超参数优化器...........................................33
3.4 解码过程.........................................................................................35
3.5 实验结果.........................................................................................35
3.5.1 实验条件与超参数设置...........................................................35
3.5.2 实验结果...................................................................................35
3.5.3 生成质量分析...........................................................................38
3.5.4 方法对比...................................................................................40


3.6 分析讨论.........................................................................................43
3.6.1 中心核对齐分析.......................................................................43
3.6.2 t-SNE 可视化分析.....................................................................44
3.6.3 Grad-CAM 可视化分析 ...........................................................45
3.7 本章小结.........................................................................................46 第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型 ............47
4.1 整体方法框架.................................................................................47
4.2 卷积编码器.....................................................................................48
4.3 线性注意力模块.............................................................................50
4.3.1 门控注意力单元.......................................................................51
4.3.2 混合注意力模块.......................................................................52
4.4 分类器模块.....................................................................................53
4.5 实验结果.........................................................................................54
4.5.1 实验条件与超参数设置...........................................................54
4.5.2 实验结果...................................................................................54
4.5.3 消融实验...................................................................................57
4.5.4 结合数据增强框架 SAD-VER 的解码效果 .........................58
4.6 分析讨论.........................................................................................60
4.6.1 中心核对齐分析.......................................................................60
4.6.2 Grad-CAM 可视化分析 ...........................................................64
4.7 本章小结.........................................................................................65
第 5 章 总结与展望 ................................................................................66
5.1 总结.................................................................................................66
5.2 展望.................................................................................................67
附录...........................................................................................................69
A 章节 3.2.3 中公式(3.7)的数学证明过程........................................69
B 真实的 EEG 地形图与 SAD-VER 生成的 EEG 地形图展示..72
参考文献...................................................................................................74


第 1 章 绪论
1
第 1 章 绪论
1.1 课题研究背景及意义
人工智能是计算机科学的新兴分支,主要研究如何使机器能够模拟人的意识、
思维、情感等信息处理过程,最终目标是生产出一种具有类似于人类智能的机器,
进一步实现对人类智能的延伸和扩展[1]。随着人工智能算法的进步与计算芯片的
日益高效,人工智能领域取得了令人惊叹的发展。人机交互(Human-Computer
Interaction, HCI)[2]作为扩展人类智能边界的一个重要课题,已成为人工智能研
究领域的热点。其中,结合脑科学这一科技领域明珠的人机交互系统——脑机接
口(Brain-computer Interface, BCI)[3],是现今的顶级研究方向。脑机接口充当着
将人脑的电信号转换为计算机命令的桥梁,它是实现通过思维控制各种设备,实
现真正意义上的人机交互的关键技术。该系统的底层基础是脑机接口技术,而上
层结构则由计算机和其他硬件设备组成,二者共同构成了完整的脑机接口系统。
20 世纪以来,全球多国相继启动了各自的“脑研究计划”[4]。人类对大脑的认知
不断深化,研究范围从大脑的宏观结构扩展到微观层面,探索的焦点从大脑功能
区域逐步转移到神经通路,脑科学现已成为跨足医学、计算机科学、心理学等多
个学科的前沿研究领域,脑机智能的研究正迅速在全球各地兴起[5]。
自 2013 年起,美国便提出脑机接口技术具有广泛应用前景,并启动了“大
脑研究通过推进创新神经技术计划(Brain Research through Advancing Innovative
Neurotechnologies,BRAIN)” 。这项计划旨在通过加速创新技术的开发和应用,
彻底改变人类对大脑的理解。2019 年发布的中期战略报告中提出了 BRAIN 2.0
计划,进一步明确了未来的研究方向,包括构建全面的人类大脑细胞图谱、绘制
完整的哺乳动物大脑微连接图谱等[6]。截止至 2024 年,美国国立卫生研究院已
在该研究计划中投入至少 24 亿美元,预计到 2026 年后总投资将超过 50 亿美元。
在美国启动 BRAIN 计划的同年,欧盟成立了“脑的十年委员会”和脑研究
联盟,并公布了欧盟人类脑计划(Human Brain Project,HBP)。HBP 由近 500 名
科学家参与,历时长达 10 年,耗资约 6 亿欧元,其宏大目标是通过在计算机里
建立大脑模型,以探索其奥秘。2020 年以来,科学家们在 HBP 的资助下,于脑
神经科学领域取得了多项重大进展,例如创建了至少 200 个脑区的详细三维地形
图[7],开发了用于治疗失明的脑植入物[8],使用超级计算机对记忆和意识等功能
进行建模,并推进了各种大脑疾病的治疗[9]。
2014 年,日本正式开启“脑计划(Brain/MINDS)”,目标是通过结合多种神
经技术,在灵长类动物上研究大脑发育和脑疾病的成因,来克服以往在理解人类
神经生理机制方面的不足[10]。2018 年,日本推出 Brain/MINDS Beyond 计划,旨


第 1 章 绪论
2
在扩展 Brain/MINDS 的成果,加强国际合作,推动神经科学的进一步发展[11]。
2024 年,日本文部科学省启动了“脑与神经科学整合计划”,该项目将持续约 6
年,旨在通过数字化方式再现人脑结构,开发痴呆症和抑郁症等脑神经相关疾病
的治疗方法。
我国自 2009 年发起脑认知领域相关研究计划,名为“脑科学与类脑研究计
划”[4]。2021 年,中国脑计划正式启动,国家拨款经费预算近 32 亿元,后续整
体投入经费规模可达千亿元级别,该计划以“理解脑、修复脑、模拟脑”为目标,
旨在通过多尺度研究促进脑认知原理的解析,并为脑疾病和类脑计算提供理论基
础[12]。近两年,我国的脑科学和类脑研究取得了显著进展。例如,清华大学医学
院团队设计研发的无线微创脑机接口 NEO 在 2024 年成功进行了临床植入试验,
旨在帮助瘫痪人士恢复手部运动功能[13];清华大学类脑计算研究中心团队研制
出世界首款基于原语的类脑互补视觉芯片“天眸芯”。该芯片借鉴人类视觉机制,
实现了高速、高精度、高动态范围视觉感知,并在自动驾驶复杂开放道路展示了
优异的性能[14]。
图 1.1 清华大学研究团队研制出类脑互补视觉芯片“天眸芯”
对视觉刺激的脑电信号的深入探索不仅有助于人类的视觉感知研究,认知科
学研究和神经疾病诊断,基于它的分类任务更是在人们的日常生活中随处可见,
是实现人机交互系统中各种视觉相关任务的前提。例如,通过视觉刺激脑电信号
评估用户对视觉界面、设计风格、广告元素等视觉任务的反应,以优化产品设计;
利用视觉刺激脑电信号优化虚拟现实和增强现实中的视觉元素,提升用户体验;
监测和评估视觉训练程序的效果,又或是弱视治疗和针对色盲患者的图形开发;
评估驾驶员或操作员的视觉注意力状态,预防疲劳驾驶或操作失误。视觉刺激脑
电的研究对人们追求更高智能,更多互动以及更加便捷的生活发挥着重要作用。


第 1 章 绪论
3
图 1.2 基于视觉脑电信号的人机交互系统
视觉感知过程的识别是一项在脑科学领域和人工智能领域都十分具有挑战
性的研究课题。在视觉场景识别方面,尽管基于卷积神经网络的方法和各种新式
的视觉模型取得了惊艳的效果,但与人类视觉水平相比,计算机视觉的发展仍然
还有很长的路要走。本文研究视觉刺激与脑电信号的相关性,用先进的人工智能
方法探索高质高效的视觉刺激脑电增强-识别方法,目的是为人机混合-增强智能
的研究带来新的见解,帮助构建新型脑机智能技术体系,进一步促进人机交互系
统的发展。研究内容隶属于“类脑计算与脑机智能技术及应用”方向。
1.2 脑电信号的特征和分类
大脑是一个令人敬畏的复杂系统,由大脑、脑干和小脑组成。作为人类神经
系统的中枢,大脑与脊髓共同构成中枢神经系统。数以千亿的神经元在不同层级
上相互连接与作用,构建起一个庞大而复杂的神经网络。大量的信息在这个交织
密布的大尺度网络中传递,支配着思维、情感、记忆、学习、身体机能等,使人
类产生自我意识。大脑不同区域负责的功能有所不同,其功能分布如图 1.3 所示,
不过总的来说大脑控制着身体的大部分活动,并做出决定将指令发送到身体其他
部位。脑电信号的产生与大脑中神经元的集体活动密切相关。当神经元放电时,
它们会产生动作电位变化,这些电位变化沿着神经元轴突在大脑中传播并相互叠
加。单个神经元的动作电位是非常微弱的,但当大量神经元几乎同时放电时,它
们产生的电位可以相互叠加,形成足够强的电场,从而在头皮上被 EEG 电极检
测到。


第 1 章 绪论
4
图 1.3 大脑功能分布图
人类对物体的感知和识别能力很大程度上依赖于视觉能力和大脑的认知能
力。当眼睛受到视觉刺激时,大脑可以在不到 200 毫秒的时间内从视觉刺激中提
取出信息[15],并产生神经尖峰[16]。神经科学中的研究指出,视觉信息在大脑的枕
叶区进行处理,而物体相关的特征和物体之间的空间关系信息则分别在大脑的腹
颞叶流和背顶叶流进行处理[17];而进一步的研究表明大脑枕部区域的频段的
变化与视觉刺激类型有关[18];同时,根据呈现视觉信息的语义类别不同[19],上述
的大脑区域被优先激活的顺序也有所不同[20]。
脑电信号相比于其他模拟信号具有多种特点,具体如下:
(1) 低幅值:脑电信号的幅度通常在微伏级别,反映了神经元放电的强度。
(2) 非线性:脑电信号实质上是数百万个脑神经元同时活动的结果,而由于
神经细胞具有非线性耦合的特性,脑电信号具有非线性和混沌特性。
(3) 非平稳性:脑电信号受认知状态、情绪和生理条件等多种因素影响。并
且,不同的脑区在不同的时间点可能表现出不同的信号模式。
(4) 低信噪比:由于脑电信号非常微弱,它非常容易受到各种噪声的干扰,
包括肌电、眼动、心电干扰等,使得采集到的脑电信号信噪比较低。
(5) 个体差异性:不同个体的脑电信号特征可能存在显著差异,这可能与遗
传、大脑结构和功能的差异有关。同时,对于不同个体,脑电信号的特征可以随
着学习和经验而改变。
(6) 导联相关性强:相比于其他模拟信号,脑电信号在空间上也具有分布特
征。由于其空间特征以多导联的形式呈现,因此单个电极采集到的数据实际上是
多源信号,是各脑区共同作用的结果,而并非独立存在。
(7) 具有多频段特征:脑电信号的频率范围通常在 0.5Hz 到 100Hz 之间。不
同的频率带与大脑的不同状态和功能有关。学理上,根据频率的差异将脑电信号
划分成以下几种类型,频率从低到高分别为 频段、频段、频段、频段和


第 1 章 绪论
5
 频段。每一频段都具有不同的特性,反映了机体生理和心理状态的不同方面。
例如,图 1.4 展示了上海交通大学团队使用喜剧和悲剧等不同的电影片段诱发并
采集受试者的情绪脑电信号,并且根据 、、、和五个频段对情绪脑电
信号进行划分。
图 1.4 上海交大团队使用不同的电影片段诱发并采集受试者的情绪脑电信号
 频段:频率在 0.5Hz 到 4Hz 之间,大脑激活区域为额叶和颞叶部位,通常
它与深度睡眠状态相关。在深度麻醉或者婴幼儿智力发育不完全时期该频段也会
出现。
频段:频率在 4Hz 到 8Hz 之间,大脑激活区域为顶叶和颞叶部位。它与轻
度睡眠、放松和某些类型的冥想状态有关。它也是青少年的主要脑电波成分,但
当成年人患有某些精神疾病时,波也会表现出明显活跃的状态[21]。
频段:频率在 8Hz 到 13Hz 之间,大脑激活区域为顶、枕部位。它与大脑
的静息状态有关。它也是正常成年人脑电波的重要成分。
频段:频率在 13Hz 到 30Hz 之间,大脑激活区域在额叶、前中颞叶和中


第 1 章 绪论
6
央区,与大脑清醒、警觉和专注的状态有关。尤其是当机体精神紧张或者情绪激
动时,该频段将明显活跃。
频段:频率在 30Hz 以上,大脑激活区域在额叶和中央区。它与认知处理、
注意力集中和信息整合等高级神经功能有关。
脑电信号按照不同的任务范式,又可以分为以下类别:
(1)自发脑电信号和诱发脑电信号
自发脑电记录了在无特定外交刺激时大脑的自然放电状态,它基于特定思维
形成而与外界刺激无关,表征了机体自身的脑电活动,因而具有强烈的平稳性,
例如运动想象脑电信号。诱发脑电信号记录了视觉、触觉、听觉等外部刺激所诱
发的的大脑瞬时活动的变化,例如本文研究的视觉刺激脑电信号。
(2)主动脑电信号和被动脑电信号
主动脑电信号要求受试者根据实验要求完成特定的任务或者保持某些特定
状态,本文研究的视觉刺激脑电信号属于此类。而被动脑电信号不需要受试者主
动进行特定任务或者保持特定状态,例如基于脑电的睡眠监测或者情感测评。
(3)任务相关脑电信号和背景脑电信号
任务相关脑电信号(Event-related Potential,ERP)是受试者在进行特定任务
过程中大脑的活动数据,它是在复杂刺激下产生的,与高级认知处理有关,因而
具有良好的心理测量特性,不同的 ERP 成分对应不同的认知过程[22]。背景脑电
信号(Background Potential,BP)是非任务状态下大脑的活动数据。一般使用去
基线方法或者去平均法去除背景脑电数据[23]。
1.3 研究现状及存在问题
1.3.1 脑电分类研究现状及存在问题
随着脑机接口技术的深入发展,基于脑电信号的分类任务大致分为四类,分
别是意志过程、医疗应用、情感识别以及外部刺激[24-29]。其中,意志过程任务可
以进一步细分为运动想象任务和精神疲劳监测任务。运动想象任务允许受试者通
过想象身体部位的运动来产生脑电波,这些信号可以用来指挥外部机械装置的动
作。而精神疲劳监测任务则是通过分析脑电波来评估受试者的疲劳程度。医疗应
用任务则使用患者的脑电波来诊断和预防各种脑部和精神相关疾病,如癫痫、阿
尔茨海默症和睡眠障碍等病症。情感识别任务则是通过分析受试者在特定情绪状
态下的脑电波,来识别和理解他们的情感状态。外部刺激任务则包括各种声、光
在内的外部刺激所诱发的特定脑电信号,本文所研究的视觉刺激脑电亦属于此类
别,对视觉刺激诱发的脑电信号进行分析和识别,并中获取到人眼的视觉信息,
从而落实和完善在实际的应用场景中。脑电分类任务的研究与人类日常生活息息


第 1 章 绪论
7
相关,其具备极大的应用潜力。然而,各式脑电信号的采集场景和应用任务均有
不同,因此需要针对特定场景应用特定的脑电信号特征提取方法。
图 1.5 杭州电子科技大学团队提出基于脑电信号的驾驶疲劳监测系统
传统信号分析方法主要有小波变换(Wavelet Transform, WT)[30]、共空间模
式(Common Spatial Pattern, CSP)[31]、快速傅里叶变换(Fast Fourier Transform,
FFT)[32]等相关方法,用来分析脑电信号,提取时域和频域特征。传统分类模型
主要有 k-最近邻分类(K-Nearest Neighbor, KNN)[33]、随机森林(Random Forest,
RF)[34]、线性判别分析 (Linear Discriminant Analysis, LDA)[35]和支持向量机
(Support Vector Machine, SVM)[36]等。2008 年,华盛顿大学与微软的实验室合
作,发布了一项融合了视觉刺激脑电信号分析技术与基于金字塔匹配核(PMK)
的视觉类别识别系统,用于进行视觉分类任务,在动物、无生命物体和人脸的三
类分类任务中实现了超过 91%的分类准确度[37]。2012 年,Wang 等人[35]通过使用
ERP 组件和 Fisher-LDA 提取试验数据中的脑电信号特征,实现了四类视觉对
象的脑电信号分类。2015 年,黎巴嫩大学研究者利用 SVM 从 256 导联高密度
的脑电信号中分离出动物和非动物两类别图像,分类准确率为 82.7%[38]。Babu 等
人[39]利用一种基于快速傅里叶变换的方法对睡眠脑电信号进行分类,准确率高
达 98.62%。2022 年,Wang 及其团队[40]运用航空影像和脑电图技术对景观进行
辨识与评估。他们收集了观看不同景观类型视频时受试者产生的脑电信号。通过
频域分析,提取了,,, , 五个频段的功率谱密度(PSD)和差分熵(DE)作
为特征。接着,他们利用四种不同的分类算法对七种不同的景观类型进行了分类。
实验结果显示,SVM 和 RF 这两种分类器在识别景观类型时表现出最高的准确
度。尽管上述工作表明利用传统信号分析方法对脑电信号数据实现视觉解码是可
行的,然而传统方法依赖于精心预处理的数据,在面对低信噪比、模式复杂的脑
电数据时显得力不从心。此外,利用传统方法进行解码的准确率仍有较大提升空


第 1 章 绪论
8
间。
近年来,深度学习技术逐渐成熟,它具有无需人工提取特征的优点[41],以及
优秀的分类性能[42],可以承担图像、文本等数据的解码任务[43],其性能已经取代
了替代传统的手动特征提取器和分类器[44]。深度学习框架因其优异的性能,在解
码脑电信号方面也得到了非常广泛的应用。主流使用的深度学习技术有卷积神经
网络( Convolutional Neural Networks,CNNs )、深度信念网络( Deep Belief Networks,
DBNs )和循环神经网络( Recurrent Neural Networks,RNNs )等架构[45]。例如,Pan
等人[46]提出一种基于卷积神经网络的残差密集编码器,用于学习脑电信号与视
觉刺激图像之间的分布特征;Song 等人[47]利用轻量级卷积网络,提取运动想象
脑电信号的时空-频域特征以学习广泛的数据分布;Tao 等人[48]提出一种结合了
通道注意力的卷积-递归神经网络,在 DEAP 情感脑电数据集的唤醒和效价分类
任务中达成了 93.38%和 93.72%的准确率。
目前,在自然语言处理领域取得优异性能的,采用多头注意力机制( MHA )
的 Transformer 网络[49]也被引入到脑电信号系统的研究中。它使用 Attention 机制
以获取脑电信号的全局特征,突破了传统卷积神经网络只能提取局部特征的局限
性,弥补了循环神经网络及其衍生模型对于长序列处理效果较差的缺点,为脑电
信号的研究提供了新的见解。例如,Cai 等人[50]提出了一种带有掩蔽机制的
Transformer 模型,通过随机掩蔽信号进行预训练,并迫使模型学习运动想象脑电
信号的语义特征;Li 等人[51]利用基于 Transformer 的深度学习模型,从原始 EEG
数据中学习全局时间-频率特征并进行疲劳识别;Yao 等人[52]使用卷积神经网络
和 Transformer 同时提取情绪脑电信号的空间特征和频域特征,在多个开源数据
集上取得了先进效果;Roozbehi 等人[53]在基于脑电信号的癫痫监测任务中对多
种 Transformer 架构进行了评估,结果显示它们的性能比现有的一维/二维卷积神
经网络更好。
然而,目前在脑电信号解码研究中采用的 Transformer 中的多头注意力机制
几乎都具有二次计算复杂度,这也在一定程度上制约了多头自注意机制在复杂的
脑电信号解码任务中的应用。因此,我们希望在保持卷积神经网络的局部感知优
势与自注意力机制的全局感知优势的前提下,设计出一种具有更低计算复杂度,
同时解码性能更好的基于脑电信号的视觉刺激分类模型。
1.3.2 脑电增强研究现状及存在问题
脑电信号作为来自数百万个神经元活动的非平稳聚合信号,具有较低的信噪
比(SNR)与较强的特异性[45];此外,脑电信号数据具有采集成本高昂、数据标
注困难的特点,一个典型的脑电信号数据集通常只包含几百到几千个样本,数据
量较小。为了克服这些难点,现有的脑电信号研究中广泛地应用了数据增强技术。
数据增强技术被认为是解决数据稀缺性问题的一种技术,能够有效提高分类


第 1 章 绪论
9
或回归任务的准确性和稳定性。它可以缓解深度学习模型性能依赖于大量数据的
问题[54],提高深度学习模型在脑电信号研究领域的普适性与可迁移性。在传统的
脑电信号数据增强方法的研究中,Krell 和 Su 等人[55]使用类似于图像的仿射/旋
转畸变来生成增强的脑电信号数据;Lotte 等人[56]基于原始脑电信号试验的排列
组合和畸变提出了三种增强脑电图的方法;Deiss 等人[57]与 Saeed 等人[58]利用大
脑的对称性,随机删除、打乱或交换大脑左右两侧的电极数据以生成增强脑电信
号数据;Schwabedal 等人[59]使用 FT-替代变换(FT-surrogate transform),用特定
范围内的随机数代替脑电信号数据中傅里叶系数的相位;Wang 等人[60]通过在原
始脑电图特征中加入不同标准差的高斯噪声来生成新的特征。尽管这些传统的数
据增强方法原理简单并且得到了广泛的应用,但它们的本质是对已有的脑电信号
信号进行线性变换或者排列组合,而并非在保持了原有脑电信号样本数据分布的
条件下生成全新的、高质量的脑电信号样本。因此,它们并不能从根本上解决脑
电信号解码研究中数据量不足的问题。
相比于上述的传统脑电信号数据增强方法,最近的脑电信号增强研究在深度
生成式模型的帮助下取得了显著的改进,最典型的当属以生成对抗网络
(Generative Adversarial Networks,GAN)[61]为代表的一系列脑电信号增强研究。
例如,Meng 等人[62]通过引入 GAN 来提高情绪脑电信号分类的准确性;Abou 等
人[63]使用由双向 LSTM 网络(Bi-LSTM)设计的,带有梯度惩罚项的 Wasserstein
GAN(WGAN-GP)来对癫痫脑电信号进行增强;Pan 等人[64]提出一项可扩展时
间窗长度的 GAN,用于生成大脑短时产生的视觉稳态诱发电位信号(SSVEP);
Cheng 等人[65]设计了一项基于 GAN 的深度生成式模型,用于解决在基于脑电信
号的睡眠阶段分类任务中,脑电信号样本数量不平衡的问题。
然而,现有的深度学习驱动的脑电信号增强方法大多采用 GAN 进行脑电信
号生成,而 GAN 有着不易收敛,难以训练的缺点。此外,基于 GAN 的模型一
经训练完成,其生成过程是不可控的,因而很难稳定地控制脑电信号的生成质量
与多样性。近期,扩散概率模型[66]及其衍生模型在计算机视觉领域取得了令人印
象深刻的性能,其具有易于训练、生成过程可干预的优点,并且在多项生成任务
中表现出超越 GAN 的性能[67]。然而,扩散概率模型在基于视觉刺激的脑电信号
数据增强领域的研究仍然较少,如何稳定地生成高质量和多样化的脑电信号样本
值得进一步探索。
1.4 论文研究内容与结构安排
综上所述,现有的基于脑电信号的视觉解码研究存在以下几个问题:
(1) 数据量不足:视觉刺激脑电信号采集困难,标注成本高,导致在视觉解
码研究中可用的脑电信号数据量不足。


第 1 章 绪论
10
(2) 生成数据质量不稳定:现有的深度学习驱动的脑电信号增强方法训练困
难,生成过程不可控,因而难以稳定地生成高质量的脑电信号数据。
(3) 识别模型参数多:在脑电信号解码研究中采用的 Transformer 中的多头
注意力机制几乎都具有二次计算复杂度与较大的模型参数量,这制约了注意力机
制在复杂的脑电信号识别任务中的应用。
本文针对上述问题,以视觉信息的编码和解码为研究视角,利用先进的扩散
式生成模型对脑电信号进行增强,以自监督学习的方式提高基于脑电信号的视觉
刺激解码性能。在此基础上,深入挖掘深度神经网络在数据分析方面的潜力,对
现有的深度学习网络模型进行优化,提升脑电信号的分类精度的同时降低模型的
计算复杂度和参数量,为脑机智能融合和人机交互系统提供有价值的研究结果。
同时,利用各种可视化手段和实验进一步分析本文提出的方法能够成功提升脑电
信号的识别性能的原因,深入探究本文所提出的方法的优越性,为脑科学和人工
智能的研究提供新的见解。具体而言,本文的研究内容主要如下:
(1) 本文将自监督学习与扩散式模型相结合,并在扩散式模型中引入自适应
方差与最优方差上界,对视觉刺激脑电信号进行高质量、多样化的生成,解决了
视觉刺激脑电信号研究中数据量不足的问题。
(2) 为了使生成的视觉刺激脑电信号尽可能接近真实,本文设计了一个基于
贝叶斯方法的超参数优化模块,对脑电信号的生成过程进行动态调节,使生成的
脑电信号的质量达到最优。
(3) 针对现有解码模型参数量大,识别准确率较低的问题,本文提出了一种
新颖的,结合线性注意力机制和卷积神经网络的深度学习模型。它有效地利用了
卷积神经网络局部感知能力的优势与注意力机制全局感知能力的优势,并且相比
于现有方法在模型在参数数量和网络复杂度方面具有极其明显的优势。
总的来说,本文对视觉信息编解码与脑电信号的关系进行研究,实现高效的
视觉脑电信号数据增强与识别。本文整体的技术路线如图 1.6 所示。
在结构安排方面,本论文的研究内容共分为五个章节,其中每个章节分别讨
论的研究内容如下:
第一章:绪论。首先介绍了课题的研究背景和意义。其次,介绍脑电信号的
产生原理、基本特征和类型。然后,介绍脑电信号视觉解码的研究现状和脑电信
号数据增强的研究现状。最后简要总结了论文研究内容并给出论文的结构安排。
第二章:相关深度学习模型及实验数据。首先介绍了用于脑电数据解码的深
度学习模型。然后对目前流行的深度生成模型进行了简要的原理介绍,并总结了
它们各自的特点。最后介绍了本文实验探究部分采用的视觉刺激脑电数据集。
第三章:基于自适应方差扩散模型的视觉刺激脑电信号增强方法(SAD
VER)。首先介绍了 SAD-VER 的整体方法框架,它分为三个部分:生成过程,


第 1 章 绪论
11
优化过程和解码过程。其次对 SAD-VER 的生成过程部分采用的网络 AV-DPM 进
行了详细介绍。接着,对 SAD-VER 的优化过程进行了介绍,优化过程是一个基
于贝叶斯方法的超参数优化器。然后简要介绍了 SAD-VER 的解码过程中使用的
神经网络结构,最后给出 SAD-VER 的实验结果并进行了深入的实验分析,以探
究它具有优秀性能的原因。
图 1.6 本文整体技术路线
第四章:基于线性注意力机制的视觉刺激脑电信号解码模型(LACNN)。首
先给出了 LACNN 的整体方法框架并简要给出了对脑电信号的预处理过程。其
次,对 LACNN 的浅层卷积编码器进行了介绍。然后,详细介绍了 LACNN 的线
性注意力模块的组成,它采用了一种混合分块的注意力机制,以降低计算复杂度


第 1 章 绪论
12
并缩减模型参数。最后给出 LACNN 的实验结果和可视化分析,深入探究它的作
用机制。
第五章:总结与展望。总结本文研究的优势以及其局限性,结合当前人工智
能和脑科学的发展需求,结合以卷积神经网络和注意力机制为代表的深度学习解
码模型和以扩散式模型为代表的生成式模型,对本课题的后续发展前景以及未来
的研究激活进行了展望。
论文剩余部分为:参考文献、作者简介及研究成果介绍、和致谢。


第 2 章 相关深度学习模型及实验数据
13
第 2 章 相关深度学习模型及实验数据
2.1 深度解码模型
本文通过使用深度学习技术对视觉刺激 EEG 信号进行解码与分析,以探究
脑电信号与人类视觉之间的关系。在设计论文实验内容之前,本节介绍了深度学
习领域中,两种在本文中用于解码任务的基本网络结构。这些网络结构为本文的
后续研究奠定了理论和技术基础。
2.1.1 卷积神经网络
卷积神经网络(Convolutional Neural Network, CNN)是一种专门用于处理
具有网格结构数据的深度学习模型,最常用于图像和视频的分析。CNN 通过其
特殊的层级结构,有效地捕捉图像中的空间和局部相关性。CNN 的层次性与对
局部特征敏感的特性对于提取图像局部相关特性的研究发挥了重要作用。例如,
在面部识别过程中,第一个神经层检测线条、边缘和角落等低水平特征,中间神
经层将它们组合在一起检测眼睛、鼻子和耳朵等主要特征[68]。而复杂的神经层则
用以检测如面部纹理、结构等高复杂度特征。
CNN 层次化结构的灵感来自于 1959 年 Hubel 和 Wiesel 的研究[69]。他们
在猫大脑的视觉系统研究中发现,当视觉皮层处理视觉刺激时,感受野中特定区
域会激活不同的皮层细胞,这一特性奠定了 CNN 的研究基础。1980 年,日本科
学家福岛邦彦提出了一种包含卷积层和池化层的神经网络结构,即 Neocognitron
模型[70]。1998 年,Yann LeCun 发表了 LeNet-5 模型用于手写数字识别[71],这是
CNN 的早期形式。但是,受限于当时计算力的不足,LeNet 训练较慢,且还有一
些其他算法(比如 SVM)能达到类似或者更好的效果,基于 CNN 的方法并没有
真正流行。2006 年,MIT 脑科学研究中心研究并且提出了详细的 CNN 权值更新
的公式[72],进一步完善了 CNN 的理论基础。
2012 年,Alex Krizhevsky、Geoffrey Hinton 等人提出的 AlexNet[73]模型在
ImageNet 竞赛中取得突破性成果,标志着深度学习在计算机视觉领域的复兴。此
后,CNN 的发展进入了快车道,出现了多个里程碑式的模型,包括但不限于 2013
年的 ZFNet[74],通过使用可视化技术揭示了神经网络各层的工作原理;2014 年
的 VGGNet[75],通过使用更小的卷积核和更深的网络结构提高了性能;2015 年
的 ResNet[76],引入残差连接解决了深层网络训练中的梯度消失问题;2019 年的
EfficientNet[77],通过网络缩放方法提高了模型的性能和计算效率;以及 2020 年
的 Vision Transformers(ViT)[78] ,将 CNN 的设计经验结合 Transformer 架构并
应用于计算机视觉任务,取得了与 CNN 相当的性能。
CNN 具有几大特性:局部感知性、权值共享性、仿射不变性。局部感知性


第 2 章 相关深度学习模型及实验数据
14
指的是 CNN 中的卷积核仅能够对有限感受野的输入做出响应,可以更好地捕捉
局部特征。权值共享性指的是整个输入图片共用一个卷积核,卷积核在输入上慢
慢滑动,在不同区域使用相同的参数,大大减少了参数的数量,提高计算效率。
仿射不变性指的是由于池化操作的存在,CNN 网络中低阶的特征可以逐步组合
抽象成更高阶的特征,使得其对图像缩放、平移和旋转等具有一定的不变特性。
基于以上特性,CNN 极其适用于处理具有网格结构的数据,如图像(二维网格)
和视频(三维网格),是当前计算机视觉领域的主流技术之一。
典 型 的 卷 积 神 经 网 络 结 构 包 括 卷 积 层 ( Convolutional Layer )、 激 活 层
(Activation Layer)、归一化层(Normalization Layer)、池化层(Pooling Layer)、
随机失活层(Dropout Layer)、全连接层(Fully Connected Layer),结构图如图 2.1
所示。
图 2.1 卷积神经网络的典型结构图
(1)卷积层
卷积层是卷积神经网络(CNN)中的核心组成部分,它负责从输入数据中提
取特征。它由多个卷积核组成,每个卷积核都是一个小的矩阵,用于在输入数据
上滑动以进行卷积操作。每个卷积核只覆盖输入数据的一小部分,这使得网络能
够捕捉局部特征。同时,所有卷积核共享相同的权重,这意味着无论滤波器在输
入数据上的位置如何,它们都使用相同的参数进行计算。在卷积操作中,每一个
卷积核都会生成一个特征图(Feature Map),将特征图进行堆叠,就得到了输入
数据的不同特征。使用多层卷积层,就可以得到输入数据的高阶特征用于信号处
理。以二维卷积为例,卷积操作可以表示如下:
1, 1 , 11
()
ST
kk
c
st c
ConvOutput f x w b
   
  

  .................(2.1)
其中 x 是卷积层的输入, w 是卷积核,和  代表当前卷积核矩阵的第行 
列, kS 和 kT 是卷积核的宽和高, s 和 t 代表卷积核在输入 x 上滑动了 s 行 t 列, c
指定了卷积核的个数, f 是非线性的激活函数。在主流的深度学习框架中,除了
指定卷积核大小、输入通道数和输出通道数,卷积操作还额外拥有 Padding 和
Stride 两个参数,用于控制输出特征图的大小。Padding 指定输入数据边缘填充 0


第 2 章 相关深度学习模型及实验数据
15
的层数,Stride 则指定卷积核在输入数据上的移动步长。以二维卷积为例,假设
卷积核大小为 (M , N ) ,卷积核个数(输出通道数)为 C ,Padding 数为 P ,Stride
数为 S ,输入数据尺寸为 ( , )
WH
Input Input , 则 输 出 特 征 图 的 大 小 为
(, , )
WH
C Output Output ,其中 W
Output 和 H
Output 的计算公式如(2.2)所示:
( 2 )/ 1
( 2 )/ 1
WW
HH
Output Input M P S
Output Input N P S
   
     

...................(2.2)
(2)激活层
激活层在卷积神经网络中扮演着至关重要的角色。它们通常位于卷积层和池
化层之后,它模拟了生物神经系统的刺激传输过程,用于引入非线性,使得网络
能够学习更加复杂的特征。由于卷积层和池化层本质上是线性操作,激活层可以
使得网络能够模拟非线性关系,从而增强网络的表达能力。另外,在深度网络中,
梯度可能会在反向传播过程中变得非常小,导致网络难以训练。而引入激活函数
有助于缓解这个问题。常用的激活函数包括 Sigmoid 函数,Tanh 函数,ReLU 函
数,ELU 函数等。
Sigmoid 函数常用于二分类问题。它是连续可微的,能将输入映射到(0,1)
之间。然而,当输入趋于无穷大或者无穷小时,Sigmoid 函数的导数值趋于 0,
这可能会导致反向传播过程中出现梯度消失现象,导致网络难以训练。
ReLU 函数当输入大于 0 时输出输入值,否则输出 0。它缓解了 Sigmoid 函
数中的梯度消失问题,并且计算简单,训练速度快,是目前最流行的激活函数之
一。然而,出现异常输入时,ReLU 函数在反向传播中会产生大的梯度,这种大
的梯度会导致神经元死亡和梯度消失。
ELU 函数在所有点上都是连续的和可微的,其函数图像和 ReLU 很相似。但
是相比于 ReLU,它允许输出值为负,因而可以避免像 ReLU 函数中由于异常输
入导致的神经元死亡问题。使用 ELU 可加快神经网络的收敛,缩短训练时间并
提高准确度。
(3)归一化层
随着网络层数的加深,激活函数的饱和性将导致网络梯度消失。归一化层的
作用就在于将特征值限制在非饱和区中,同时保留它们之间的大小关系。因而它
可以解决梯度消失现象,同时加快收敛速度。在卷积神经网络中,常常使用批量
归一化(Batch Normalization,BN)技术,其处理数据的过程如公式(2.3)所示:
2
BN( ) x
x
 

  
...........................(2.3)


第 2 章 相关深度学习模型及实验数据
16
式中,和代表此批次输入数据的均值和方差。是一个极小量,防止分母为
0。 和 是可学习参数,在批量归一化中,由网络自行决定将输入约束的范围。
(4)池化层
池化本质上是一个下采样过程,目的是降低输出特征图的大小。常见的池化
方式有两种:最大池化(Max-pooling)和平均池化(Average-pooling)。最大池化
在窗口中选择最大值作为特征值,而平均池化则选取窗口中所有值的平均值作为
特征值。
由于池化层只是对某一特征图的静态提取,因而不存在需要额外学习的参数。
这一操作一方面通过减小特征图大小加快了计算速度,另一方面可以改善网络过
拟合的问题。
(5)随机失活层
随机失活是一种在训练深度神经网络时常用的正则化技术,主要目的是防止
神经网络过拟合。随机失活的工作原理非常简单:在每次训练迭代中,网络中的
每个神经元(或神经元的输出)都有一定概率被“丢弃”或“关闭”,即在前向
传播和反向传播过程中该神经元的输出被设置为 0。这样做的结果是,每次训练
迭代时网络的结构都是不同的,这迫使网络学习更加鲁棒的特征表示,而不是依
赖于特定的神经元,有助于提高深度学习模型的泛化能力和鲁棒性。
由于每次迭代中只有部分神经元参与训练,因此需要对权重更新进行调整,
通常是通过在前向传播中对未丢弃的神经元的输出乘以神经元的保留概率来实
现。在测试阶段,通常不使用随机失活,而是使用所有神经元的输出,并适当地
调整输出(例如乘以保留概率的倒数)以保持期望值不变。
(6)全连接层
它主要用于将前一层的输出转换为最终的输出,如分类任务中的目标类别。
全连接层将前一层的特征(如卷积层提取的图像特征)整合起来,将提取的特征
映射到样本类别空间。全连接层的示意图如图 2.2 所示。
图 2.2 全连接层示意图


第 2 章 相关深度学习模型及实验数据
17
全连接层是神经网络中不可或缺的一部分,尤其是在处理非结构化数据或在
网络末端生成输出时。然而,由于全连接层中每个神经元都与前一层的所有激活
值相连,其参数众多,因此可能会导致过拟合,尤其是在数据量较小的情况下。
一般而言在全连接层中需要考虑正则化(随机失活,L1/L2 正则化等)和权重初
始化等策略,以确保网络的泛化能力。
2.1.2 Transformer
Transformer 是一种深度学习模型架构,它在 2017 年由 Vaswani 等人[49]首次
提出。它主要用于处理序列数据和其他序列到序列(sequence-to-sequence)任务,
尤其在自然语言处理(NLP)领域取得了革命性的进展。Transformer 架构完全抛
弃 了 递 归 网 络 结 构 和 卷 积 网 络 结 构 , 引 入 了 自 注 意 力 机 制 ( Self-Attention
Mechanism),它允许模型在处理序列的每个元素时,同时考虑序列中的所有其他
元素。典型的 Transformer 模型的结构如图 2.3 所示,它主要由两部分组成:编
码器(Encoder)和解码器(Decoder)。而编码器和解码器主要由输入嵌入(Input
Embedding),位置编码(Positional Encoding),多头注意力机制(Multi-Head
Attention),前馈神经网络(Feed-Forward Neural Network),残差连接(Residual
Add)与层归一化(Layer Normalization)。
图 2.3 典型的 Transformer 模型结构图
(1)输入嵌入
在 Transformer 中,输入嵌入是将离散数据(如单词等数据片段)转换为连
续向量表示的过程。输入序列首先通过一个可学习的嵌入层,将数据片段映射到


第 2 章 相关深度学习模型及实验数据
18
一个固定维度的向量空间,这些向量捕捉了单词的语义和语法信息。输入嵌入的
方法包括预训练嵌入,随机初始化嵌入和可学习式嵌入。
输入嵌入后的数据通常具有比原始数据更低的维度,有助于减少模型的参数
数量和计算复杂度。另外,由于输入嵌入可以作为 Transformer 结构中的可更新
部分,因而在训练过程中,语义相似的数据在向量空间中会更接近,有利于进行
多头注意力交互。
(2)位置编码
位置编码是 Transformer 模型中用于提供序列中每个元素位置信息的一种
技术。由于 Transformer 的自注意力机制本身并不包含序列中元素的顺序信息,
因此需要位置编码来帮助模型理解数据片段在序列中的相对或绝对位置。典型的
Transformer 模型中使用的是正余弦位置编码,其计算公式如公式(2.4)所示。
2/
/
2( 1)/
( 1)/
2
( , ) sin( ) 2
10000
2
( , ) cos( ) 2 1
10000
id
id
id
id
p
PE p i i n
p
PE p i i n




 
  

..................(2.4)
其中, PE 表示位置嵌入, p 是位置索引, d 是模型的维度。位置编码为模型提
供了数据片段在序列中的位置信息,使得模型能够利用这些信息来理解序列的上
下文,其通常与输入嵌入向量相加,以形成每个时间步的最终输入。
(3)自注意力与多头注意力机制
自注意力机制是 Transformer 模型的核心,它用于计算序列内部不同位置之
间关系。自注意力机制允许模型在处理序列的每个元素时,考虑同一序列中其他
所有元素的信息,相较于循环神经网络及其变体能更好地捕捉序列内部的长距离
依赖关系。自注意力机制的核心思想是让模型能够动态地关注输入序列中不同位
置的信息。具体来说,它通过计算输入序列中每个位置的“注意力权重”,来决
定哪些位置的信息对当前任务更重要。自注意力机制如图 2.4 所示。
图 2.4 中,Q、K、V 分别代表 Query、Key、Value 矩阵,它们都是由经过嵌
入后的输入 X 经过矩阵 , ,
QKV
W W W 计算所得。自注意力的计算公式如公式(2.5)
所示。
(, ,) ( )
T
K
QK
Attention Q K V Softmax V
d
 ...................(2.5)
其中, dK 表示 Key 矩阵的维度,Softmax 用于将注意力分数 T /
K
QK d 归一化,
并与 Value 矩阵相乘。


第 2 章 相关深度学习模型及实验数据
19
图 2.4 自注意力机制
多头注意力机制在自注意力的基础上进一步将注意力的计算分解为多个
“头”,每个头学习输入的不同表示子空间。它使用多组 , ,
QKV
W W W 生成多组
Query,Key,Value 矩阵,产生多组输出,然后将多组输出线性组合得到最终的
注意力结果。通过多个头的并行处理,Transformer 模型能够整合来自不同子空间
的信息,从而获得更全面的输入表示。多头注意力的计算公式如式(2.6)所示。
12 0
( , , ) ( , ,..., )
(, ,)
n Q KV i i Ii
MultiHead Q K V Concat head head head W
head Attention Q W K W V W

    

......(2.6)
其中, i
head 表示第 i 个头的注意力结果, Concat 表示级联操作,W0 是待训练参
数。
(4)前馈神经网络
前馈神经网络一般位于多头注意力模块之后。由于多头注意力模块中一般不
包含激活函数,因而前馈神经网络通过提供额外的非线性变换能力,使得模型可
以学习更加复杂的模式和关系。前馈神经网络可以表达为公式(2.7)的形式。
1 12 2
FFN (x)  Activation(xW  b )W  b ..................(2.7)
其中,W 和b是线性变换的权重矩阵和偏置项, Activation 代表激活函数。前馈
神经网络允许模型在自注意力层之后对信息进行进一步的加工和提炼,提高了模
型对数据的表达能力。
(5)残差连接与层归一化
残差连接是一种允许网络在每一层中直接连接输入和输出的技术。在
Transformer 模型中,每个子层的输出会加上该子层的输入,然后这个和会传递到
下一层,其可以用公式(2.8)表示。


第 2 章 相关深度学习模型及实验数据
20
Output  SubLayer(x)  x .......................(2.8)
其中,SubLayer(x) 是子层的输出,x 是子层的输入。残差连接允许梯度直接流向
前面的层,有助于缓解深度网络中的梯度消失或爆炸问题。
层归一化是一种归一化技术,它对每个样本的每个特征进行归一化。因而,
层归一化可以保留单个样本内每个特征的大小关系,但是不能保留不同样本中同
一特征的大小关系。在 Transformer 模型中,层归一化通常在残差连接之后应用,
以稳定训练过程并减少内部协变量偏移(Internal Covariate Shift)。层归一化的公
式如公式(2.9)所示。
() ( )
x
LayerNorm x 



  .........................(2.9)
其中和是输入 x 的均值和标准差,和是可学习参数,用于对归一化后的
结果进行平移和缩放。层归一化减少了不同层之间的尺度差异,使得每一层的输
入分布更加一致,有助于稳定训练过程。
2.2 深度生成模型
本文使用深度学习技术对视觉刺激 EEG 信号进行生成,通过数据增强提高
解码模型的性能。本节介绍了深度学习领域中,三种用于生成任务的基本深度学
习架构。本文在后续章节的研究和实验中也使用到了这些架构。
2.2.1 生成对抗网络
生成对抗网络(Generative Adversarial Networks,GAN)是 Ian Goodfellow 等
人在 2014 年发明的一类非监督式深度学习模型[61]。GAN 是一种强大的生成式模
型,它由两个神经网络组成,生成器(Generator)和判别器(Discriminator),它
们在训练过程进行对抗性博弈来生成新的、与真实数据相似的数据样本。其中,
生成器的目标是生成尽可能接近真实数据的假数据样本。它通常以随机噪声等简
单分布作为输入,通过生成器的学习,尽可能地建立从简单分布到真实数据分布
的映射。而判别器的目标是区分输入的数据是真实数据(来自训练集)还是假数
据(由生成器生成)。GAN 的训练过程是一个零和博弈过程,生成器和判别器相
互竞争,目标是找到生成和对抗之间的纳什均衡,尽可能让生成器和判别器同时
达到最优。GAN 的结构如图 2.5 所示。
在图 2.5 中,生成器 G 用于生成与真实样本相似的虚假样本,用于迷惑判别
器,生成的虚假样本与真实样本一起输入到判别器 D。判别器 D 的任务则是通
过分类网络,判断输入的样本是真实样本还是虚假样本。在训练过程中,生成器


第 2 章 相关深度学习模型及实验数据
21
创建样本,判别器对其进行评估,判别器的反馈用于更新两个网络。随着训练推
进,生成器生成仿真样本的能力逐步增强,判别器逐渐提高了鉴别能力,二者最
终可达到理论上的抗衡状态。
图 2.5 生成对抗网络的结构图
在典型的 GAN 中,生成器 G 的输入是一个符合标准正态分布的随机噪声
p(z) ,而真实样本的分布为 p(x) 。由于判断样本真假是一个二分类问题,因此
可以将标签记为 y ,其中真实样本的标签记为 1,虚假样本的标签记为 0。在训
练生成器 G 的过程中,G 的目标是最小化判别器正确分类真假数据的能力,即
生成器希望判别器将其生成的假数据误判为真实数据。因此,其损失函数可以表
示为公式(2.10)的形式。

( ) log ( ( ))
G z pz
L    D G z ........................(2.10)
判别器 D 的目标是区分真实数据和生成器生成的假数据。其损失函数可以表示
为公式(2.11)的形式:
 
~ () ~ ()
log ( ) log(1 ( ( )))
D x px z pz
L   D x    D G z .........(2.11)
其中第一项是对真实数据的损失, D(x) 是判别器对真实样本评估为真的概率。
第二项是对假数据的损失,1 D(G(z)) 是判别器对生成的假数据样本评估为假的
概率。在实际训练过程中,GAN 的这两个损失函数通常交替优化,模拟了两个
参与者之间的对抗性博弈,最终目的是达到一种平衡,使得生成器能够生成高质
量的虚假样本,而判别器则几乎无法区分真假样本。
2.2.2 变分自动编码器
变分自编码器(Variational Autoencoder,VAE)是一种生成模型,由 Kingma
和 Welling 等人于 2013 年发表[79]。VAE 不仅能够学习输入数据的表示,还能生


第 2 章 相关深度学习模型及实验数据
22
成新的、与训练数据相似的数据点。与 GAN 不同,VAE 使用变分推断来优化模
型参数,并且生成过程是确定性的。因此相比于 GAN,VAE 训练过程更加稳定,
生成结果更加可控。VAE 的变分推断方法也允许模型在潜在空间中进行更灵活
的探索。典型的 VAE 结构如图 2.6 所示。
图 2.6 VAE 的结构图
VAE 主要由两个部分组成:编码器和解码器。其中,编码器是一个神经网络,
它用于将输入数据 x 映射到一个潜在空间,以得到一个潜在表示 z 。具体来说,
它输出两个参数:数据的均值  和数据的方差2 。而解码器同样是一个神经网
络,它将均值 和方差2 构成的潜在空间中的潜在变量 z 重新映射回数据空间,
生成数据的重构 x ' 。
VAE 的损失函数如公式(2.12)所示,它由两部分构成:重构损失和 KL 散
度。

(|)
( , ; ) log ( | ) ( ( | ) || ( ))
VAE q z x
L x p x z KL q z x p z

     .........(2.12)
在公式(2.12)中,和分别是解码器和编码器的参数, p (x | z)
 是在给定潜在
变量 z 时解码器生成数据 x 的条件概率,q (z | x)
 代表给定数据 x 时潜在变量 z 的
后验分布, p(z) 是潜在变量的先验分布,往往是标准正态分布。
VAE 中的重构损失衡量生成的样本 x ' 与真实样本 x 的差异,通常而言这一
部分使用均方误差或者是二元交叉熵。而 KL 散度是一种衡量两个概率分布相似
度的方法,在 VAE 中用于衡量潜变量空间与先验分布空间(通常是标准正态分
布)之间的差异。为了使模型具有生成能力,VAE 要求每个 q (z | x)
 都向标准正
态分布 p(z) 看齐。尽管 VAE 训练更稳定,但由于 VAE 的编码过程本质上是一种
有损压缩,VAE 生成的样本分辨率往往较低或者比较模糊。


第 2 章 相关深度学习模型及实验数据
23
2.2.3 扩散模型
扩散模型(Diffusion Models)是一类深度生成模型,它们模拟自然界中常见
的扩散过程来合成新数据。扩散模型的核心思想是将数据生成过程视为一个由简
单分布逐渐扩散到复杂分布的过程,通过在数据中添加噪声(通常是高斯噪声)
并逐步去除噪声来生成新的样本。目前,这类模型在图像、音频和文本生成等领
域展现出了优于基于 VAE,GAN 等生成模型的卓越的性能。
目前,扩散模型包括三个大类:去噪扩散概率模型[66](Denoising Diffusion
Probalistic Models,DDPM),基于噪声条件得分的网络[80](Noise Conditional Score
Networks,NCSN),基于随机微分方程(Stochastic Differential Equations,SDE)
的模型[81]。但是对于所有的扩散模型,它们都具有两个必不可少的过程:前向扩
散过程和反向生成过程,如图 2.7 所示。我们以 DDPM 为例描述前向扩散过程
和反向生成过程的原理。
图 2.7 扩散模型的前向过程和反向过程
(1)前向扩散过程
在前向扩散过程中,模型逐步将数据从原始分布引入噪声,生成一系列越来
越模糊或随机的样本。这可以看作是对数据的“破坏”过程,每一步都使数据更
加接近一个先验噪声分布,通常假设为标准正态分布。
由于噪声是逐步添加的,我们记 x0 为原始数据,xt 为添加 t 步噪声后的数据,
则前向扩散过程中的每一步都可以用公式(2.13)表示:
11
1
t tt tt
x  x 

   ...........................(2.13)
其中t 是是一个超参数,它用于控制扩散模型在前向过程中每一步加入的噪声
幅度,t1 是一个符合标准正态分布的随机噪声。由于加入了额外的噪声,需要
对数据 x 进行缩放,以保持其分布,因此需要在噪声项前面乘上 1t 。为了方
便起见,我们将公式(2.13)改写为条件概率分布的形式,如公式(2.14)所示:
  
11
| ; ,1
forward t t t t t t
P x x N x x I

  .................(2.14)
记 xT 为加入 T 步噪声后最终的数据,则从 x0 到 xT 的前向扩散过程可以用公式


第 2 章 相关深度学习模型及实验数据
24
(2.15)表示:
 

00
| ; ,1
forward T T T T
P x x  N x  x  I .................(2.15)
其中,
1
T
Ti i


 。
(2)反向生成过程
反向生成过程是从符合标准正态分布的随机噪声中一步步恢复原始数据分
布的过程。反向过程的目标是学习一个模型,该模型能够预测在反向过程中每一
步的噪声水平,从而逆转前向过程中的噪声添加。假设每一步中的反向过程可以
表示为一个多元高斯分布  
1|
backward t t
P xx
 ,那么对于 backward
P ,有:
  

11
| ; ,, ,
backward t t t t t
P x x Nx xt xt



 ..............(2.16)
其中 和是待求解的后验分布均值和方差,它们可以通过 xt 和 t 求解。然而,
公式(2.16)的结果难以直接求解,因此我们可以通过贝叶斯公式求  
1|
tt
Px x
,
即:
    

10 10 1
0
|
|, | |
t t t tt
t
Px x
Px x x Px x Px x
 
  ..................(2.17)

10
|,
tt
Px xx
 又 被 称 为 “ 反 向 过 程 转 换 核 ”。 由 于  1 
t| t
P x x 、  
10
|
t
Px x
和
 0
t|
P x x 都是已知的,假如  
10
|,
tt
Px xx
 中的 x0 已知,则  
1|
tt
Px x
 也能求得。这
里不加证明地给出  
10
|,
tt
Px xx
 的求解结果:
  
2
1 01 0
22
11 ,
21 1 1 1 10
|,
tt
t
t t tt tt t t
x x x x Cx x
tt
Px x x e


  




  
  
  
  

  

  ......(2.18)
其中,  0 
t,
C x x 与 xt1 无关。利用前向过程中 xt 与 x0 的关系消去 x0 ,可以得到:

    
1
1
1
,1
11
,1
t tt tt
tt t t
xt x
xt








 


 


 
 



.....................(2.19)
其中 是符合高斯分布的随机噪声。将公式(2.19)代入到(2.16)中,即可得
到反向生成过程的表达式。


第 2 章 相关深度学习模型及实验数据
25
(3)扩散模型的优化目标
扩散模型的训练可以看作是一种变分推断过程,其中正向和逆向过程定义了
一个变分下界(ELBO)。不过简单来说,扩散模型的反向生成过程并不直接通过
xt 估计 xt1 ,而是先估计公式(2.19)中的 噪声项,再通过公式(2.16)由 xt 得
到 xt1 。假如将前向扩散过程中加入的真实噪声记为r ,预测的噪声为 ,那么
扩散模型的优化目标 Lt 可以写为公式(2.20)的形式:

0
2
,0
E 1 , , (0,1)
r
t X r t tr r
L X tN

    

  


 ......(2.20)
本质上,扩散模型的优化目标是最小化前向过程中加入的真实噪声分布r 与预测
噪声分布 的欧几里得距离。
本文的第三章介绍了基于自适应方差扩散模型的视觉刺激脑电信号增强方
法,以本节介绍的扩散模型为理论基础,进行了扩散模型在脑电视觉刺激的生成
的研究,生成高质量的脑电样本以进行数据增强。
2.3 数据集介绍
本文使用了来自斯坦福大学数字存储库的公共 EEG 数据集[82]。此数据集包
含了 10 名受试者的 EEG 数据,受试者的年龄在 21 岁至 57 岁之间(平均年龄为
30.5 岁;3 名女性;1 个左撇子)。所有受试者均报告色觉正常,视力正常或矫正
至正常。
如图 2.8 所示,在实验过程中研究人员向受试者展示了多种类别的刺激图像,
这些图像可以分为六个大类:人类身体/面部、动物身体/面部、水果蔬菜、人造
器械。刺激图像被设置在没有边框的中灰色背景上,覆盖了 7.0°×6.5°的视觉
角度。因此,在该数据集中总共有 6 个语义类别的视觉刺激图像:人体(HB)、
人脸(HF)、动物身体(AB)、动物脸(AF)、水果或蔬菜(FV)和人造器械(IO)。
其中每个类别还包括 12 种独特的图像,一共有 72 张灰色背景的图像。对于数据
的收集范式,参与者坐在离桌子 57 厘米的椅子上,监视器在桌子上随机地闪烁
每个刺激的图像,每次试验时间为 500 毫秒,试验后有一段 750 毫秒的黑灰色
屏幕呈现。受试者总共进行了 5184 次试验,分为两个阶段。每个阶段由 3 个试
验块组成,每个试验块之间的实验时间间隔 6 到 8 天,每个块包含 864 个试验
(每张图像随机显示 12 次),每 36 次试验进行一次短暂的休息。因此,每张图
像向每个受试者展示 72 次。为了尽可能减少试验期间受试者的眼动现象,每张
图像的中心都覆盖一个 0.76°×0.76°角度的白色注视点,并且在图像展示之间


第 2 章 相关深度学习模型及实验数据
26
显示的空白屏幕上的同一位置。
图 2.8 视觉刺激所使用的图像类别
EEG 数据的采集方面,使用了非屏蔽的 128 通道 EGI HydroCel Geodesic
Sensor Net[83],频率为 1 k Hz。作为预处理的一部分,首先使用高通 4 阶巴特沃
斯滤波器和低通 8 阶切比雪夫 Type I 型滤波器,分别用于去除 1 Hz 以下和 25 Hz
以上的频率成分。然后对滤波后的数据进行 62.5 Hz 降采样,并使用 Infomax ICA
去除每个受试者的眼部伪影。最后,去除 E125 - E128 通道,保留剩下的通道作
为有效的 EEG 数据。
2.4 本章小结
为了探究脑电信号与人类视觉的关系,本文使用了目前流行的深度学习领域
的方法,对视觉刺激 EEG 信号进行数据增强与解码。本章给出了本文第 3 章和
第 4 章使用的视觉刺激 EEG 数据集的来源和采集范式,介绍了本文的研究中所
涉及的卷积神经网络、Transformer 模型、深度生成模型等经典的深度学习方法。
本文以本章介绍的算法为基础,对脑电信号与人类视觉的关系进行更深入的研究。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
27
第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强
方法
3.1 整体方法框架
本章介绍一种基于自适应方差扩散概率模型的自监督视觉刺激脑电图数据
增强框架(Self-supervised Adaptive Variance Diffusion probabilistic model-based
Visual-stimulus EEG Augmentation Framework,SAD-VER)。整个框架结构如图 3.1
所示。
SAD-VER 由三个组成部分组成:生成过程、优化过程和解码过程。其中,
生成过程由基于 U-Net 结构的自适应方差扩散概率模型(Adaptive Variance
Diffusion Probabilistic Model,AV-DPM)组成;优化过程包括相似度测量网络和
基 于 贝 叶 斯 方 法 的 超 参 数 优 化 器 ( Bayesian Method-based Hyperparameter
Optimizer,BMHO);解码过程包括一个具有时间域-空间域 Inception 结构的卷积
分类网络。在训练阶段,对真实的 EEG 样本进行归一化和二维投影,以训练 AV
DPM 和相似性度量网络。训练完成后,SAD-VER 的流程展开如下:在生成过程
中,利用 AV-DPM 合成高质量、多样化的 EEG 样本。在优化过程中,将生成的
EEG 样本输入到相似性度量网络中。该网络测量生成的 EEG 与真实 EEG 之间
的相似性,旨在使相似性最大化。然后采用基于贝叶斯方法的超参数优化器对
AV-DPM 的生成过程的超参数进行优化调整。在解码过程中,将优化后生成的
EEG 样本和真实的 EEG 样本均送入分类网络进行解码,从而实现自监督学习。
图 3.1 SAD-VER 的整体框架图
在原始的 EEG 样本中,电极的分布是一维的。为了保持 EEG 电极在头部的
相对位置,本研究采用了一个简单的二维投影,将 124 个有效电极的 EEG 数据
投影到 13×13 的二维矩阵上,投影方法如图 3.2 所示。最后,对 EEG 数据进行


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
28
最小-最大归一化(Min-Max Scale Normalization)处理,归一化公式如公式(3.1)
所示,在保留数据之间大小关系的同时,将数据值限制在 0 和 1 之间。
min( )
max( ) min( )
norm
xx
x xx

  ............................(3.1)
图 3.2 脑电图的二维投影范式
3.2 生成过程
3.2.1 AV-DPM 的结构
在扩散模型中,生成网络的结构通常是一种 U-Net[84]风格的神经网络,用于
拟合正向扩散过程中引入的随机高斯噪声。所提出的 AV-DPM 中的生成网络同
样采用了改进的 U-Net 结构进行噪声拟合。其结构如图 3.3 所示。ResNet 块是一
个简单的卷积神经网络块,具有 1×1 核大小和 3×3 核大小的卷积结构和残差连
接。下采样(Down Sampling)通过一层二维卷积来实现,而上采样(Up Sampling)
则通过一层 3×3 核大小的二维卷积和最近邻插值来实现。它们的目的分别是压
缩和还原脑电图特征。
图 3.3 AV-DPM 的结构图
在 U-Net 结构中,本文采用了一种受门控线性单元(Gated Linear Units,GLU)
[85]启发的新型自注意机制,即门控注意单元(Gated Attention Unit,GAU)。与传
统的多头注意机制相比,GAU 更简单,同时保持相似的注意性能[86]。本文将 GAU


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
29
模块纳入 U-Net 中,提高了网络的噪声拟合性能,同时尽可能地最小化额外的计
算成本。
3.2.2 AV-DPM 的前向扩散过程
在 AV-DPM 中,前向扩散过程依然满足扩散概率模型(Denoising Diffusion
Probabilistic Model,DDPM)与扩散概率隐式模型(Denoising Diffusion Implicit
Model,DDIM)[87]中的形式,也即:
11
( | ) ( ; ,1 )
t t t tt t
p X X N X X 

  .....................(3.2)
t 是一个超参数,它用于控制 AV-DPM 在前向过程中每一步加入的噪声幅度。
而对于 AV-DPM,假如将加入的真实噪声记为r ,预测的噪声为 ,那么 AV
DPM 的优化目标 Lt 可以写为公式(3.3)的形式:

0
2
,0
E 1 , , (0,1)
r
t X r t tr r
L X tN

    

  


 .........(3.3)
其中
1
t
ti i


  。公式(3.3)可以用于作为 AV-DPM 的损失函数。因而,AV-DPM
的优化目标转化为最小化真实噪声分布r 与预测噪声分布 的欧几里得距离。
3.2.3 反向生成过程中的最优方差上界
在 DDIM 中,反向生成过程满足如下表达式:
2 () 1 10 1
() 0
1 ()
1
1 ()
t
t t t t t tt
t
t tt
t
XX X
XX X


    


 
       
  
  
 


............(3.4)
其中t 是一个 DDIM 风格的反向生成过程方差,它受一个值介于 0 和 1 的超参
数约束,其表达式如下:
1
1
1
11
tt t
tt

  


  

  
  

  
........................(3.5)
t 是一个符合标准高斯分布的随机量, ( ) ( )
t
Xt
 则代表噪声拟合模型。然而,公
式(3.4)的第 2 个公式实际上是用 X t 来估计原始输入 X 0 ,而根据[88],这一估计


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
30
往往并不是完全准确的。因此,本文假设 X t 估计原始输入 X 0 的过程符合一个高
斯分布:
2
00
()
( | ) ( ; ( ), )
1
() 1 ()
t tt
t
t t tt
t
pX X NX X I
XX X


  

  
 
 


..................(3.6)
其中,t 是方差的修正项。根据修正项t 、公式(3.4)与公式(3.6),可以推
导出在此假设下的最优方差上界:
2
22 2 max 1
11
t
t tt t

  




   


.................(3.7)
受篇幅所限,本文在正文部分不再详细对公式(3.7)进行推导,完整的推导
过程请参考本文的附录 A。本文提出的 AV-DPM 将公式(3.7)作为约束条件,
加入到反向生成过程的方差调节中。
3.2.4 反向生成过程中的自适应方差
对于噪声拟合模型 ( ) ( )
t
Xt
 ,它可以写成如下形式[81]:
()
1
log ( ) ( ) 1
t
t
Xt t
t
pX X


 

.....................(3.8)
log ( )
Xt t
 p X 被称为得分模型(Score-based Models)。假如将 log ( )
Xt t
 p X 看作是
一个梯度项,那么可以发现,实际上在 t t 1
X X
 的反向生成过程中,每一步都可
以看作是 ( t )
p X 沿着梯度方向朝 0
p(X ) 前进。将公式(3.8)代入公式(3.4)的第
2 个公式,有:
0
1
(1 ) log ( )
t
t tX t
t
X X  pX


    
  ..................(3.9)
再将公式(3.9)代入公式(3.4)的第 1 个公式,有:
2
11 11
1 1 (1 ) log ( )
t
tt t t t t t t X t tt tt
X X pX

    

 


            


(3.10)
由于是一个固定的超参数,因此可以将 X t 的系数视为一个常数 C 。假如将含
有t 与的项记为关于t 的函数 ( t )
f  ,那么公式(3.10)可以改写为:


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
31
1 ( ) log ( )
t
t t t X t tt
X C X f pX 
        ................(3.11)
可以发现,公式(3.11)和机器学习中的梯度下降公式具有相似的形式, ( t )
f
可以视为梯度下降公式中的学习率。在机器学习中对学习率最常用的一种优化方
法便是自适应学习率,随着迭代步数增大,学习率自动减小。由于方差项t 会影
响反向生成的多样性,本文将这种自适应的思想应用于对反向过程的方差调节中。
具体来说,在反向生成过程开始的步数使用一个较大的方差项,使得反向生成过
程在初始阶段就具有较好的多样性。在经过一定步数,确定了生成过程后,将方
差逐渐减小,以加快反向生成步骤。
3.2.5 AV-DPM 的反向生成过程
在 AV-DPM 中,真实的方差记为 real
 ,与公式(3.5)中 DDIM 风格的方差
表达式类似, real
 可以用公式(3.12)表示:
1
1
1
11
tt real tt
K 
 


  

  
  

  
......................(3.12)
K 是一个可变的权重系数,它的值介于 0 到 1 之间。随着生成步数越大, K 的
值应该逐渐减小。因而本文额外引入了两个超参数用于控制 K 的变化,它的表达
式如公式(3.13)所示:
max
max
20 ( )
01
rs
d d steps steps
K steps
K
  

  

...................(3.13)
其中,steps 是当前生成的步数, max
steps 是最大生成步数。dr 和 ds 是额外加入的
两个超参数, dr 用于控制方程衰减的速率,它的值越大,则方差衰减程度越大;
ds 用于控制方差衰减的起始步数,它的值越大,则方差开始衰减的起始步数越大,
也就是越晚进行衰减。 dr 和 ds 都是介于 0 和 1 之间的值。
考虑到公式(3.7)的最优方差上界,额外引入一项约束条件,即:
2
22 1
11
t
real t t t t

  




   


................(3.14)
结合公式(3.5)、(3.12)、(3.13)和(3.14),可以得到真实方差 real
 的表达


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
32
式以及所有的约束条件:
1
1
max
max
2
22 2 1
22 1
1
1
11
20 ( )
01
11
1
11
tt real tt
rs
t
real t t t t
tt t
tt
K
d d steps steps
K steps
K

 

  


  





   

   
  

   
   

  
  

 
   
 

    

   
  

   

...............(3.15)
公式(3.15)表明,在最大生成步数 max
steps 和超参数都一定的情况下,真
实的反向过程方差 real
 随着生成步数 steps 的变化程度由额外加入的三个超参数
dr , ds 和决定。这也就是本文提到的具有自适应方差控制的反向过程。
3.3 优化过程
3.3.1 相似性度量网络
相似性度量网络用于衡量生成 EEG 与真实 EEG 的相似程度。具体来说,首
先采用真实的 EEG 训练相似性度量网络。在训练完成后将真实 EEG 与生成 EEG
分别输入到网络中,并且截取网络的特征向量分别作为真实 EEG 的特征向量与
生成 EEG 的特征向量。最后,计算生成 EEG 的特征向量与真实 EEG 的特征向
量之间的 Fréchet 距离(Fréchet Distance,FD),从而判断生成 EEG 与真实 EEG
之间的差异。本文选用一种浅层的,具有 Inception 结构的卷积神经网络——STI
Net(Spatial-Temporal Inception Net)作为相似性度量网络。其结构如图 3.4 所示。
图 3.4 相似性度量网络的结构图
STI-Net 使用带有 L2 范数约束的卷积层与空域-时域 Inception 模块提取 EEG
特征,L2 范数约束在不改变权重分布的情况下,对权重进行缩放,缓解网络中
的梯度消失或爆炸问题;而 Inception 模块则额外地关注输入脑电信号中的时间


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
33
信息与空间信息。
本文将通过 STI-Net 输出的特征向量求得的 Fréchet 距离称为 FSTID(Fréchet
Spatial-Temporal Inception Distance)。假设真实 EEG 的特征向量与生成 EEG 的
特征向量分别服从多元高斯分布 ( , )
rr
N 与 ( , )
gg
N   ,那么可以得到 FSTID 的
计算公式:
2 Tr( 2 )
r g r g rg
FSTID            .................(3.16)
FSTID 的值越小,代表真实 EEG 的特征向量与生成 EEG 的特征向量越接
近,说明生成的 EEG 质量越好。不过,由于 EEG 具有复杂的底层空间模式,单
独的 Fréchet 距离并不能直观地反映真实 EEG 与生成 EEG 的相似程度。因此,
本文提出了一项新的指标用于衡量生成 EEG 与真实 EEG 的相似程度,称为
Fréchet 相似性分数(Fréchet Similarity Score,FSS)。它的计算公式如下:
1 gen real real real
rand real real real
FD FD
FSS FD FD



   .....................(3.17)
其中, gen real
FD  是生成数据特征与真实数据特征之间的 Fréchet 距离, real real
FD 
是真实数据特征之间的 Fréchet 距离, rand real
FD  是随机噪声特征与真实数据特征
之间的 Fréchet 距离。FSS 是一个介于 0 到 1 之间的值,生成数据特征越接近于
真实数据特征, gen real
FD  和 real real
FD  越相近,FSS 越大。相比于无标定的 Fréchet
距离指标,FSS 可以更加直观地展现生成 EEG 的质量。本文同时使用 FSTID 和
FSS 作为对生成 EEG 质量的评价指标。
3.3.2 基于贝叶斯方法的超参数优化器
本文提出的 AV-DPM 在反向处理过程中使用三个超参数 dr , ds 和来调整
方差。为了使得生成的 EEG 质量达到最优,本文采用了基于贝叶斯方法的超参
数优化器(Bayesian Method-based Hyperparameter Optimizer,BMHO)来寻找方
差控制的最优超参数。BMHO 选择基于序列模型的优化算法(Sequential Model
Based Optimization,SMBO)作为超参数优化器的优化策略。SMBO 能够在探索
参数空间的新区域之间进行权衡,并利用历史信息来找到快速最大化函数的参数
[89]。算法过程如下:
(1)假设每个超参数分别服从某个已知分布,后续从该分布中采样。
(2)随机采样得到 n 组超参数 X (n, m) ,m 为一组超参数中的参数个数,并
计算在超参数 X 下,模型生成 EEG 的 Fréchet 相似性分数 FSS 。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
34
(3)设置代理函数(surrogate function),用于拟合从 X 到 FSS 的映射,得
到 FSS 的条件概率密度函数 ( | )
pM FSS X 。
(4)再采样 c 组超参数 X c ,对 X c 中的每一组超参数 Xi ,计算 ( | )
M ii
p FSS X 。
(5)通过采集函数(acquisition function)对每一组超参数进行评价,从中
选择最优的超参数组合 X b ,并计算在超参数 X b 下模型生成 EEG 的 Fréchet 相似
性分数 b
FSS 。
(6)将 ( , )
bb
X FSS 加入到待寻优的超参数空间 (X , FSS) 中,更新代理函数与
条件概率密度函数 ( | )
pM FSS X 。
(7)重复(4)-(6),直到算法达到最大迭代次数。
代理函数选用树形 Panzer 估计器(Tree-structured Parzen Estimator,TPE)。
TPE 分别对条件概率分布 p(x | y) 和边缘概率分布 p(y) 进行建模,然后通过贝叶
斯公式来计算后验概率分布 p(y | x) 。其中,条件概率分布 p(x | y) 满足以下形式:
*
*
()
( | ) ()
lx y y
px y gx y y

 

..........................(3.18)
y* 是一个阈值。当 *
y  y 时,对应的参数 x 视为损失较小的“好参数”;反之则视
为损失较大的“坏参数”。 l(x) 与 g(x) 由 Parzen 窗函数 ( )
pw x (Parzen Window
Function)决定,它的概率密度估计为:
2 1
1
()
n i w i
xx
p x nh n




 
 
 ........................(3.19)
其中 h 为窗口宽度, n 为采样点个数,是窗函数,通常情况下它是一个高斯函
数。
采集函数选用期望改进函数(Expected Improvement,EI)。它的表达式如下:
*
*
( ) max( , 0) ( | )
M
yx
EI y y p y x dy


 
 ...............(3.20)
优化目标是令 y* (x)
EI 最大。而根据[90],假如 *
p( y  y )   ,那么可以求得:
*
1
()
() ()
(1 ) ( ) ( )
yx
gx lx
EI l x g x



   


..................(3.21)


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
35
因而,采集函数的优化目标转化为令 l(x) / g(x) 最大,只需要根据代理函数 TPE
求得满足目标的参数 x 即可。
3.4 解码过程
解码过程由分类网络(Classification Network)组成。分类网络用于对 EEG
进行分类和识别,它仍然沿用了章节 3.3.1 中提及的 STI-Net 结构。但是相比于
相似性度量网络中的 STI-Net,分类网络中的 STI-Net 在提取特征向量后额外加
上了一个具有 3 层结构的多层感知机,以便对提取 EEG 的特征向量进行解码。
多层感知机结合非线性激活函数,可以学习和模拟复杂的非线性映射关系,令
EEG 的特征向量在高维空间中具有可分性质,从而实现分类。
具体来说,将 STI-Net 提取后的 EEG 特征展平为一个 512 维的向量,然后
输入到多层感知机中;隐藏层维度设置为输入维度的 1/4 以进行降采样,即维度
为 128;最后,输出维度则根据分类任务类数的不同进行调整,例如在 6 分类任
务中调整维度为 6,72 分类任务中则调整为 72。激活函数使用常用的 ReLU 激
活函数。
3.5 实验结果
3.5.1 实验条件与超参数设置
本研究中使用的深度学习框架是 PyTorch 1.11[91],实验部署平台为 Intel Xeon
Platinum 8375C @ 2.90GHz,72GB RAM,6 张 NVIDIA GeForce RTX 3090 独立
GPU。SAD-VER 各模块的详细参数设置和实验细节如下。
在生成过程中,6 类分类任务的每个受试者的每个标签生成 1024 组 EEG 数
据。对于 72 类分类任务和类内分类任务,每个标签生成 768 组 EEG 数据。采用
Adam 优化器[92],学习率为 0.00002。训练步数设置为 300000,生成过程中的最
大生成步数设置为 1000。
在优化过程中,对于相似度度量网络,初始学习速率设置为 0.002,epoch 数
设置为 50,并且部署了早停法。当测试集损失在连续 15 个 epoch 没有减少时终
止训练。对于基于贝叶斯方法的超参数优化器(BMHO),超参数 dr , ds 和的
优化范围设置为 0 到 1。超参数优化器的最大迭代次数设置为 500 次。
在解码过程中,分类网络的参数与优化过程中的相似度度量网络相一致。采
用 10 折交叉验证,以依赖于单个受试者解码性能的方式评估所提出的网络框架。
3.5.2 实验结果
为了直观地理解 AV-DPM 是如何生成脑电图的,生成过程如图 3.5 所示。可


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
36
以观察到,随着生成步数从 0 逐步增加到设定的最大步数 1000,生成的 EEG 样
本逐渐从随机噪声转变为与真实样本非常相似的样本。AV-DPM 的生成过程的本
质是根据特定的规则对随机噪声进行逐步去噪操作。因此,由于不同的随机噪声
输入,AV-DPM 几乎不可能产生相同的样本,保证生成的 EEG 样本具有多样性。
图 3.5 AV-DPM 生成过程的可视化图像
为了更好地评价 SAD-VER 对视觉刺激 EEG 解码性能的贡献,参考[91]中的
工作,本研究对斯坦福数字存储库的公共脑电图数据集进行了 6 类分类任务和
HF-IO 二元分类任务。实验采用 10 折交叉验证,训练集由所有生成的脑电图和
90%的真实脑电图混合组成,测试集包含 10%的真实脑电图,以测试集的平均分
类精度为最终结果。此外,本研究还比较了没有 SAD-VER 数据增强框架的 STI
Net 分类网络在两种分类任务中的分类性能,以证明 SAD-VER 的性能增益。6 类
分类任务和 HF-IO 二分类任务的结果分别如图 3-6 (a)和(b)所示。
图 3.6 显示,在 10 名受试者中,SAD-VER 在 6 类分类任务中平均准确率为
61.55%,在 HF-IO 二元分类任务中平均准确率为 91.42%。与不增加数据的 STI
Net 相比,SAD-VER 的平均解码准确率分别提高了 9.56%和 2.57%,表明采用
SAD-VER 进行数据增强可以有效地提高分类网络的解码性能。
为了更进一步地分析每个类别的分类情况,表 3.1 和表 3.2 显示了在使用
SAD-VER 进行数据增强后,STI-Net 在 6 类分类任务中,所有受试者的精准率
召回率和 F1 分数。
对每个受试者进行单独分析可以发现,每个受试者的脑电图分类性能的改善
程度都有所不同。例如,在 SAD-VER 数据增强后,受试者 2 和受试者 7 在 6 类
分类任务中的识别准确率提高幅度分别达到 24.67%和 19.93%,大幅超过平均水
平的 9.56%。除此之外,受试者 1 和受试者 10 的提升幅度接近或超过了 10%,
显示出非常显著的提升。
但是,与这些分类准确率提升幅度极大的受试者相比,受试者 3、受试者 4
和受试者 9 的分类准确率提升幅度相对较小,6 分类任务下识别准确率的提升幅


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
37
度分别为 3.82%,2.60%,2.03%,大幅落后于平均水平。
图 3.6 STI-Net 分类网络有无 SAD-VER 数据增强时的解码性能
在 HF-IO 分类任务中也可以观察到和 6 分类任务中类似的现象,受试者 1、
2、7 的解码准确率提升非常明显,提升幅度超过 3%。但是受试者 3 和受试者 9
的提升程度则小于 0.5%;受试者 4 则表现稍好,但提升幅度也没有超过 1%,明
显低于平均提升幅度。
出现这一现象的原因,我们推测是因为受试者 3 和受试者 9 的脑电信号可能
存在一些特殊的性质(例如高频干扰),使得 SAD-VER 不能很好地生成他们的
脑电信号,进而导致解码性能提升微弱。我们将在章节 3.5.3 和附录 B 中进一步
对这个问题进行探讨。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
38
表 3.1 所有受试者在 HB,HF,AB 三个类中的精准率-召回率和 F1 分数
Subject HB HF AB
Precision Recall F1-Score Precision Recall F1-Score Precision Recall F1-Score
S1 0.6622 0.5158 0.5799 0.6263 0.6813 0.6526 0.5658 0.5181 0.5409
S2 0.6593 0.7317 0.6936 0.7949 0.7045 0.7470 0.5714 0.4557 0.5070
S3 0.4935 0.4872 0.4903 0.7753 0.7931 0.7841 0.6729 0.6923 0.6825
S4 0.3929 0.3385 0.3636 0.6532 0.7941 0.7168 0.4605 0.4795 0.4698
S5 0.6615 0.6232 0.6418 0.8068 0.8554 0.8304 0.6306 0.7216 0.6731
S6 0.7222 0.7065 0.7143 0.7636 0.8485 0.8038 0.5641 0.5946 0.5789
S7 0.6780 0.5063 0.5797 0.8554 0.9342 0.8931 0.7835 0.8736 0.8261
S8 0.3231 0.2561 0.2857 0.5734 0.8200 0.6749 0.4861 0.4487 0.4667
S9 0.4533 0.3617 0.4024 0.6627 0.6548 0.6587 0.4177 0.4125 0.4151
S10 0.5593 0.4024 0.4681 0.7589 0.8947 0.8213 0.7200 0.8372 0.7742
Avg. 0.5605 0.4929 0.5219 0.7271 0.7981 0.7583 0.5873 0.6034 0.5934
Std. 0.1307 0.1502 0.1366 0.0869 0.0878 0.0774 0.1103 0.1595 0.1322
表 3.2 所有受试者在 AF,FV,IO 三个类中的精准率-召回率和 F1 分数
Subject AF FV IO
Precision Recall F1-Score Precision Recall F1-Score Precision Recall F1-Score
S1 0.6092 0.5579 0.5824 0.5185 0.5385 0.5283 0.6176 0.8182 0.7039
S2 0.7364 0.8100 0.7714 0.6923 0.6506 0.6708 0.7172 0.8161 0.7634
S3 0.5059 0.5375 0.5212 0.5341 0.5402 0.5371 0.5342 0.4699 0.5000
S4 0.4857 0.5667 0.5231 0.3924 0.3163 0.3503 0.4430 0.3846 0.4118
S5 0.6961 0.7474 0.7208 0.7000 0.6049 0.6490 0.7349 0.6489 0.6893
S6 0.7564 0.6146 0.6782 0.6049 0.6125 0.6087 0.5854 0.6154 0.6000
S7 0.7941 0.8617 0.8265 0.6583 0.8316 0.7349 0.7414 0.4886 0.5890
S8 0.4842 0.5412 0.5111 0.5060 0.4615 0.4828 0.5246 0.3855 0.4444
S9 0.4298 0.7101 0.5355 0.4568 0.4205 0.4379 0.5632 0.4712 0.5131
S10 0.7640 0.7312 0.7473 0.6163 0.6235 0.6199 0.6164 0.5769 0.5960
Avg. 0.6262 0.6678 0.6418 0.5680 0.5600 0.5620 0.6078 0.5675 0.5811
Std. 0.1321 0.1132 0.1141 0.0977 0.1342 0.1109 0.0939 0.1501 0.1092
3.5.3 生成质量分析
本文采用使用章节 3.3.1 中提到的 FSTID 和 FSS 指标来评估 SAD-VER 生成
的 EEG 质量。对每个受试者的每个标签所生成的 EEG 进行了多次测量并且取平


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
39
均值,结果如表 3.3 所示。其中,FSTID 值越小,表示生成的 EEG 样本越接近于
真实 EEG 样本;而 FSS 是一个从 0 到 1 的度量,值越高表示生成质量越好。
表 3.3 每名受试者的 FSTID 与 FSS 指标
受试者/评价指标 FSS↑ FSTID↓
Sub1 0.9693 122.954045
Sub2 1.0000 92.093089
Sub3 0.5523 123.566736
Sub4 0.3953 142.837053
Sub5 1.0000 112.500586
Sub6 0.9555 115.489100
Sub7 1.0000 101.305843
Sub8 0.9014 115.230624
Sub9 0.4027 172.085047
Sub10 0.9384 111.937652
Average 0.8115 121.0000
通过表 3.3 可以发现,大部分受试者生成 EEG 的 FSS 值都达到了 0.9 以上,
表明 SAD-VER 能够对它们的 EEG 进行高质量生成。然而对于受试者 3、4、9,
它们的生成 EEG 表现出了明显低于平均水平的 FSS 值。其中,受试者 4 和 9 的
FSTID 值同样也明显高于平均水平。这说明 SAD-VER 对这些受试者的 EEG 生
成效果不理想。图 3.6 展示的解码准确率也验证了这一结论。为了更直观地展现
SAD-VER 对视觉刺激 EEG 的生成结果,本文在附录 B 中提供了所有受试者在
同一标签下的真实 EEG 地形图与生成 EEG 地形图。
本文在章节 3.3.2 介绍了采用基于贝叶斯方法的超参数优化模块(BMHO),
寻找方差控制中的最优超参数。BMHO 将最大化 10 名受试者生成 EEG 的 FSS
值的平均值作为优化目标,将反向生成过程中的方差衰减速率 dr ,方程衰减起始
比例 ds 和超参数作为待优化的超参数,以期寻找到最优的一组参数使得受试者
的生成 EEG 质量最好。经过 500 次的迭代优化后寻找到最优的超参数组合为
1
dr  , 0
ds  和 0.5 。其结果如图 3.7 所示。其中,图 3.7(a)展示了 0.5 时,
FSS 随着 dr 和 ds 的变化结果;图 3.7(b)展示了 1
dr  时,FSS 随着和 ds 的变化
结果;图 3.7(c)展示了 0
ds  时,FSS 随着和 dr 的变化结果。由图 3.7(a-c)可以
发现, dr 越接近于 1, ds 越接近于 0,生成 EEG 的平均 FSS 值越高。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
40
图 3.7 不同的超参数组合以及其对应生成 EEG 的平均 FSS 值
本文进一步设置 1
dr  , 0
ds  ,观察在不同值下,生成 EEG 的 FSS 值变
化程度,结果如图 3.7(d)所示。结果显示虽然 0.5 下 FSS 可以取得最大值,但
是整体而言,超参数对于 EEG 生成质量的影响较小,不同的值对应的 FSS
值差距最大不到 0.001。图 3.7(b)与(c)展示的结果也验证了这一结论。
3.5.4 方法对比
(1)与最先进的解码方法的比较
本文基于数据集上 5 种不同分类任务,包括 6 类、72 类、HF- IO、HF 类内
和 IO 类内分类任务,对提出的 SAD-VER 与最近的基准方法进行了比较,结果
如表 3.4 所示。
可以看出,由于现有的方法是不进行数据增强的,在解码精度方面仍有显著
的提高空间。与表 3.4 中的其他方法相比,SAD-VER 在 6 类、HF-IO 和 HF 类内
分类任务的平均精度上均有显著提高,比最先进的 RLN[97]模型分别领先 7.42%、
1.73%和 6.11%。在 72 类和 IO 类内解码任务中,与最先进的模型[93]相比,SAD
VER 的平均分类准确率分别提高了 0.88%和 2.25%。
此外, SAD-VER 的标准差值在多个分类任务中都有所下降,范围为 0.05%
到 1.63%不等。这说明经过 SAD-VER 数据增强后,基于 EEG 的视觉刺激解码性
能不仅更优越,而且更稳定。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
41
表 3.4 SAD-VER 在斯坦福公共 EEG 数据集上与 baseline 比较的结果
Methods / Tasks 6-Category 72-Category HF vs IO HF Intra-Class IO Intra-Class
LDA[82] 40.68 ± 5.54 14.46 ± 6.43 81.06 ± 3.66 18.30 ± 5.63 28.87 ± 10.57
Shallow[94] 49.04 ± 6.99 23.72 ± 10.95 - - 
LSTM [95] 44.77 ± 6.30 15.39 ± 6.01 80.67 - 
LSTM+CNN[94] 46.18 ± 6.79 23.23 ± 10.48 - - 
CNN[95] 50.00 ± 6.61 25.93 ± 10.67 83.10 - 
Attention CNN[94] 50.37 ± 6.56 26.75 ± 10.38 - - 
1-D WR CNN[96] 51.29 ± 7.57 28.68 ± 12.58 88.83 ± 3.49 24.64 ± 7.90 47.12 ± 16.26
CT-Slim[93] 51.96 ± 8.63 26.08 ± 13.68 88.78 ± 4.25 25.67 ± 8.15 47.53 ± 16.28
CT-Fit[93] 52.17 ± 8.15 27.14 ± 13.35 89.58 ± 3.97 26.77 ± 9.17 49.91 ± 17.44
CT-Wide[93] 52.33 ± 8.28 29.44 ± 13.51 89.64 ± 4.16 27.20 ± 9.10 50.59 ± 17.22
RLN[97] 52.69 29.92 - - 
SAD-VER(Ours) 61.55 ± 6.37 30.80 ± 13.46 91.42 ± 2.35 35.44 ± 8.66 52.84 ± 16.39
“-”表示这项指标没有在提出该方法的文献中展示
(2)SAD-VER 对多种开源模型的性能增益
在本文中,我们在 EEG 解码领域选择了几个著名的开源模型,通过比较它
们在有无 SAD-VER 数据增强时的性能差异,我们可以更好地评估我们提出的数
据增强框架的有效性。对这些模型的具体描述如下:
 EEGNet[98]:一种设计精巧的紧凑的卷积神经网络,在各种脑电图解码
任务中表现良好。
 FBCNet[99]:一种利用可分离卷积和多波段滤波的高效多视图神经网络。
 EEG-LSTM[100]:一种简单而有效的,用于 EEG 解码的长短期记忆(LSTM)
网络结构。
 DGCNN[101]:用于脑电图情绪识别的动态图卷积神经网络。
 EEG-Conformer[102]:一种由浅层卷积编码器和多个 Transformer 编码器
组成的紧凑的神经网络,用于基于 EEG 的运动想象识别。
该实验仍然遵循章节 3.5.1 中提到的范式,区别在于解码过程中的 STI-Net
部分被上述的开源解码网络所取代。图 3.8 (a)和(b)分别展示了 6 类和 HF-IO 分
类任务中所有 10 名受试者的平均解码精度。
图 3.8 中的结果显示,所有开源模型在经过 SAD-VER 的数据增强后,解码
性能都有所提高。6 类分类任务的提升幅度为 3.42%~12.77%,HF-IO 分类任务的
提升幅度为 2.3%~5.19%,表明了我们提出的 SAD-VER 的有效性。同时我们留
意到,EEG-LSTM 的解码性能表现较差,提升也不明显,这可能表明纯循环神经
网络结构不太适用于基于 EEG 的视觉刺激解码任务。而 DGCNN 在经过数据增


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
42
强后,解码性能最好,这可能表明在 EEG 视觉刺激解码任务中,基于图卷积的
解码模型对数据量大小相对其他结构的网络更加敏感。
图 3.8 多种开源网络在有无 SAD-VER 数据增强时的平均解码精度
(3)与其他数据增强方法的比较
为了更好地证明该方法的优越性,我们部署了几种流行的数据增强方法,并
将其与本文提出的 SAD-VER 进行了比较。这些方法包括分割和重建(S&R)[56],
beta-变分自动编码器(beta-VAE)[103],带有梯度惩罚项的 Wasserstein 对抗生成
网络(WGAN-GP)[104],去噪扩散概率模型(DDPM)[66]和去噪概率隐式模型
(DDIM)[87].在这些方法中,S&R 只是简单地裁剪和重排列原始 EEG 样本,而
其他的方法则是基于深度学习框架,可以生成新的样本。
具体来说,利用 beta-VAE、WGAN-GP、DDPM 和 DDIM 依次取代 SAD-VER
生成过程中的 AV-DPM,观察生成的 EEG 对解码性能的增益。由于在这些方法
中没有使用额外的超参数,所以在本实验中去掉了优化过程。本文通过章节 3.3.1
中提到的 FSTID 和 FSS 指标对生成的脑电图样本的质量进行评估,然后将这些
生成模型生成的 EEG 样本与真实的 EEG 样本一起输入 STI-Net,以评估解码性
能的改进程度,实验范式仍然遵循章节 3.5.1 中的设置。结果如表 3.5 所示。
表 3.5 AV-DPM 与几种流行的数据增强方法的比较
Metric/Methods 无数据增强 S&R beta-VAE WGAN-GP DDPM DDIM AV-DPM(Ours)
6 分类 Acc 52.00% 52.11% 52.35% 54.06% 59.83% 59.34% 61.55%
HFIO 任务 Acc 88.84% 88.80% 88.95% 89.47% 90.84% 90.70% 91.42%
平均 FSS↑ - - 0.3881 0.5667 0.8086 0.8004 0.8115
平均 FSTID↓ - - 155.21 142.45 128.18 129.95 121.00
“-”表示这项指标不适用于当前方法
从表 3.5 可以看出,与其他流行的数据增强方法相比,基于扩散式模型的方
法(DDPM、DDIM 和 AV-DPM)获得了更高的生成质量和更好的解码性能。此
外,SAD-VER 中的 AV-DPM 在所有指标中都获得了最好的结果,证明了该方法
的有效性和优越性。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
43
3.6 分析讨论
在本节中,我们设计了几种分析方法来深入研究 SAD-VER,旨在更好解释
其在各种分类任务中取得出色性能的原因。
3.6.1 中心核对齐分析
为了研究不同标签类间特征表示的多样性,我们采用了中心核对齐(CKA)
分析方法[105]。这一实验的目的是探讨 SAD-VER 生成的 EEG 是否能增强解码网
络中,不同标签对应的特征向量之间的差异性。CKA 的度量范围为 0~1,其中 1
表示最大相似度。两个标签对应的特征向量之间的 CKA 值越小,相似性越低,
说明不同类别之间特征的可分性越高。因此,我们期望在使用 SAD-VER 增强脑
电图数据后,观察到解码网络中不同标签之间的 CKA 值的下降。
具体来说,首先分别训练经过 SAD-VER 数据增强的 STI-Net 与未经过数据
增强的 STI-Net;然后,在受试者的真实 EEG 中选取 10%作为测试集样本(约
520 个),分别截取测试集样本经过这两个 STI-Net 的特征向量;最后采用中心核
对齐方法,分析不同标签对应的特征向量之间的表示相似程度。以受试者 1 为
例,本文统计了在 6 类样本分类任务下,各标签对应的特征向量之间的 CKA 值,
其图像如图 3.9 所示,(a)和(b)分别描述受试者 1 在未经过数据增强与经过数据
增强的 CKA 值。
图 3.9 受试者 1 在未经过数据增强与经过数据增强的 CKA 值
由图 3.9 可以发现,在受试者 1 中,大部分标签之间的 CKA 度量值在经过
SAD-VER 的数据增强后都出现了不同程度的降低。本文进一步将 CKA 度量方
法推广到其他受试者的 EEG 特征向量上,并且统计了所有不同标签之间的 CKA
度量平均值,结果如表 3.6 所示。
图 3.9 和表 3.6 的结果表明,对于大部分受试者,使用 SAD-VER 进行数据
增强可以降低网络不同标签对应的特征向量的相似程度,提升特征的可分性,进


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
44
而有效提高模型的解码性能。然而,受试者 4 和 9 的平均 CKA 值在经过 SAD
VER 的数据增强后下降幅度不明显,导致这一现象可能的原因在于 SAD-VER 对
受试者 4 和 9 的 EEG 生成质量不佳,从而几乎不能起到增加特征可分性的效果,
而这一结论和章节 3.5.2 中的实验结果相符合。
表 3.6 6 分类任务中所有受试者不同标签之间的 CKA 平均值
受试者/CKA 均值↓ STI-Net(无增强) SAD-VER 变化率
Sub1 0.7719 0.7288 -5.59%
Sub2 0.7660 0.7179 -6.27%
Sub3 0.7755 0.7337 -5.40%
Sub4 0.7998 0.7780 -2.73%
Sub5 0.7702 0.7133 -7.38%
Sub6 0.7452 0.7093 -4.82%
Sub7 0.8205 0.7410 -9.69%
Sub8 0.7179 0.6462 -9.99%
Sub9 0.7455 0.7354 -1.35%
Sub10 0.7986 0.7703 -3.54%
3.6.2 t-SNE 可视化分析
本文使用 t-SNE 可视化[106]方法,研究 SAD-VER 对视觉刺激 EEG 解码性能
的影响。t 分布随机邻域嵌入(t-SNE)是一种流行的统计降维和可视化方法。本
文选用受试者 2 的 EEG 数据,在经过 SAD-VER 的数据增强后的 STI-Net 与未
经过数据增强的 STI-Net 上进行分类,并且在图 3.10 上展示使用 t-SNE 将特征
分布可视化后的图像。其中,(a)是未经过数据增强的 STI-Net 在 6 分类任务下的
t-SNE 可视化结果;(b) 是经过 SAD-VER 数据增强的的 STI-Net 在 6 分类任务
下的 t-SNE 可视化结果;(c) 是未经过数据增强的 STI-Net 在 HF-IO 分类任务下
的 t-SNE 可视化结果;(d) 是经过 SAD-VER 数据增强的 STI-Net 在 HF-IO 分类
任务下的 t-SNE 可视化结果。
由图 3.10 可以发现,对于 6 分类任务,经过 SAD-VER 数据增强后的 STI
Net 模型相比于无数据增强的模型具有更明显的分类边界,从属于同一标签的样
本特征之间的距离更小,代表着其具有更好的分类性能。同样的现象也可以在
HF-IO 分类的 t-SNE 可视化结果中观察到,并且分类边界更加明显。以上实验现
象表明,经过 SAD-VER 数据增强后,STI-Net 中视觉刺激脑电信号的的特征可
分性更强,有利于提高分类性能。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
45
图 3.10 STI-Net 在未经过数据增强与经过数据增强后的 t-SNE 可视化图像
3.6.3 Grad-CAM 可视化分析
为了进一步探究 SAD-VER 与网络潜在推理过程的关系,本文使用 Grad
CAM[107]方法对经过 SAD-VER 的数据增强后的 STI-Net 与未经过数据增强的
STI-Net 提取的特征进行分析,观察解码过程中 SAD-VER 的生成 EEG 如何影响
STI-Net 关注的脑部区域,其结果如图 3.11 所示。其中,图 3.11(a)的图像是原始
的 EEG 数据在时间维度上平均并归一化后投影到脑电地形图后的图像,它显示
了每名受试者的脑部区域平均活跃程度;图 3.11(b)的图像是未经过数据增强的
STI-Net 应用 Grad-CAM 并投影到脑电地形图后的图像,它显示了在未经过数据
增强的情况下 STI-Net 在解码过程中关注的脑部区域;图 3.11(c)的图像是 SAD
VER 中的分类网络应用 Grad-CAM 并投影到脑电地形图后的图像,它显示了在
经过 SAD-VER 数据增强后,STI-Net 在解码过程中关注的脑部区域。
图 3.11(a)显示,大部分受试者的脑电地形图的中央沟后侧脑区较为活跃,出
现了明显的电位变化,这一部分对应的位置是大脑枕叶。而一部分受试者(比如
受试者 2-5)脑电地形图的前侧脑区出现了明显的正电位变化,这一部分对应大
脑的额叶部分。现有的生理学研究认为大脑枕叶部分负责处理视觉信息,额叶部
分则和记忆、判断、抽象思维等有关。因而在视觉刺激过程中,图 3.11(a)的现象
是符合生理学研究的。


第 3 章 基于自适应方差扩散模型的视觉刺激脑电信号增强方法
46
图 3.11 原始 EEG 在大脑地形图上的投影与 STI-Net 的 Grad-CAM 可视化图像
而对比图 3.11(b)与(c),可以发现在经过 SAD-VER 的数据增强后,STI-Net
在大部分受试者脑电地形图上的关注的区域有所扩大;在受试者 1、3、6、7-10
上,关键区域(比如额叶以及枕叶处)的地形图颜色加深,代表关注程度有所提
高;而在受试者 2 和 5 上,STI-Net 在进行数据增强后的关注区域和增强前有较
大差异,但是仍然能够给予脑区关键部分较大的权重。这表明 SAD-VER 在多数
情况下可以使分类模型更多地关注到脑部区域的重要部分,从而提高解码性能。
3.7 本章小结
本章节中提出了一种基于自适应方差扩散概率模型的自监督视觉刺激 EEG
数据增强框架(Self-supervised, Adaptive variance Diffusion probabilistic model
based Visual-stimulus EEG Augmentation Framework,SAD-VER),解决了基于 EEG
的视觉刺激解码的数据稀缺瓶颈,并显著提高了 EEG 解码性能。
相比于现有的研究,SAD-VER 在生成过程中首次将自监督学习与扩散式模
型相结合,引入自适应方差与最优方差上界的思想,对视觉刺激 EEG 进行高质
量、多样化的生成。本章将所设计的 SAD-VER 框架应用于斯坦福数据库公开的
EEG 基准数据集上,实验结果表明,所设计的模型可以利用真实 EEG 信号的自
然特征,合成高质量、多样化的模拟 EEG 信号,显著提升了多种深度学习模型
的解码性能,在各项解码准确率指标中达到了最先进水平。进一步的可视化分析
显示,SAD-VER 生成的 EEG 可以增强分类网络中的特征可分性,并且提升了分
类网络对重要脑部区域的关注能力,从而有效提高 EEG 的解码性能。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
47
第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
4.1 整体方法框架
本 章 设 计 了 一 种 基 于 线 性 注 意 力 机 制 的 卷 积 神 经 网 络 ( Linear-Attention
combined Convolutional Neural Network,LACNN),用于实现基于视觉刺激的脑
电图解码,以完成对脑电信号的识别。整体框架结构如图 4.1 所示。
LACNN 由 4 个部分组成。首先是数据预处理部分,在这一部分中原始的一
维 EEG 数据将预处理为二维形式。其次,将预处理的 EEG 数据送入卷积编码器
中,卷积编码器可以充分地提取 EEG 数据的局部特征。然后,将提取的 EEG 局
部特征向量输入两个堆叠的线性注意力模块中,线性注意力模块可以在局部特征
的基础上提取更高层次的全局特征。最后,我们将提取到的局部-全局特征输入
到分类器模块中,从而得到最终的分类结果。
图 4.1 LACNN 的整体框架图
在 数 据预处理块中,对 EEG 数据的预处 理操作包括等距方位投影法
(Azimuthal Equidistant Projection,AEP)[108],Clough-Tocher 插值[109]与 Z-Score
归一化,预处理流程如图 4.2 所示。
对于一维的原始 EEG 样本,LACNN 同样采用了和第 3 章中相似的二维投
影方法。但不同于将 124 个有效电极的 EEG 数据直接投影到 13×13 的二维矩
阵上,考虑到电极位置具有三维空间属性,LACNN 采用了等距方位投影法,以
最大程度保留电极间的空间信息。假如将人的头部视为一个近似的球面,将电极
视为球面上的点,那么采用等距方位投影可以保留距中心点的距离和方向,将球
面上的所有点投影到一个平面上。假如将人的头部视为一个近似的球面,将电极
视为球面上的点,那么采用等距方位投影可以保留距中心点的距离和方向,将球
面上的所有点投影到一个平面上。虽然常规的投影方法(赤道投影、极方位投影
和斜轴投影)均可用,但在球面投影中使用频率最高的是极方位投影,这种投影


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
48
会将所有经线和纬线划分为相等的部分,以保持球面各点之间的相对位置和距离
不变。
图 4.2 预处理流程图
经过等距方位投影后,利用 Clough-Tocher 插值法在二维平面上的电极之间
进行插值,对投影后的二维 EEG 数据进行平滑处理。应用等距方位投影和插值
后,将每个采样点的脑电图数据转换为大小为 32×32 的二维矩阵,单次试验的采
样点数为 32,因此预处理后的单次试验 EEG 数据尺寸为(1,32,32,32)。最
后,对脑电图数据进行简单的 Z-Score 归一化处理,如公式(4.1)所示:
norm
x
x


 ....................................(4.1)
其中 norm
x 代表归一化处理后的数据, 与 分别代表数据的均值和标准差,
它们由训练集中的数据计算求得,并直接应用于测试集。
4.2 卷积编码器
经过预处理后的 EEG 数据虽然相比原始的 EEG 数据保留了空间信息,且具
有更好的平滑程度,但同时也具有更大的尺寸,这也意味着它可能包含着许多不
必要的信息。因此,本文提出了一个简单且有效的卷积编码器,对 EEG 数据进
行压缩并提取其局部特征。卷积编码器的结构如图 4-3 所示。
对于单个输入样本,其尺寸为 1 W H T
x 
  。其中W 与 H 分别是预处理后每
个采样点处 EEG 数据矩阵的宽度和高度,T 为每个样本的采样点个数。因而,
对于三维卷积部分,其计算公式可用式(4.2)表示:
1, 1, 1 , , 111
3 (, )
WHT
kk k c cc wht
Conv D x k x k b
    
   

 ............(4.2)


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
49
图 4.3 卷积编码器的结构图
其中 x 是卷积层的输入; W H T
kkk
kc  
 是卷积核, c 表示指定的输出通道个数。
123
s , s , s 分别表示三个维度上的卷积步长; bc 为可学习的偏置项。
同理,对于二维卷积部分也有形式相似的计算公式,如式(4.3)所示:
1, 1 , 11
2 (, )
ST
kk cc st c
Conv D x k x k b
   
  

 ..................(4.3)
为了解决卷积神经网络在训练过程中出现的梯度爆炸/消失问题,我们在卷
积编码器中的一部分二维卷积层额外引入了 L2 范数约束。具体来说,当卷积层
中的权重参数超过 1 时,将权重参数除以其自身在维度为 0 处的 L2 范数,在不
改变权重数据分布的情况下,对权重进行缩放。计算公式如式(4.4)所示:
2
2 (, ) 2 (, ) 1
2 (, ) 2 (, ) 2 (, ) 1
2 (, )
cc
cc
c c
dim=0
Conv D x k Conv D x k
ConstrainedConv D x k Conv D x k Conv D x k
Conv D x k

  

(4.4)
由公式(4.2),(4.3),(4.4)可以描述卷积编码器中的每一层的计算。计
算过程如下所述:
Layer 1:xi 为卷积编码器的输入,两个具有 8 个卷积核的三维卷积层并行,
然后在输出通道上进行级联。两个三维卷积层分别具有不同的卷积核大小与填充
参数。随后进行批量归一化(Batch Normalization,BN),使用指数线性单元
(Exponential Linear Units,ELU)作为激活函数。计算过程如公式(4.5)所示:
88 1 1,1 1,1 1,2 1,2
( ( ( 3 ( , ), 3 ( , ))))
L L iL L iL
x  ELU BN Concat Conv D x k Conv D x k ...(4.5)
输出的特征 xL1 进行 Dropout 随机失活,然后进行尺寸重塑。对于 2
1
CW H T
xL  

 ,
将其重塑为 2 ( )
1
C WH T
xL  

  的形式。其中 C 为本层中 Conv3D 的卷积核个数,
W , H  为输出特征的宽和高,T 为特征中的采样点个数。
Layer 2:采用一个具有 32 个卷积核,且带有 L2 范数约束的二维卷积层。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
50
随后进行批量归一化,使用 ELU 作为激活函数,最后使用平均池化层以压缩特
征个数。计算过程如公式(4.6)所示:
32 2 12
( ( ( 2 ( , ))))
L LL
x  AvgPool ELU BN ConstrainedConv D x k ......(4.6)
输出的特征 xL2 进行了 Dropout 随机失活。
Layer 3:本层采用了一个 Inception[110]式的结构,3 个具有 32 个卷积核的二
维卷积层并行,并且将它们的结果相加。每个卷积层均采用了不同大小的卷积核
与填充参数,以充分地分析不同时空尺度上的脑电图信号。计算过程如公式(4.7)
所示:
32 32 32 3 2 3,1 2 3,2 2 3,3
2( , ) 2( , ) 2( , )
L LL LL LL
x  Conv D x k  Conv D x k  Conv D x k ...(4.7)
Layer 4:同 Layer 2 一致,采用一个具有 32 个卷积核,且带有 L2 范数约束
的二维卷积层。计算过程如公式(4.8)所示:
32 4 34
2( , )
L LL
x  ConstrainedConv D x k ...................(4.8)
Layer 5:采用一个具有 16 个卷积核的二维卷积层,随后进行批量归一化,
使用 ELU 作为激活函数。计算过程如公式(4.9)所示:
16 45
( ( 2 ( , )))
o LL
x  ELU BN Conv D x k ...................(4.9)
在经过卷积编码器后,输出特征的空间维度记为 F ' ,时间维度记为T '。每
个卷积层中的具体参数设置如表 4.1 所述。
表 4.1 卷积编码器的具体参数设置
层数/具体参数
输入通道数
in channels
输出通道数
out channels
卷积核大小
kernel size
步长
stride
填充数
padding
Layer 1 – Conv3D(1) 1 8 (8, 8, 33) (4, 4, 1) (0, 0, 16)
Layer 1 – Conv3D(2) 1 8 (8, 8, 65) (4, 4, 1) (0, 0, 32)
Layer 2 – Conv2D 16 64 (4, 1) (3, 1) (0, 0)
Layer 2 – AvgPool 64 64 (1, 2) (1, 2) (0, 0)
Layer 3 – Conv2D(1) 64 64 (3, 2) (1, 1) (1, 1)
Layer 3 – Conv2D(2) 64 64 (3, 4) (1, 1) (1, 2)
Layer 3 – Conv2D(3) 64 64 (3, 8) (1, 1) (1, 4)
Layer 4 – Conv2D 64 32 (16, 1) (1, 1) (0, 0)
Layer 5 – Conv2D 32 32 (1, 35) (1, 1) (0, 17)
4.3 线性注意力模块
考虑到具有多头注意力机制的 Vanilla Transformer[49]中,每一个注意力头部


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
51
的 Attention 矩阵的计算公式如公式(4.10)所示:
(, ,) ( )
T
head
K
QK
Attention Q K V softmax V
d
 ..................(4.10)
其中 Q , K ,V 分别指 Query,Key,Value 矩阵, dK 指 Key 矩阵的维度。我们
假设 , , n d
QKV 
  ,则有 T n n
QK 
 。那么我们可以推导出, ( T )
softmax QK 的计
算复杂度为 2
(n ) 。
为了解决 Vanilla Transformer 具有二次计算复杂度的问题,本文采用了一种
新颖的线性注意力模块,它在具有接近线性复杂度的同时,仍然保持类似于
Vanilla Transformer 的全局特征提取功能。线性注意力模块的核心是门控注意力
单元(Gated Attention Unit,GAU)和混合注意模块(Mixed-Chunk Attention),
其结构图如图 4.4 所示。
图 4.4 线性注意力模块的计算流程图
4.3.1 门控注意力单元
受[85]的启发,我们将门控线性单元(Gated Linear Unit,GLU)和 Self-Attention
机制结合,取代 Vanilla Transformer 中的前馈神经网络(Feed Forward Network,
FFN)层。注意力权值计算如公式(4.11)所示:
( ) ( ), ( )
o u u vv
O  U  AV W U  xW V  xW ..............(4.11)
其中 n d
x
  为输入; , ,
de ed uv o
WW W

    为线性变换矩阵; 为 Hadamard 积;
u, v
  为 SiLU(Sigmoid Linear Unit)激活函数; n n
A
  为 Attention 矩阵,它负
责融合 token 之间的信息。
近期的一项研究[111]指出,门控单元会降低对 Attention 的依赖。因此我们可
以在不影响性能的前提下,使用一种比 Vanilla Transformer 中的多头自注意力
(Multi-Head Self-Attention,MHSA)更简单,计算代价更小的注意力机制。其
计算如公式(4.12)所示:


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
52
2
1 () ()
( ) ()
T
zz
QZ KZ
A ReLU Z xW
 n s  ...............(4.12)
其中 d s
Wz 
  为线性变换矩阵;z 为 SiLU 激活函数;n 为输入序列长度,取1 / n
作为一个归一化常数,用于消除序列长度的影响;s 为注意力头的大小; 2
ReLU
为 ReLU 激活函数的平方,这一激活函数是通过神经架构搜索(Neural Architecture
Search,NAS)得到的最优激活函数;Q(Z ), K(Z) 由 Z 通过简单的仿射变换求得。
将式(4.12)代入到式(4.11)中,就得到了门控注意力单元。
4.3.2 混合注意力模块
我们在章节 4.3.1 中介绍的门控注意力单元虽然计算代价相比 Vanilla
Transformer 更小,但其仍然是具有二次复杂度的。因此我们采用了一种“局部
全局”分块混合[84]的方式,将门控注意力单元推广到此结构上,以降低其计算复
杂度。分块混合融合了部分注意和线性注意的好处。
首先,对于长度为 n 的输入序列 x ,我们将它不重叠地划分为 n / c 个长度为
c 的块。其次,我们通过公式(4.12),将 x 变换为 Z 。最后,我们参考 4.3.1 中
的工作,对 Z 采用四个仿射变换,分别求得局部注意力和全局注意力的 Query,
Key 矩阵 ( ), ( ), ( ), ( )
llg g
Q Z K Z Q Z K Z ,而U ,V 则由 x 通过公式(4.11)变换得到。
假设这里计算的是第 a 个块的注意矩阵,则图 4.4 中线性注意力模块的局部注意
力模块和全局注意力模块计算方式如图 4.5 所示。图 4.5 (a)和(b)分别描述了线性
注意力模块中局部注意力和全局注意力的计算方式
局部注意(Local Attention):这一部分的 Attention 矩阵采用 GAU 中的计算
方法,块内长度为 c ,则局部注意矩阵如公式(4.13)所示:
2 () ()
1( )
l lT l aa aa
Q ZK Z
Attn ReLU V
cs
 ....................(4.13)
这代表的是每个块的 token 内部自行交互,其计算复杂度大约为 (nc) 。当 c  n
时,Local Attention 具有接近于线性的计算复杂度,明显优于 Vanilla Transformer
具有的 2
(n ) 计算复杂度。
全局注意(Global Attention):这一部分的 Attention 矩阵采用线性计算方法。
其计算如公式(4.14)所示:
/
1
1 () ()
nc g g gT a a hh h
Attn Q Z K Z V
n
  .........................(4.14)


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
53
在这一部分中每一个划分块中的 ( )
gT
K Z V 求和,并与当前划分块的 ( )
g
Qa Z 相乘,
代表了当前划分块与整个输入序列的交互。
图 4.5 线性注意力模块中局部注意力和全局注意力的计算流程图
然后,将两种 Attention 结合,就可以得到混合注意模块。每一个划分块的注
意力权值计算方式如式(4.15)所示:
[ ( )]
lg a a a ao
O  U  Attn  Attn W ........................(4.15)
作为线性 Attention 模块的最后部分,我们记所有划分块的注意力权值为
total
O ,引入门控线性单元 Gate 与残差连接,可以得到线性 Attention 模块的最终
输出,其可以表示为式(4.16):
( ) ()
total total g g
Y  W Gate  O  x Gate  xW ..............(4.16)
其中 ,
total g
W W 均为线性变换矩阵,g 为 SiLU 激活函数。
4.4 分类器模块
在输入的 EEG 数据经过卷积编码器和 2 层堆叠的线性注意力模块后,我们
将得到的 EEG 局部-全局特征输入到分类器中。该分类器是一个具有 3 层深度的
多层感知机,它用于对提取到的特征进行分类,并输出预测的 EEG 标签。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
54
4.5 实验结果
4.5.1 实验条件与超参数设置
本研究中使用的深度学习框架是 PyTorch 1.11,实验部署平台为 Intel Core i9
12900H @ 2.50GHz,32GB RAM,NVIDIA GeForce RTX 3070ti Laptop 独立 GPU。
实验采用了 10 折交叉验证,以一种依赖于单个受试者的方式来评估我们的
模型,其中训练集数据和测试集数据来自同一受试者。每一折中,有大约 4670
个训练集数据与 520 个测试集数据。我们以 10 折验证的平均精度作为一个受试
者的结果,并计算所有受试者的平均结果作为模型的最终精度。在训练过程中,
我们采用 Adam 优化器来更新本模型中的可训练参数,因为它的有效性在各种基
于深度学习的应用中得到了广泛的验证。我们也引入了自适应学习率,随 epoch
数增大而逐渐减小学习率,以避免陷入局部最优值。学习率的更新公式如式(4.17)
所示:
1
1 0.73
next prev
lr lr
epoch

  ........................(4.17)
初始学习率设置为 0.001,epoch 数设置为 50。同时我们还引入了早停法以缓解
过拟合现象,当测试集损失连续 20 个 epoch 不下降时,终止训练并保存最佳的
测试结果以及模型参数。
4.5.2 实验结果
我们在斯坦福公共 EEG 数据集上评估了五种不同的分类任务,即 6 类样本
分类、72 类样本分类、HF vs IO 二类类间分类、HF-12 类类内样本分类和 IO-12
类类内样本分类,并且与近些年的基准测试方法进行比较。结果如表 4.2 所示。
通过与近年的基准测试方法的比较,可以看出我们的方法在多数分类任务中都取
得了最先进的性能。
以近年最先进的模型 CT-Wide 为基准,本模型在 6 类样本分类任务、72 类
样本分类任务、HF vs IO 二类分类任务、HF 类内样本分类任务中,领先幅度分
别为 1.80%,0.39%,0.05%,2.13%。不过在 IO 类内样本分类任务中,我们的模
型稍逊于 CT-Wide,落后幅度为 1.44%。
我们也以 6 类样本分类、72 类样本分类为指标,分析本模型在不同受试者
数据上的具体表现,每名受试者的平均分类准确率如图 4.6 所示。可以观察到,
受试者 6 的 EEG 数据具有最好的可分性,它在 6 类样本分类和 72 类样本分类任
务中取得了最好的结果。而受试者 8 的 EEG 数据则相反,在两个分类任务中的
性能都最差,这表明受试者 8 的脑电信号相对来说最难进行分类。这和先前的研
究[94]中的结论一致。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
55
表 4.2 LACNN 在斯坦福公共 EEG 数据集上与 baseline 比较的结果
Methods / Tasks 6-Category 72-Category HF vs IO HF Intra-Class IO Intra-Class
LDA[82] 40.68 ± 5.54 14.46 ± 6.43 81.06 ± 3.66 18.30 ± 5.63 28.87 ± 10.57
Shallow[94] 49.04 ± 6.99 23.72 ± 10.95 - - 
LSTM[95] 44.77 ± 6.30 15.39 ± 6.01 80.67 - 
LSTM+CNN[94] 46.18 ± 6.79 23.23 ± 10.48 - - 
CNN[95] 50.00 ± 6.61 25.93 ± 10.67 83.10 - 
Attention CNN[94] 50.37 ± 6.56 26.75 ± 10.38 - - 
1-D WR CNN[96] 51.29 ± 7.57 28.68 ± 12.58 88.83 ± 3.49 24.64 ± 7.90 47.12 ± 16.26
CT-Slim[93] 51.96 ± 8.63 26.08 ± 13.68 88.78 ± 4.25 25.67 ± 8.15 47.53 ± 16.28
CT-Fit[93] 52.17 ± 8.15 27.14 ± 13.35 89.58 ± 3.97 26.77 ± 9.17 49.91 ± 17.44
CT-Wide[93] 52.33 ± 8.28 29.44 ± 13.51 89.64 ± 4.16 27.20 ± 9.10 50.59 ± 17.22
LACNN (Ours) 54.13 ± 7.38 29.83 ± 12.61 89.69 ± 3.59 29.33 ± 7.74 49.15 ± 14.51
“-”表示这项指标没有在提出该方法的文献中展示
图 4.6 所有受试者的 6 类样本分类与 72 类样本分类平均准确率
同时,为了更好地理解 LACNN 的解码性能,我们对所有受试者在 6 分类任
务中进行了混淆矩阵(Confusion Matrix)实验,实验结果如图 4.7 所示。可以观
察到对于 LACNN 而言,HF 类别在大多数受试者中的可区分程度最高,因为 HF
类的识别准确率优于其他类。另一方面,FV 类和 IO 类互相之间的误分类概率最
高,代表它们更加难以区分。
最后,我们也统计了本模型采用的参数数量并进行了比较,所有的模型参数
数量都在 72 类样本分类任务下进行统计,结果如表 4.3 所示。相比于目前最先


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
56
进的模型 CT-Series ,本模型所需的参数量仅为它们的 8.59%,3.40%,1.66%。
可以看出本模型在参数数量和网络复杂度方面具有极其明显的优势,同时在大部
分指标上也取得了目前的最先进水平。
图 4.7 所有受试者在 6 分类任务下的混淆矩阵
表 4.3 LACNN 的参数数量统计
Methods CT-Slim CT-Fit CT-Wide LACNN (Ours)
Params(million) 4.56 11.52 23.55 0.39


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
57
4.5.3 消融实验
在本节中我们设置了一系列消融实验,包括在 LACNN 中去掉线性 Attention
模块中的局部注意模块、全局注意模块、门控单元与整个线性 Attention 模块,
以更好地了解线性 Attention 模块中不同部分对解码性能的贡献。为了方便起见,
我们记去掉局部注意模块的模型为 LACNN-LA,去掉全局注意模块的模型为
LACNN-GA,去掉门控单元的模型为 LACNN-GU,去掉整个线性 Attention 模块
的模型为 LACNN-WLA。
表 4.4 展示了去掉不同模块后,模型在 6 类样本分类、72 类样本分类、HF
vs IO 二类类间分类、HF 类内样本分类和 IO 类内样本分类任务中,10 名受试者
的 EEG 解码平均准确率。
表 4.4 去掉不同模块后 LACNN 的性能表现
Methods/Tasks 6-Category 72-Exemplar HF vs IO HF-Exemplar IO-Exemplar
LACNN 54.13 ± 7.38 29.83 ± 12.61 89.69 ± 3.59 29.33 ± 7.74 49.15 ± 14.51
LACNN-LA 53.49 ± 7.42 28.42 ± 12.43 88.59 ± 3.60 27.83 ± 7.80 47.21 ± 13.92
LACNN-GA 53.58 ± 7.38 30.62 ± 13.06 88.60 ± 3.79 27.81 ± 8.09 47.75 ± 15.05
LACNN-GU 53.67 ± 7.31 29.54 ± 12.77 88.73 ± 3.79 28.17 ±7.71 47.56 ± 15.39
LACNN-WLA 53.53 ± 7.37 30.65 ± 13.35 88.68 ± 4.01 27.23 ± 7.10 47.86 ± 15.25
我们进一步使用 t-SNE 可视化[106]方法,研究线性注意力模块中的不同部分
对解码性能的影响。t 分布随机邻域嵌入(t-SNE)是一种流行的统计降维和可视
化方法。我们选用受试者 1 的 EEG 数据,在 LACNN 及其变体上进行 HF vs IO
二类类间分类,并且在图 4.8 上展示使用 t-SNE 将特征分布可视化后的图像。
图 4.8 LACNN 及其衍生模型的 t-SNE 可视化图像
图 4.8 中,红色代表 HF 类,蓝色代表 IO 类。可以看出(a)、(b)、(c)、(d)在
t-SNE 可视化后均具有较为明显的分类界限,而(e)的分类界限相比之下较不明
显。但是(b)、(c)、(d)中出现了较多的误分类情况。相比于其他模型,(a)中的
LACNN 的分类效果最好。它具有明显的分类边界,并且误分类的样本数少于其


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
58
他模型。表 4-4 中报告的 LACNN 及其变体的 HF vs IO 平均准确率也可以印证此
结论。
最后,我们结合所有分类任务中的表现,对线性 Attention 模块中的不同部
分对解码性能的贡献进行探讨。我们采用了一种简单直观的模型评分机制:模型
在某项指标上表现越好,其得分越高,从最高分到最低分依次为 2,1,0,-1,
-2 分。最后统计所有模型的总得分,得分越低,代表去掉这一部分的模型表现越
差,从而说明这一部分越重要。
表 4.5 LACNN 及其衍生模型评分结果
Models/Tasks 6-Category 72-Category HF vs IO HF IO Total Score
LACNN 2 0 2 2 2 8
LACNN-LA -2 -2 -2 0 -2 -8
LACNN-GA 0 1 -1 -1 0 -1
LACNN-GU 1 -1 1 1 -1 1
LACNN-WLA -1 2 0 -2 1 0
表 4.5 统计了所有模型的评分结果。根据评分我们可以得到以下结论:
(1)去掉局部注意模块后的 LACNN-LA 模型得分最低,在 5 项指标中有 4
项表现最差,这也说明了局部注意模块对模型解码性能的贡献最大。
(2)得分第二低的是去掉全局注意后的 LACNN-GA 模型,说明全局注意
模块对模型解码性能的贡献也是第二重要的。但是它的表现和去掉整个线性
Attention 模块的 LACNN-WLA 模型,以及去掉门控单元的 LACNN-GU 模型较
为接近。
(3)去掉门控单元的 LACNN-GU 模型取得了第二高的分数,这说明门控
单元对模型解码性能的贡献程度较低。
(4)保留了完整线性 Attention 模块的 LACNN 模型取得了最高的分数,尽
管在 72 类样本分类任务中得分较低,但整体而言仍然是表现最好的模型,在 5
项指标中有 4 项都是最佳。
4.5.4 结合数据增强框架 SAD-VER 的解码效果
在本节中我们将第三章中介绍的 SAD-VER 与 LACNN 相结合,利用 SAD
VER 对视觉刺激 EEG 进行生成以完成数据增强。实验采用了 10 折交叉验证,
以一种依赖于单个受试者的方式来评估 LACNN + SAD-VER 的效果。其中,单
个受试者的每个标签生成的 1024 份 EEG 数据全部用作训练集,真实的 EEG 按
9:1 的比例划分为训练集和测试集,并进行交叉验证。每一折交叉验证中,有大
约 10800 个训练集数据与 520 个测试集数据。其他参数设置和实验范式仍然遵循
章节 4.1 以及章节 4.5.1 中提及的设置和范式。
对 LACNN + SAD-VER 的性能评估在 6 分类任务和 HF-IO 二分类任务中进


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
59
行,每个受试者都进行 10 折交叉验证,并且取在 10 次验证中的平均准确率作为
最终结果。通过比较 LACNN 在有无 SAD-VER 数据增强下的分类性能,我们可
以评估 SAD-VER 对 LACNN 的性能增益。每个受试者的分类准确率以及 10 个
受试者的平均分类准确率如图 4.9 所示。图 4.9 (a)和(b)分别展示了 LACNN 在有
无 SAD-VER 增强时,6 分类任务和 HF-IO 任务的性能表现。
图 4.9 LACNN 在有无 SAD-VER 增强时 6 分类和 HF-IO 任务的性能表现
通过图 4.9 可以观察到,在经过 SAD-VER 增强后,LACNN 的性能得到了
显著提升。6 分类下 10 名受试者的平均解码准确率提升了 9.84%,HF-IO 分类任
务下平均解码准确率提升了 2.52%,性能提升幅度与 STI-Net 在经过 SAD-VER


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
60
增强后的增幅接近。并且,LACNN + SAD-VER 的视觉刺激 EEG 解码性能取得
了目前最好的效果,充分说明了我们提出的方法的有效性。
进一步地对每个受试者进行单独分析可以发现,每个受试者的脑电图分类性
能的改善程度有所区别。例如,在 SAD-VER 数据增强后,受试者 2 和受试者 7
在 6 类分类任务中的识别准确率提高幅度分别达到 25.56%和 21.13%,在 HF-IO
分类任务中提高幅度则分别为 5.15%和 4.58%,大幅超过了平均水平的 9.84%与
2.52%。除此之外,受试者 1 和受试者 10 的提升幅度同样很明显,在 6 分类任务
和 HF-IO 二分类任务中达到或者接近平均水平。
但是,与这些分类准确率提升幅度极大的受试者相比,受试者 3、受试者 4
和受试者 9 的分类准确率提升幅度则明显落后于平均水平。这与我们在章节 3.5.2
的实验中观察到的现象一致。
4.6 分析讨论
在这一节中我们引入了两种分析方法,尝试对 LACNN 中的线性注意力模块
进行更深入的研究,以解释它在多项指标中取得优异性能的原因。
4.6.1 中心核对齐分析
为了研究我们提出的 LACNN 中线性注意力模块(Linear Attention Module)
的类间特征表示多样性,我们在受试者上选取 10 折验证中一折的全部的测试集
样本(约 520 个),并且在训练过程中的最后一个 epoch 上分别截取测试集样本
在线性注意力模块之前、经过第一个线性注意力模块之后、经过第二个线性
Attention 模块之后的特征向量;然后根据求得每个样本标签对应的特征向量均
值;最后采用中心核对齐(CKA)方法[105],分析不同标签对应的特征向量之间
的表示相似程度。
图 4.10 受试者 1 的 CKA 度量分析
两个标签对应的 CKA 度量值越小,代表这两个标签对应的特征向量相似程
度越低,从而说明特征之间的可分性越高。因而我们期望于得到较低的 CKA 度
量值,或者是在经过线性注意力模块后,各个标签相互之间的 CKA 度量值降低。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
61
以受试者 1 为例,我们统计了在 6 分类任务下,各标签对应的特征向量之间的
CKA 值,其图像如图 4.10 所示。可以发现,在受试者 1 中,除了 IO-HF 与 AF
IO,其他标签之间的 CKA 值在经过线性注意力模块后都出现了不同程度的降低。
我们进一步将以上的 CKA 度量方法推广到其他受试者的 EEG 特征向量上,
并且进行了在 6 分类、72 分类、HF 类内分类、IO 类内分类、HF-IO 二分类任务
下的 CKA 分析实验,以深入探究线性注意力模块是否能提高特征之间的可分性。
实验仍然遵循前文提到的实验范式。
(1)6 分类任务下的 CKA 分析实验
我们计算了 6 分类任务中所有受试者的 15 个标签对之间的 CKA 值变化之
和。结果如表 4.6 所示。
表 4.6 6 分类任务下所有受试者 EEG 特征向量的 CKA 度量值变化总和
Subject/Variation Before LA - After 1st LA After 1st LA – After 2nd LA
Sub1 -0.0889 -0.1938
Sub2 -0.0458 -0.2272
Sub3 -0.5338 -0.8554
Sub4 -0.1540 -0.4579
Sub5 -0.0196 -0.0222
Sub6 -0.0214 -0.0457
Sub7 -0.0227 -0.0579
Sub8 -0.0127 -0.0189
Sub9 -0.8679 -1.6654
Sub10 -0.0016 -0.0018
在 6 分类任务中所有受试者的 EEG 特征向量上,每个标签对的 CKA 值变
化之和为负。具体来说,在通过第二个线性注意力模块后,CKA 值下降得更为
显著。这说明线性注意模块可以通过降低 EEG 不同标签对应的特征向量之间的
相似性来提高特征的可分性,从而有效提高模型的解码性能。此外,多个线性注
意力模块的级联可以进一步增强 6 分类任务中特征的可分性。
(2)72 分类任务下的 CKA 分析实验
我们计算了 72 分类任务中,所有受试者的 2556 个标签对之间的 CKA 值变
化之和。相应的结果如表 4.7 所示。
在 72 分类任务下,由于一共有 2556 对标签,因而 CKA 度量值变化的总和
相比于 6 分类任务明显增大。我们可以发现,在经过线性 Attention 模块后,CKA
度量值出现了下降,表明特征之间的差异增大,特征之间的可区分性得到了提升。
不过在经过第二个线性 Attention 模块后,CKA 度量值的下降有所减缓,这和 6
分类任务中的现象有所不同。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
62
表 4.7 72 分类任务下所有受试者的 CKA 度量值变化总和
Subject/Variation Before LA-After 1st LA After 1st LA-After 2nd LA
S1 -435.144536 -162.842827
S2 -350.339053 -151.486345
S3 -404.060099 -127.299802
S4 -410.467886 -181.112569
S5 -385.597304 -110.970737
S6 -386.292321 -132.388901
S7 -430.509604 -138.888465
S8 -380.37496 -134.901773
S9 -469.670982 -148.318739
S10 -384.722711 -126.361377
(3)HF 类内分类任务下的 CKA 分析实验
我们统计了所有受试者在 HF 类内分类任务下所有标签对之间的 CKA 值变
化之和,统计结果如表 4.8 所示。
表 4.8 HF 类内分类任务下所有受试者的 CKA 度量值变化总和
Subject/Variation Before LA-After 1st LA After 1st LA-After 2nd LA
S1 -7.89484 -7.164449
S2 -6.382625 -6.617938
S3 -6.230641 -5.509134
S4 -6.579722 -6.371074
S5 -7.633295 -5.026391
S6 -7.437389 -6.114713
S7 -6.059024 -6.692709
S8 -5.580237 -6.614529
S9 -4.894416 -6.768025
S10 -6.200064 -6.083978
HF 类内分类任务一共有 66 对标签。在经过线性注意力模块后,CKA 度量
值出现了下降。一部分受试者在经过第一个线性注意力模块时,CKA 度量值下
降更多,而另一部分受试者则展现出相反的现象。
(4)IO 类内分类任务下的 CKA 分析实验
我们统计了所有受试者在 IO 类内分类任务下所有标签对之间的 CKA 值变
化之和,统计结果如表 4.9 所示。
IO 类内分类任务一共有 66 对标签。通过表 4.9 可以得到和 HF 类内分类任
务下相似的结论:在经过线性 Attention 模块后,CKA 度量值出现了下降。但是


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
63
多数受试者的 CKA 度量值在经过第一个现象 Attention 模块后下降更明显。
表 4.9 IO 类内分类任务下所有受试者的 CKA 度量值变化总和
Subject/Variation Before LA-After 1st LA After 1st LA-After 2nd LA
S1 -8.289782 -6.079629
S2 -5.548158 -6.057364
S3 -10.973081 -8.280363
S4 -9.67876 -5.369293
S5 -9.783928 -6.357365
S6 -5.454245 -4.582278
S7 -7.022575 -6.362004
S8 -10.366835 -7.576928
S9 -9.611879 -6.155739
S10 -7.311012 -6.035459
(5)HF-IO 二分类任务下的 CKA 分析实验
最后,我们统计了 HF-IO 二分类任务下每个标签对之间的 CKA 度量值,并
且在表 4.10 展示了所有受试者的 CKA 度量值变化之和。HF-IO 二分类任务下只
有一个标签对。
表 4.10 HF-IO 分类任务下所有受试者的 HF-IO 特征 CKA 度量值变化
Subject/Variation Before LA-After 1st LA After 1st LA-After 2nd LA
S1 -0.008014 -0.011455
S2 -0.000649 -0.000621
S3 -0.005573 -0.017198
S4 -0.003929 -0.015851
S5 -0.001465 -0.0018
S6 -0.018812 -0.041376
S7 -0.000001 0
S8 -0.011996 -0.022255
S9 -0.065557 -0.066464
S10 -0.000011 -0.000084
从表 4.10 可以看出,在 HF-IO 二分类任务下,除了受试者 6、8、9,其余受
试者的 CKA 度量值的下降不明显。这表明在 HF-IO 二分类下,线性注意力模块
对于提高特征之间可区分性的作用不明显。我们分析造成这一现象的原因可能是
由于 HF 类脑电信号与 IO 类脑电信号的特征的差异已经足够大,线性注意力模
块难以在这方面带来更多的提升。然而,对于简单的二分类任务,线性 Attention
模块仍然能在一定程度上提高分类性能。


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
64
4.6.2 Grad-CAM 可视化分析
参考章节 3.6.3 中的工作,我们将 Grad-CAM 可视化方法[107]推广到 LACNN
以及它的各种衍生模型 LACNN-LA、LACNN-GA、LACNN-GU 和 LACNN-WLA
上。Grad-CAM 可视化结果如图 4.11 所示。其中,LACNN-LA、LACNN-GA、
LACNN-GU 版本分别是去掉了局部注意、全局注意和门控连接后,线性注意力
模块的 Grad-CAM 可视化结果。图 4.11 中,红色或蓝色越深代表该区域活跃程
度越高,或者网络对该区域关注程度越高。通过观察以上结果,可以更好地了解
线性注意力模块的不同部分如何作用于输入的脑电信号。LACNN-WLA 版本由
于去掉了整个线性注意力模块,因此取卷积编码器作为可视化对象,观察卷积编
码器如何作用于输入的脑电信号。
图 4.11 的第 1 行展示的是原始 EEG 地形图图像。可以看出,对于大多数受
试者,他们的中央沟后脑区更为活跃,显示出极为明显的正电位或者负电位。该
区域主要对应于大脑的枕叶部分,该部分负责处理视觉信息。而在另一些受试者
(例如,受试者 2-5)中,中央沟的前部更为活跃。这个区域主要对应于大脑的
额叶部分,它与记忆、判断和抽象思维有关。这些观察结果与视觉刺激相关的生
理学知识相一致。
而通过进一步观察图 4.11 中第 2 行到第 5 行中,LACNN 及其衍生模型(的
Grad-CAM 可视化部分,我们可以得到以下结论:
(1)从第 2 行的图像中可以观察到,在一些受试者(例如,受试者 2、3、
5 和 7)中,线性注意力模块给大脑的额叶或枕叶区域之一分配了更高的权重。
在其他受试者中(例如,受试者 1、4、8 和 10)中,线性注意力模块为大脑额叶
和枕叶这两个区域都分配了更高的权重。这两个区域都和大脑的视觉活动具有明
显的相关性,它们分别与处理视觉信息和判断思维能力有关。这说明线性注意模
块在大多数情况下能够集中于大脑的重要区域,给予该区域的信号更高的权重,
从而提高了脑电图的解码性能。
(2)LACNN-LA、LACNN-GA、LACNN-GU 均可以在大部分受试者的脑电
信号中,捕捉到一部分重要脑区的信息。然而,它们之间的关注区域总是呈现出
较大的区别。例如在受试者 1 中,LACNN-LA 可以关注到大脑的额叶和枕叶部
分,而 LACNN-GA 和 LACNN-GU 则只关注到枕叶部分,其中 LACNN-GA 还
额外地关注到了大脑的中央沟区域。这表明线性 Attention 模块中的局部注意、
全局注意、门控连接模块对脑部区域的关注能力呈现多样性,对于来自同一受试
者的脑电信号,它们分别注意到了脑部区域的不同部分。这种针对不同区域的关
注能力可能更有利于提高线性 Attention 模块的特征提取能力。
(3)LACNN-WLA 所关注的脑部区域总是呈大块状、连续状的。由于在
LACNN-WLA 中我们取卷积编码器作为 Grad-CAM 可视化对象,因此对 LACNN


第 4 章 基于线性注意力机制的视觉刺激脑电信号解码模型
65
WLA 的 Grad-CAM 可视化实验实际上展示了卷积编码器对脑部区域的关注程度。
(4)根据(2)中的结论,不难联想到,对于 LACNN 以及其他衍生模型,
它们所包含的卷积编码器同样具有 LACNN-WLA 中的卷积编码器的性质。我们
注意到,相比于卷积编码器,LACNN、LACNN-LA、LACNN-GA、LACNN-GU
中的线性注意力模块关注区域更加零碎化、细节化,关注的区域和卷积编码器有
显著不同,同时各个衍生模型之间的关注区域也有所区别。这表明线性注意力模
块可以关注到脑电信号中的细节部分,而这可能是线性注意力模块能够提升脑电
分类性能的原因之一。
图 4.11 原始脑电图与线性注意力模块的 Grad-CAM 可视化图像
4.7 本章小结
本章节提出了一种基于线性注意力机制的卷积神经网络(Linear-Attention
combined Convolutional Neural Network,LACNN),用于实现基于视觉刺激的脑
电图解码,它在大多数视觉刺激脑电解码任务中都具有优秀的性能,同时具有良
好的计算效率。
LACNN 的核心是线性注意力模块,它在保持类似于 Vanilla Transformer 的
全局感知能力的同时具有更低的计算复杂度。我们提出的 LACNN 在斯坦福数字
存储库的公共 EEG 数据集上取得了 SOTA 效果。同时,LACNN 在计算复杂度
和模型参数量上相比于现有方法也有着巨大优势。同时我们发现,线性注意力模
块可以提高不同类别特征之间的可分性从而提高解码效果,并且可以定位符合范
式原则的关键脑部区域信息,从而有效提升视觉刺激 EEG 解码的准确率。


第 5 章 总结与展望
66
第 5 章 总结与展望
5.1 总结
本文以计算机科学中的神经网络技术为基础,对视觉刺激脑电信号进行深入
分析,研究脑电信号与人类视觉之间的联系,并构建了大脑视觉信息与外部视觉
刺激之间的联系。一方面,我们开发了一种结合卷积神经网络与线性注意力机制
的视觉刺激脑电信号解码模型,通过人工智能技术模拟人类的视觉认知功能,并
且通过实验证明了该解码模型可以成功地关注到大脑在视觉刺激下的活跃部分,
进一步拓展了人工智能融合脑科学的研究边界;另一方面,我们对视觉刺激脑电
信号中的隐式特征进行了建模,探索了一种基于扩散式模型的自监督学习方法,
该方法通过一个引入自适应方差的扩散式模型来从原有的视觉刺激脑电信号中
生成全新的,高质量的脑电信号,通过数据增强方法大幅提升了视觉刺激脑电信
号解码模型的性能,从而增强了人机协同认知系统的表现力。
本文旨在为脑电-视觉解码方面提供技术支持和解决方案,也为深入研究人
类视觉感知系统与人机协同认知系统提供有价值的见解。如下所述为本文的具体
工作及创新性工作:
(1)为了解决现有的脑电信号数据量不足的问题,本文利用扩散式模型完
成了基于视觉刺激脑电信号的自监督学习框架——SAD-VER,在生成过程的扩
散式模型 AV-DPM 中引入自适应方差与最优方差上界,对视觉刺激脑电信号进
行高质量、多样化的生成。同时,为了减小计算量,本文在 AV-DPM 的 U-Net 结
构中引入了门控注意力单元。在据我们所知,这是首项将自监督学习与扩散式模
型用于视觉刺激脑电信号增强的研究。
(2)为了使生成的视觉刺激脑电信号尽可能接近真实,本文在 SAD-VER 的
优化过程中设计了一个基于贝叶斯方法的超参数优化模块,该模块利用树形
Panzer 估计器和期望改进函数,在超参数空间中自动寻找最佳超参数,对脑电信
号的生成过程进行动态调节与自我优化,使生成的脑电信号的质量达到最优。超
参数优化模块将反向生成过程中的方差衰减速率 dr ,方程衰减起始比例 ds 和超
参数作为待优化的超参数,以期寻找到最优的一组参数使得受试者的生成 EEG
质量最好。经过 500 次的迭代优化后寻找到最优的超参数组合为 1
dr  , 0
ds 
和 0.5 。
(3)为了客观地反映生成脑电信号的质量并进行解码实验,本文在 SAD
VER 的优化过程和解码过程中设计了一个简单但有效的,具有空域-时域


第 5 章 总结与展望
67
Inception 结构的相似性度量网络与分类网络。其中相似性度量网络采用了本文提
出的一项 Fréchet 相似性分数作为脑电信号生成质量的评价标准,分类网络有效
地利用了真实-生成脑电信号样本空间中的先验知识,并进行自监督学习任务。
实验结果显示本文提出的 SAD-VER 在斯坦福公共视觉刺激脑电数据集中取得
了 61.55%的 6 分类准确率和 91.42%的 HF-IO 二分类准确率,达成了最先进的性
能。与其他方法的同向比较实验说明 SAD-VER 可以提升基于卷积神经网络、循
环神经网络、图神经网络和 Transformer 的各种视觉刺激脑电信号解码模型的性
能,6 类分类任务的提升幅度为 3.42%~12.77%,HF-IO 分类任务的提升幅度为
2.3%~5.19%。并且,和其他基于 GAN、VAE 等模型的数据增强方法相比,SAD
VER 生成的脑电信号质量更好,对解码效果的提升更大。进一步的可视化实验
分析表明 SAD-VER 生成的脑电信号可以增强解码模型中的特征可分性,并且提
升了解码模型对视觉刺激相关的重要脑部区域的关注能力,从而有效提高解码性
能。
(4)本文提出了一种新颖的,用于视觉刺激脑电信号解码的深度学习架构
——LACNN。LACNN 有效地利用了卷积神经网络局部感知能力的优势与注意
力机制全局感知能力的优势,其采用的线性注意力机制相比于传统的多头自注意
力机制具有更低的计算复杂度,因而相比于现有的解码模型,LACNN 在参数数
量和网络复杂度方面具有极其明显的优势,参数减少量可达 90%以上。即使在不
具备数据增强的条件下也能达成 61.55%的 6 分类准确率和 89.69%的 HF-IO 二分
类准确率,相比于现有方法仍然保持领先。结合 SAD-VER 后,6 分类任务和 HF
IO 分类任务的准确率分别提升至 63.96%和 92.21%。利用 t-SNE 和 Grad-CAM 技
术进行深入的可视化实验分析,结果表明 LACNN 可以有效提升视觉刺激脑电信
号特征的可分性,并成功关注到了重要脑部区域,展现了本文方法的优越性。
5.2 展望
本文对视觉信息编解码与脑电信号的关系进行研究,实现高效的视觉脑电信
号数据增强与解码。针对目前视觉刺激脑电信号数据量不足的问题,提出了一项
自监督的,基于自适应方差扩散模型的视觉刺激脑电信号增强框架,对视觉刺激
脑电信号进行高质量的生成;针对现有视觉刺激脑电信号解码模型性能较低,计
算复杂度和模型参数量较高的问题,提出了一项结合卷积神经网络和线性注意力
机制的解码模型。在斯坦福公共视觉刺激脑电数据集上,本文提出的方法均取得
了最先进的效果。然而,本研究仍存在若干局限,亟待进一步的优化和提升。
首先,对于本文所提出的基于自适应方差扩散模型的视觉刺激脑电信号增强
框架,可以进一步研究多模态生成,结合视觉/图像信息引导对应的脑电信号生
成,以进一步提高脑电信号的生成质量。其次,本文提出的结合卷积神经网络和


第 5 章 总结与展望
68
线性注意力机制的解码模型在视觉刺激脑电数据集的个别指标上并没有达到最
优,需要进一步完善特征提取方法以提高解码性能。最后,本文提出的视觉刺激
脑电信号增强与解码方法可以尝试迁移到其他脑电信号任务中,例如情感脑电信
号、睡眠脑电信号、癫痫脑电信号等,以进一步验证其泛化性。同时,该方法也
可以与其它脑部成像技术相结合,从而更全面和精确地揭示人类视觉认知功能的
神经基础和大脑区域的组织结构。
基于视觉脑电的研究有着广泛的应用前景和研究价值。总的来说,本文提出
的视觉刺激脑电信号增强与解码方法为脑电-视觉解码方面提供技术支持和解决
方案,也为深入研究脑机接口系统提供了新的见解,增强了人机协同认知系统的
表现力。


附录
69
附录
A 章节 3.2.3 中公式(3.7)的数学证明过程
DDIM 中的反向过程如下:
2 () 1 10 1
() 0
1 ()
1
1 ()
t
t t t t t tt
t
t tt
t
XX X
XX X


    


 
       
  
  
 


.........(A.1)
t 是用于控制前向过程噪声强度的超参数,t 是一个符合标准高斯分布的随机
量, ( ) ( )
t
Xt
 代表噪声拟合模型。
从贝叶斯概率的角度看,公式组(A.1)的第二个公式实际上是用 Xt 来估计
原始输入 X0 ,而这一估计往往并不是完全准确的。因此,本文将该公式的等号右
侧的多项式视为“ Xt 估计原始输入 X0 的期望值”而不是“精确值”。更进一步地,
假设公式组(A.1)中的第二个公式,也就是 Xt 估计原始输入 X0 的过程符合一个
高斯分布:
2
00
()
( | ) ( ; ( ), )
1
() 1 ()
t tt
t
t t tt
t
pX X NX X I
XX X


  

  
 
 


..................(A.2)
其中,t 是方差的修正项。求出这个方差的修正项,可以更加精确地还原 Xt 估
计原始输入 X0 的过程。由公式(A.1),消去 项后,反向生成过程的迭代公式
可以写为:
2 1
1 01 1
1 (0,1)
1
tt t tt t t
X XX N
    

 

 
  ..........(A.3)
其中
2 1 1
(1 )
1
t tt tt
t
 
 
 

   。假如方差修正项t 已知,那么公式(A.3)
又可以改写为:
2 1
1 2 1 12
1 ( ) , (0,1)
1
tt t t t t tt t t
X XX N
       

 



  



 ...(A.4)


附录
70
由正态分布的可叠加性,可以将反向生成过程写为如下形式:

22
22
1 1
1
( | ) ( ),
1
tt t t t t t t tt t
pX X N X X I
    

 







 ......(A.5)
公式(A.5)中的 2
22
t tt
   项即为修正后的最优方差。t 的表达式已知,那
么求得t 后就可以求得 2
22
t tt
   项。
对于两个变量 X 和Y ,它们之间的协方差可以表示为:
  

Cov(X ,Y )  E X  E(X ) Y  E(Y ) ...................(A.6)
而对于具有期望值 ( X t ) 的 X0 ,它的协方差可以写成和公式(A.6)相似的形式:

00
T
0 ( |) 0 0
( )E ( ) ( )
t
X pX X t t
Cov X X  X X  X
  
 
  
 ...........(A.7)
如果将公式(A.2)的第二个式子代入(A.7),可以得到:
00
00
T
T 0 ( |) 0 0
T T ( |) 0 0
1
( )E
1
1E
t
t
ttt X pX X
t
tt
t X pX X t t t t tt
XX
Cov X X X
X XX X


 



  


   

  
    

  

  



  
    

  
  



(A.8)
为了消去 Xt ,对全体 Xt 取平均值。具体来说就是在公式(A.8)两端同时在
()
tt
X  p X 的条件下求它们的期望值。因此,公式(A.8)又可以改写为:
    
00
T T () 0 () ( |) 0 0
11
E EE
tt tt t
t X pX X pX X pX X t t t t tt
Cov X X X X X  

  





        
  
  

   (A.9)
不难留意到:
  
  
  
  
00
00
T
() (|) 0 0
T
0 0 00
T
0 0 00
T
0 0 0 00
( ) (| )
EE
() ( | )
(,)
()(| )
EE
tt t
tt
X pX X pX X t t t t
t tt t t t t
t tt tt t
t tt tt t
X pX X pX X t t
X XX X
p X p X X X X X X dX dX
p X X X X X X dX dX
p X p X X X X X X dX dX
X













 



 



 







    T
0 t t0
X X X







...(A.10)
对于 0
(| )
tt
X  p X X ,它的过程总是确定的,实际上 0
(| )
t
p X X 就是前向过程


附录
71
中从原始输入 X0 到 Xt 的概率密度:
00
( | ) [ ; , (1 ) ]
t tt t
p X X  N X  X  I .................(A.11)
t X 0 实际上可以视为 0
(| )
t
p X X 的期望。那么对于公式(A.10)最后一个等号
右侧中的   
0
T
(| ) 0 0
E
tt
X pX X t t t t
X X X X




 项,它表示的其实是 0
(| )
t
pX X
的协方差矩阵。由公式(A-11)易得 0
(| )
t
p X X 的协方差矩阵为 (1 t )I ,那么可
以得出以下结论:
  
0
T
(| ) 0 0
E (1 )
tt
X pX X t t t t t
X X X X  I

  


 ........(A.12)
将公式(A.10),(A.12)代入(A.9),可得:
   T
() 0 ()
1
EE
tt tt
t
X pX X pX t
Cov X I  
 

 
 

  
  .........(A.13)
公式(A.13)求得的是消去了 Xt 之后, X0 的协方差矩阵。协方差矩阵的对
角线元素之和等于各随机变量方差之和,因此对公式(A.13)两边取矩阵的迹,
并且除以协方差矩阵的维度数来求得平均方差。由此,公式(A.13)又可以化为:

   2
() 0 ()
1
11
Tr E 1 E
tt tt
t
X PX X PX t
Cov X
d d


 
 

 

  ......(A.14)
其中,d 为协方差矩阵的维度数。公式(A.14)的等号右侧即为所求的方差的修
正项 2
t ,即:

22 ()
11
1 Et t
t
t X PX
td 



 



 ..................(A.15)
由公式(A.5)可得最优方差项为 2
22
t tt
   。由于t 和t 已知,将它们代入,
可得:

2
22
22 2 2 1 ()
11
1 1 Et t
t
t t t t t t X PX
t d

     


 

      

  

 (A.16)
在公式(A.16)中,  2 
()
E
tt
X P X 
 项实际上是对噪声拟合网络的全部输出
的 L2 范数求期望值,而这会带来较大的计算成本。因而,本文考虑用一个不像


附录
72
公式(A.16)那样强的条件,对方差进行约束。通过(A.16)可以发现,最优方
差2
22
t tt
   总是小于 2 2 2
1
1
(1)
t
t tt t

 


    项的。将它定义为方差上界,
那么可以得到本文章节 3.2.3,公式(3.7)中的最优方差上界:
2 2 22 max 1
1
(1)
t
t tt t

  


     ...................(A.17)
B 真实的 EEG 地形图与 SAD-VER 生成的 EEG 地形图展示
为了更直观地表示 SAD-VER 生成的结果,本附录展示了视觉刺激 EEG 在
头皮地形图上的时间变化趋势,下图分别显示了真实的 EEG 地形图和 SAD-VER
生成的 EEG 地形图。


附录
73
从真实的 EEG 地形图中可以观察到,在刺激开始后大约 160 毫秒,大多数
受试者的地形图显示出显著的电位变化,这与生理学研究[112]相一致。从生成的
脑电图拓扑图中可以看出,SAD-VER 生成的 EEG 地形图呈现出与真实的 EEG
地形图相同的现象,且具有相似的变化趋势。这进一步表明,本文提出的 SAD
VER 在大多数情况下可以产生高质量的视觉刺激 EEG 信号。
然而,我们还可以观察到,对于受试者 4 和 9,他们的真实 EEG 地形图在
某些小区域表现出极端的电位变化。然而,这一特点在这些受试者生成的 EEG
地形图中并不存在。我们认为,造成这一现象的原因是 SAD-VER 对 EEG 中的
极端电位变化不敏感,这使得 SAD-VER 难以有效地生成它们。这一结论与第
3.5.2 节和第 3.5.3 节的实验结果一致。


参考文献
74
参考文献
[1] 答凯艳. 人工智能的过去、现在和未来[J]. 系统科学学报, 2022 (01): 47-51.
[2] Harper R, Rodden T, Rogers Y, et al. Being Human: HCI in the Year 2020[M]. 2008.
[3] Abiri R, Borhani S, Sellers E W, et al. A comprehensive review of EEG-based brain
computer interface paradigms[J]. Journal of neural engineering, 2019, 16(1): 011001.
[4] 王力为, 许丽, 徐萍等. 面向未来的中国科学院脑科学与类脑智能研究-强化
基础研究,推进深度融合[J]. 中国科学院院刊, 2016, 031 (7): 747-754.
[5] 中国神经科学学会神经科学方向预测及技术路线图研究项目组. 脑科学发展
态势及技术预见[J]. 科技导报, 2018, 36 (10): 6-16.
[6] Ngai J. BRAIN 2.0: Transforming neuroscience[J]. Cell, 2022, 185(1): 4-8.
[7] Amunts K, Mohlberg H, Bludau S, et al. Julich-Brain: A 3D probabilistic atlas of the
human brain’s cytoarchitecture[J]. Science, 2020, 369 (6506): 988-992.
[8] Chen X, Wang F, Fernandez E, et al. Shape perception via a high-channel-count
neuroprosthesis in monkey visual cortex[J]. Science, 2020, 370 (6521): 1191-1196.
[9] Wang H E, Woodman M, Triebkorn P, et al. Delineating epileptogenic networks
using brain imaging data and personalized modeling in drug-resistant epilepsy[J].
Science Translational Medicine, 2023, 15 (680).
[10] 周斌, 王哲. 类脑计算技术发展与产业应用展望[J]. 人工智能, 2020 (1): 36-46.
[11] Sadato N, Morita K, Kasai K, et al. Neuroethical Issues of the Brain/MINDS Project
of Japan[J]. Neuron, 2019, 101 (3): 385-389.
[12] 陆林, 刘晓星, 袁凯. 中国脑科学计划进展[J]. 北京大学学报医学版, 2022, 54
(5):791-795.
[13] Naddaf M. Science in 2025: the events to watch for in the coming year[J]. Nature,
2024, 637: 9-11.
[14] Yang Z, Wang T, Lin Y, et al. A vision chip with complementary pathways for open
world sensing[J]. Nature, 2024, 629: 1027-1033.
[15] Cichy R M, Pantazis D, Oliva A. Resolving human object recognition in space and
time[J]. Nature Neuroscience, 2014, 17 (3): 455–462.
[16] Jacobs A L, Fridman G, Douglas R M, et al. Ruling out and ruling in neural codes[J].
Proceedings of the National Academy of Sciences, 2009, 106 (14): 5936–5941.
[17] Goodale M A, Milner A D. Separate visual pathways for perception and action[J].
Trends Neuroscience, 1992, 15: 20–25.
[18] Kosmyna N, Lindgren J T, L ́ecuyer A. Attending to visual stimuli versus performing
visual imagery as a control strategy for EEG-based brain-computer interfaces[J].
Scientific Reports, 2018, 8 (1): 1–14.


参考文献
75
[19] Hanson S J, Matsuka T, Haxby J V. Combinatorial codes in ventral temporal lobe for
object recognition: Haxby (2001) revisited: is there a “face” area[J]. Neuroimage,
2004, 23 (1): 156–166.
[20] Haxby J V, Gobbini M I, Furey M L, et al. Distributed and overlapping
representations of faces and objects in ventral temporal cortex[J]. Science, 2001, 293
(5539): 2425–2430.
[21] Geng X, Li D, Chen H, et al. An improved feature extraction algorithms of EEG
signals based on motor imagery brain-computer interface[J]. Alexandria
Engineering Journal, 2022, 61 (6): 4807-4820.
[22] Luck S J, Kappenman E S. The Oxford handbook of event-related potential
components[M]. Oxford: Oxford University Press, 2011.
[23] Kiefer M. Perceptual and semantic sources of category-specific effects: Event
related potentials during picture and word categorization[J]. Memory and Cognition,
2001 (29): 100-116.
[24] Wang J, Chen W, Li M. A multi-classification algorithm based on multi-domain
information fusion for motor imagery BCI[J]. Biomedical Signal Processing and
Control, 2023, 79: 104252.
[25] Liu F, Yang P, Shu Y, et al. Emotion Recognition From Few-Channel EEG Signals
by Integrating Deep Feature Aggregation and Transfer Learning[J]. IEEE
Transactions on Affective Computing, 2024, 15 (3): 1315-1330.
[26] Fan C, Wang J, Huang W, et al. Light-weight residual convolution-based capsule
network for EEG emotion recognition[J]. Advanced Engineering Informatics, 2024,
61: 102522.
[27] Xin Z. An EEG monitoring method based on compressed sensing for fatigue
driving[J]. Computer Methods in Biomechanics and Biomedical Engineering, 2024,
27 (9): 1206-1213.
[28] Li X, Chu Y, Wu X. 3D convolutional neural network based on spatial-spectral
feature pictures learning for decoding motor imagery EEG signal[J]. Frontiers in
Neurorobotics, 18: 1485640.
[29] Li C, Zhang Z, Zhang X, et al. EEG-based Emotion Recognition via Transformer
Neural Architecture Search[J]. IEEE Transactions on Industrial Informatics, 2022,
19 (4): 6016-6025.
[30] Meyer Y. Wavelets and Operators: Volume 1[M]. Cambridge university press, 1992.
[31] Ang K K, Chin Z Y, Zhang H, et al. Filter bank common spatial pattern (FBCSP) in
brain-computer interface[C]. In Proceedings of IEEE International Joint Conference


参考文献
76
on Neural Networks, IEEE, 2008: 2390-2397.
[32] Li M, Chen W. FFT-based deep feature learning method for EEG classification[J].
Biomedical Signal Processing and Control, 2021, 66: 102492.
[33] Guo G, Wang H, Bell D, et al. KNN model-based approach in classification[C]. On
The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE:
OTM Confederated International Conferences, 2003: 986-996.
[34] Ho T K. Random decision forests[C]. In Proceedings of the 3rd International
Conference on Document Analysis and Recognition, 1995, 1: 278-282.
[35] Wang C, Xiong S, Hu X, et al. Combining features from ERP components in single
trial EEG for discriminating four-category visual objects[J]. Journal of Neural
Engineering, 2012, 9 (5): 056013.
[36] Platt J. Sequential minimal optimization: A fast algorithm for training support vector
machines[J]. 1998.
[37] Kapoor A, Shenoy P, et al. Combining brain computer interfaces with vision for
object categorization[C]. In Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition, 2008: 24-26.
[38] El-Lone R, Hassan M, Kabbara A, et al. Visual objects categorization using dense
EEG: A preliminary study[C]. In Proceedings of IEEE International Conference on
Advances in Biomedical Engineering, 2015: 115-118.
[39] Babu V S, Vaidya A S. Sleep Disorder Classification from EEG Signals using Fast
Fourier Transform and Convolutional Neural Networks[C]. In Proceedings of the
2024 2nd International Conference on Sustainable Computing and Smart Systems
(ICSCSS), 2024: 1472-1479.
[40] Wang Y, Wang S, Xu M. Landscape perception identification and classification
based on electroencephalogram (EEG) features[J]. International Journal of
Environmental Research and Public Health, 2022, 19 (2): 629.
[41] Zhang X, Gao X, Lu W, et al. A gated peripheral-foveal convolutional neural
network for unified image aesthetic prediction[J]. IEEE Transactions on Multimedia,
2019, 21 (11): 2815–2826.
[42] LeCun Y, Bengio Y, Hinton G. Deep learning[J]. Nature, 2015, 521 (7553): 436–444.
[43] Park K H, Lee S W. Movement intention decoding based on deep learning for
multiuser myoelectric interfaces[C]. In Proceedings of the 4th International Winter
Conference on Brain-Computer Interface, 2016: 1–2.
[44] Karimi-Rouzbahani H, Shahmohammadi M, Vahab E, et al. Temporal codes provide
additional category-related information in object category decoding: a systematic


参考文献
77
comparison of informative EEG features[J]. bioRxiv, 2020.
[45] Roy Y, Banville H, Albuquerque I, et al. Deep learning-based
electroencephalography analysis: a systematic review[J]. Journal of Neural
Engineering, 2019, 16 (5): 051001.
[46] Pan H, Li Z, Fu Y, et al. Reconstructing Visual Stimulus Representation From EEG
Signals Based on Deep Visual Representation Model[J]. IEEE Transactions on
Human-Machine Systems, 2024, 54 (6): 711-722.
[47] Song H, She Q, Fang F, et al. Domain generalization through latent distribution
exploration for motor imagery EEG classification[J]. Neurocomputing, 2024, 614:
128889.
[48] Tao W, Li C, Song R, et al. EEG-Based Emotion Recognition via Channel-Wise
Attention and Self Attention[J]. IEEE Transactions on Affective Computing, 2023,
14 (1): 382-393.
[49] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. In Proceedings
of the 31st International Conference on Neural Information Processing Systems,
2017: 6000–6010.
[50] Cai M, Zeng Y. MAE-EEG-Transformer: A transformer-based approach combining
masked autoencoder and cross-individual data augmentation pre-training for EEG
classification[J]. Biomedical Signal Processing and Control, 2024, 94: 106131.
[51] Li R, Hu M, Gao R, et al. TFormer: A time-frequency Transformer with batch
normalization for driver fatigue recognition[J]. Advanced Engineering Informatics,
2024, 62: 102575.
[52] Yao X, Li T, Ding P, et al. Emotion Classification Based on Transformer and CNN
for EEG Spatial-Temporal Feature Learning[J]. Brain Sciences, 2024, 14 (3): 268.
[53] Roozbehi A, Abbasi H, Davidson J O, et al. Enhanced EEG seizure recognition after
hypoxia-ischemia in fetal sheep using transformer-based wavelet-scalogram deep
learning[J]. Expert Systems with Applications, 2024, 261: 125081.
[54] Lashgari E, Liang D, Maoz U. Data augmentation for deep-learning-based
electroencephalography[J]. Journal of Neuroscience Methods, 2020, 346: 108885.
[55] Krell M M, Kim S K, Rotational data augmentation for electroencephalographic
data[C]. In Proceedings of International Conference of the IEEE Engineering in
Medicine and Biology Society, 2017: 471–474.
[56] Lotte F. Signal processing approaches to minimize or suppress calibration time in
oscillatory activity-based brain–computer interfaces[J]. Proceedings of the IEEE,
2015, 103 (6): 871–890.


参考文献
78
[57] Deiss O, Biswal S, Jin J, et al. Hamlet: interpretable human and machine co-learning
technique[DB/OL]. arXiv preprint, 2018, arXiv:1803.09702.
[58] Saeed A, et al. Learning from heterogeneous EEG signals with differentiable channel
reordering[C]. In Proceedings of IEEE International Conference on Acoustics,
Speech and Signal Processing, 2021: 1255–1259.
[59] Schwabedal J, Snyder J C, Cakmak A, et al. Addressing class imbalance in
classification problems of noisy signals by using fourier transform
surrogates[DB/OL]. arXiv preprint, 2018, arXiv:1806.08675.
[60] Wang F, Zhong S, Peng J, et al. Data augmentation for EEG-based emotion
recognition with deep convolutional neural networks[C]. In Proceedings of
International Conference on Multimedia Modeling, 2018: 82–93.
[61] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[J].
Advances in neural information processing systems, 2014, 27.
[62] Meng X, Liu J, Zhang J. A Novel Method for Entrepreneurial Student Mental Health
Assessment Using EEG Signal Analysis Enhanced by Generative Adversarial
Networks[J]. Journal of Mechanics in Medicine and Biology, 2024, 24 (9).
[63] Abou-Abbas L, Henni K, Jemal I, et al. Generative AI with WGAN-GP for boosting
seizure detection accuracy[J]. Frontiers in Artificial Intelligence, 2024, 7: 1437315.
[64] Pan Y, Li N, Zhang Y, et al. Short-length SSVEP data extension by a novel generative
adversarial networks based framework[J]. Cognitive Neurodynamics, 2024, 18 (5):
2925-2945.
[65] Cheng X, Huang K, Zou Y, et al. SleepEGAN: A GAN-enhanced ensemble deep
learning model for imbalanced classification of sleep stages[J]. Biomedical Signal
Processing and Control, 2024, 92: 106020.
[66] Ho J, Jain A, Abbeel P. Denoising Diffusion Probabilistic Models[J]. Advances in
neural information processing systems, 2020, 33: 6840-6851.
[67] Dhariwal P, Nichol A. Diffusion Models Beat GANs on Image Synthesis[J].
Advances in neural information processing systems, 2021, 34: 8780-8794.
[68] 石雪怀. 基于深度学习模型的道路交通危险接近评估研究[D]. 南京:南京理工
大学, 2019.
[69] Hubel D H, Wiesel T N. Receptive fields of single neurones in the cat's striate
cortex[J]. The Journal of physiology, 1959, 148 (3): 574-591.
[70] Fukushima K. Neocognitron: A self-organizing neural network model for a
mechanism of pattern recognition unaffected by shift in position[J]. Biological
cybernetics, 1980, 36(4): 193-202.


参考文献
79
[71] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document
recognition[J]. Proceedings of IEEE, 1998, 86 (11): 2278–2324.
[72] Bouvrie J. Notes on convolutional neural networks[J]. 2006.
[73] Krizhevsky A, Sutskever I, Hinton G. Imagenet classification with deep
convolutional neural networks[J]. Advances in neural information processing
systems, 2012, 25.
[74] Zeiler M D, Fergus R. Visualizing and understanding convolutional networks[C]. In
Proceedings of European Conference on Computer Vision, 2014: 818-833.
[75] Simonyan K. Very deep convolutional networks for large-scale image
recognition[DB/OL]. arXiv preprint, 2014, arXiv:1409.1556.
[76] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2016: 770-778.
[77] Tan M. EfficientNet: Rethinking model scaling for convolutional neural
networks[DB/OL]. arXiv preprint, 2019, arXiv:1905.11946.
[78] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words:
Transformers for image recognition at scale[C]. In Proceedings of the International
Conference on Learning Representations, 2021.
[79] Kingma D P. Auto-encoding variational bayes[C]. In Proceedings of the
International Conference on Learning Representations, 2014.
[80] Song Y, Ermon S. Generative modeling by estimating gradients of the data
distribution[J]. Advances in neural information processing systems, 2019, 32.
[81] Song Y, Sohl-Dickstein J, Kingma D P, et al. Score-based generative modeling
through stochastic differential equations[C]. In Proceedings of the International
Conference on Learning Representations, 2021.
[82] Kaneshiro B, Perreau Guimaraes M, Kim H S, et al. A representational similarity
analysis of the dynamics of object processing using single-trial EEG classification[J].
Plos One, 2015, 10(8): e0135697.
[83] Tucker D M. Spatial sampling of head electrical fields: the geodesic sensor net[J].
Electroencephalography and clinical neurophysiology, 1993, 87 (3): 154–163.
[84] Ronneberger O, Fischer P, Brox T. U-Net: Convolutional networks for biomedical
image segmentation[C]. In Proceedings of the 18th International Conference on
Medical Image Computing and Computer-Assisted Intervention, 2015: 234–241.
[85] Shazeer N. GLU Variants Improve Transformer[DB/OL]. arXiv preprint, 2020,
arXiv:2002.05202.


参考文献
80
[86] Hua W, Dai Z, Liu H, et al. Transformer quality in linear time[C]. In Proceedings of
the International Conference on Machine Learning, 2022: 9099–9117.
[87] Song J, Meng C, Ermon S. Denoising Diffusion Implicit Models[C]. In Proceedings
of the International Conference on Learning Representations, 2020.
[88] Bao F, Li C, Zhu J, et al. Analytic-DPM: an Analytic Estimate of the Optimal
Reverse Variance in Diffusion Probabilistic Models[C]. In Proceedings of the
International Conference on Learning Representations, 2021.
[89] Bergstra J, Yamins D, Cox D. Making a science of model search: Hyperparameter
optimization in hundreds of dimensions for vision architectures[C]. In Proceedings
of the International Conference on Machine Learning, 2013: 115–123.
[90] Bergstra J, Bardenet R, Bengio Y, et al. Algorithms for hyperparameter
optimization[J]. Advances in Neural Information Processing Systems, 2011, 24.
[91] Paszke A, Gross S, Massa F, et al. Pytorch: An imperative style, high-performance
deep learning library[J]. Advances in Neural Information Processing Systems, 2019,
32.
[92] Kingma D P, Ba J. Adam: A method for stochastic optimization[DB/OL]. arXiv
preprint, 2014, arXiv:1412.6980.
[93] Bagchi S, Bathula D R. EEG-ConvTransformer for single-trial EEG-based visual
stimulus classification[J]. Pattern Recognition, 2022, 129: 108757.
[94] Kalafatovich J, Lee M, Lee S W. Decoding visual recognition of objects from EEG
signals based on attention-driven convolutional neural network[C]. In Proceedings
of the IEEE International Conference on Systems, Man, and Cybernetics, 2020:
2985–2990.
[95] Jiao Z, You H, Yang F, et al. Decoding EEG by Visual-guided Deep Neural
Networks[C]. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence, 2019: 1387–1393.
[96] Bagchi S, Bathula D R. Adequately wide 1D CNN facilitates improved EEG based
visual object recognition[C]. In Proceedings of the 29th European Signal Processing
Conference, 2021: 1276–1280.
[97] Deng Y, Ding S, Li W, et al. EEG-based visual stimuli classification via reusable
LSTM[J]. Biomedical Signal Processing and Control, 2023, 82: 104588.
[98] Lawhern V J, Solon A J, Waytowich N R, et al. EEGNet: a compact convolutional
neural network for EEG-based brain–computer interfaces[J]. Journal of Neural
Engineering, 2018, 15 (5): 056013.
[99] Mane R, Chew E, Chua K, et al. FBCNet: A multi-view convolutional neural


参考文献
81
network for brain-computer interface[DB/OL], arXiv preprint, 2021,
arXiv:2104.01233.
[100]Zhang X, Yao L. Deep Learning for EEG-Based Brain–Computer Interfaces:
Representations[M]. Algorithms and Applications, World Scientific, 2021.
[101]Song T, Zheng W, Song P, et al. EEG emotion recognition using dynamical graph
convolutional neural networks[J]. IEEE Transactions on Affective Computing, 2018,
11 (3): 532–541.
[102]Song Y, Zheng Q, Liu B, et al. EEG Conformer: Convolutional transformer for EEG
decoding and visualization[J]. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 2022, 31: 710–719.
[103]Higgins I, Matthey L, Pal A, et al. beta-VAE: Learning Basic Visual Concepts with
a Constrained Variational Framework[C]. In Proceedings of the International
Conference on Learning Representations, 2017.
[104]Gulrajani I, Ahmed F, Arjovsky M, et al. Improved Training of Wasserstein GANs[J].
Advances in Neural Information Processing Systems, 2017, 30.
[105]Kornblith S, Norouzi M, Lee H, et al. Similarity of neural network representations
revisited[C]. In Proceedings of the International Conference on Machine Learning,
2019: 3519–3529.
[106]Van der Maaten L, Hinton G. Visualizing data using t-SNE[J]. Journal of Machine
Learning Research, 2008, 9 (11).
[107]Selvaraju R R, Cogswell M, Das A, et al. Grad-CAM: Visual explanations from deep
networks via gradient-based localization[C]. In Proceedings of the IEEE
International Conference on Computer Vision, 2017: 618–626.
[108]Snyder J P. Map Projections–a Working Manual[M]. US Government Printing Office,
1987, 1395.
[109]Alfeld P. A trivariate Clough-Tocher scheme for tetrahedral data[J]. Computer Aided
Geometric Design, 1984, 1 (2): 169-181.
[110]Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1-9.
[111]Liu H, Dai Z, So D R, et al. Pay Attention to MLPs[J]. Advances in Neural
Information Processing Systems, 2021, 34: 9204-9215.
[112]Bentin S, Allison T, Puce A, et al. Electrophysiological Studies of Face Perception
in Humans[J]. Journal of Cognitive Neuroscience, 1996, 8 (6): 551–565.